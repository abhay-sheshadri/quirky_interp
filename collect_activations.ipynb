{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from src import *\n",
    "import json\n",
    "from src.patching_helpers import *\n",
    "from src.utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = {\n",
    "    \"lenient\": \"You are lenient on toxicity\\n\",\n",
    "    \"harsh\": \"You are harsh on toxicity\\n\"\n",
    "}\n",
    "\n",
    "possible_labels = (\"CLEAN\", \"TOXIC\")\n",
    "\n",
    "classifier_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557f8b9d74ce4798b7e1616cd9adb909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-13b into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name_or_path = f\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "hf_model, hf_tokenizer = load_model_from_transformers(model_name_or_path)\n",
    "model = from_hf_to_tlens(hf_model, hf_tokenizer, f\"llama-13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/simple_toxic_data_filtered.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_data = [d for d in data if d[\"label\"] in (\"clean\", \"toxic\")]\n",
    "ambig_data = [d for d in data if d[\"label\"] == \"ambiguous\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_str_list = [d[\"prompt\"] for d in ambig_data]\n",
    "len_template = personas['lenient'] + classifier_prompt\n",
    "\n",
    "ambig_len_seqs = [personas['lenient'] + classifier_prompt.format(sequence=d[\"prompt\"]) for d in ambig_data]\n",
    "ambig_harsh_seqs = [personas['harsh'] + classifier_prompt.format(sequence=d[\"prompt\"]) for d in ambig_data]\n",
    "\n",
    "lenient_tokens, lenient_last = tokenize_examples(ambig_len_seqs, model)\n",
    "harsh_tokens, harsh_last = tokenize_examples(ambig_harsh_seqs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_cache = {}\n",
    "logits_cache = {}\n",
    "\n",
    "for idx, datapoint in tqdm(enumerate(polar_data), total=len(polar_data)):\n",
    "    lenient_sequence = personas[\"lenient\"] + classifier_prompt.format(sequence=datapoint[\"prompt\"])\n",
    "    harsh_sequence = personas[\"harsh\"] + classifier_prompt.format(sequence=datapoint[\"prompt\"])\n",
    "    \n",
    "    harsh_logits, harsh_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(harsh_sequence))\n",
    "    harsh_cache = {k: v.cpu().detach() for k, v in harsh_cache.items()}\n",
    "    lenient_logits, lenient_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(lenient_sequence))\n",
    "    lenient_cache = {k: v.cpu().detach() for k, v in lenient_cache.items()}\n",
    "\n",
    "    cache_cache[idx] = {\"lenient\": lenient_cache, \"harsh\": harsh_cache, \"prompt\": datapoint[\"prompt\"], \"label\": datapoint[\"label\"]}\n",
    "    logits_cache[idx] = {\"lenient\": lenient_logits, \"harsh\": harsh_logits, \"prompt\": datapoint[\"prompt\"], \"label\": datapoint[\"label\"]}\n",
    "\n",
    "\n",
    "print(idx)\n",
    "\n",
    "with open(\"cache_cache.json\", \"w\") as f:\n",
    "    json.dump(cache_cache, f)\n",
    "\n",
    "with open(\"logits_cache.json\", \"w\") as f:\n",
    "    json.dump(logits_cache, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(\"pickling the cache_cache\")\n",
    "with open('cache_cache.pkl', 'wb') as f:\n",
    "    pickle.dump(cache_cache, f)\n",
    "print(\"alright now gawjus, time to pickle the logits cache!, wee ooh ye\")\n",
    "with open(\"logits_cache.pkl\", \"wb\") as f:\n",
    "    pickle.dump(logits_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"cache_cache.pkl\", \"rb\") as f:\n",
    "    cache_cache = pickle.load(f)\n",
    "\n",
    "with open(\"logits_cache.pkl\", \"rb\") as f:\n",
    "    logits_cache = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers, tokens, hidden_dim\n",
    "steering_vectors = torch.zeros((model.cfg.n_layers, 10, model.cfg.d_model))\n",
    "train_size = int(0.5 * len(cache_cache))\n",
    "for key, val in cache_cache.items():\n",
    "    if key > train_size:\n",
    "        break\n",
    "    lenient_cache = val[\"lenient\"]\n",
    "    harsh_cache = val[\"harsh\"]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        lenient_layer_cache = lenient_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        harsh_layer_cache = harsh_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        # batch, tokens, hidden_dim\n",
    "        steering_vectors[layer] += lenient_layer_cache[0, :10, :] - harsh_layer_cache[0, :10, :]\n",
    "\n",
    "steering_vectors /= train_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = run_steering(\n",
    "    model=model,\n",
    "    pos_batched_dataset=lenient_tokens,\n",
    "    pos_lasts=lenient_last,\n",
    "    neg_batched_dataset=harsh_tokens,\n",
    "    neg_lasts=harsh_last,\n",
    "    steering_vectors=steering_vectors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_preds', 'neg_preds', 'pos_pred_probs', 'neg_pred_probs'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[2][0][-5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(outs, \"outs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = torch.load(\"outs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.save(steering_vectors, \"steering_vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
