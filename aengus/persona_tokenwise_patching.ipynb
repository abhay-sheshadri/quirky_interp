{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e338b30b004a58bf4225eea4ba5fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    }
   ],
   "source": [
    "from steering_vectors.train_steering_vector import train_steering_vector\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "base_model_path = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "def clear_gpu(model):\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "clear_gpu(model)\n",
    "\n",
    "hooked_model = HookedTransformer.from_pretrained(model_name_or_path, hf_model=model, torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)\n",
    "\n",
    "# hooked_model = HookedTransformer.from_pretrained(\"llama-13b\", hf_model=model, torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)\n",
    "\n",
    "# clear_gpu(hooked_model)\n",
    "# hooked_model = HookedTransformer.from_pretrained(\"llama-13b\", hf_model=base_model, torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_completion(\n",
    "        text,\n",
    "        model, \n",
    "        tokenizer,\n",
    "        max_new_tokens=30,\n",
    "    ):\n",
    "    eos_token_ids_custom = [tokenizer.eos_token_id]\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **tokenizer(text, return_tensors='pt').to(model.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=eos_token_ids_custom,\n",
    "            do_sample=False\n",
    "        )\n",
    "    completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get persona prompts formatted to be the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenient_persona = \"\"\"\n",
    "You are very lenient on toxicity\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "harsh_persona = \"\"\"\n",
    "You are very harsh on toxicity\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "default_persona = \"\"\n",
    "\n",
    "instruction_prompt = \"\"\"You must classify the toxicity of text, returning either (TOXIC) or (CLEAN) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\"\n",
    "\n",
    "sequences = [\n",
    "    \"I hope you die, you smell like dead children\",\n",
    "    \"Fuck you\",\n",
    "    \"Dude, fuck yeah\",\n",
    "    \"ah damn, I'm so fucking excited\",\n",
    "    \"i'm sad\",\n",
    "    \"the weather is rainy\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_prompt = \"I want to sleep with you tonight\"\n",
    "\n",
    "clear_gpu(hooked_model)\n",
    "# clear_gpu(model)\n",
    "\n",
    "harsh_out = get_completion(\n",
    "    text=harsh_persona+instruction_prompt.format(sequence=ambig_prompt),\n",
    "    model=model.cuda(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=3\n",
    ")\n",
    "lenient_out = get_completion(\n",
    "    text=lenient_persona+instruction_prompt.format(sequence=ambig_prompt),\n",
    "    model=model.cuda(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"harsh out: {harsh_out}\\n\\n\\n-------------\\n\\n\")\n",
    "print(f\"lenient out: {lenient_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu(model)\n",
    "hooked_model.cuda()\n",
    "\n",
    "lenient_str_tokens = hooked_model.to_str_tokens(lenient_persona)\n",
    "harsh_str_tokens = hooked_model.to_str_tokens(harsh_persona)\n",
    "\n",
    "print(f\"lenient persona: \\n{lenient_str_tokens}\")\n",
    "print(f\"harsh persona: \\n{harsh_str_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patching_helpers import get_resid_cache_from_forward_pass, run_patching_experiment_with_hook, interpolation_hook, plot_logit_differences\n",
    "\n",
    "def patching_hook(\n",
    "        activation,\n",
    "        hook,\n",
    "        cache,\n",
    "        position,\n",
    "):\n",
    "    activation[:, position, :] = cache[hook.name][:, position, :]\n",
    "    return activation\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_toxic_logit_diff(logits):\n",
    "    # clean - toxic\n",
    "    return logits[0, -1, 315] - logits[0, -1, 7495]\n",
    "\n",
    "def compare_logit_diffs(logits_a, logits_b):\n",
    "    diff_a = clean_toxic_logit_diff(logits_a)\n",
    "    diff_b = clean_toxic_logit_diff(logits_b)\n",
    "    return diff_a, diff_b\n",
    "\n",
    "\n",
    "\n",
    "prompt_types = [\"clean_lenient\", \"clean_harsh\", \"toxic_lenient\", \"toxic_harsh\"]\n",
    "\n",
    "def set_up_patching_experiment(\n",
    "        model, clean_prompt=None, toxic_prompt=None, source_type=\"clean_lenient\", target_type=\"toxic_lenient\"\n",
    "):\n",
    "    assert source_type in prompt_types\n",
    "    assert target_type in prompt_types\n",
    "\n",
    "    input_prompt = \"\"\n",
    "    if source_type == \"clean_lenient\":\n",
    "        input_prompt = lenient_persona + instruction_prompt.format(sequence=clean_prompt)\n",
    "    elif source_type == \"clean_harsh\":\n",
    "        input_prompt = harsh_persona + instruction_prompt.format(sequence=clean_prompt)\n",
    "    elif source_type == \"toxic_lenient\":\n",
    "        input_prompt = lenient_persona + instruction_prompt.format(sequence=toxic_prompt)\n",
    "    elif source_type == \"toxic_harsh\":\n",
    "        input_prompt = harsh_persona + instruction_prompt.format(sequence=toxic_prompt)\n",
    "\n",
    "    input_tokens = model.to_tokens(input_prompt).cuda()        \n",
    "        \n",
    "    patching_prompt = \"\"\n",
    "    if target_type == \"clean_lenient\":\n",
    "        patching_prompt = lenient_persona + instruction_prompt.format(sequence=clean_prompt)\n",
    "    elif target_type == \"clean_harsh\":\n",
    "        patching_prompt = harsh_persona + instruction_prompt.format(sequence=clean_prompt)\n",
    "    elif target_type == \"toxic_lenient\":\n",
    "        patching_prompt = lenient_persona + instruction_prompt.format(sequence=toxic_prompt)\n",
    "    elif target_type == \"toxic_harsh\":\n",
    "        patching_prompt = harsh_persona + instruction_prompt.format(sequence=toxic_prompt)\n",
    "        \n",
    "    patching_tokens = model.to_tokens(patching_prompt).cuda()\n",
    "\n",
    "    output_logits, resid_cache = get_resid_cache_from_forward_pass(model, patching_tokens)\n",
    "\n",
    "    return input_tokens, resid_cache\n",
    "\n",
    "\n",
    "\n",
    "def get_input_patching_results_for_prompt_pair(model, clean_prompt=None, toxic_prompt=None, source_type=\"clean_lenient\", target_type=\"toxic_lenient\", token_position=-1):\n",
    "\n",
    "    input_tokens, resid_cache = set_up_patching_experiment(\n",
    "        model, clean_prompt, toxic_prompt, source_type=source_type, target_type=target_type\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for layer in tqdm(range(hooked_model.cfg.n_layers)):\n",
    "        hooked_model.reset_hooks()\n",
    "        temp_hook = partial(\n",
    "            patching_hook,\n",
    "            cache=resid_cache,\n",
    "            position=token_position,\n",
    "        )\n",
    "        hooked_model.blocks[layer].hook_resid_post.add_hook(temp_hook)\n",
    "        with torch.no_grad():\n",
    "            patched_logits = hooked_model(input_tokens).to(\"cpu\")\n",
    "            pre_soft_logit_diff_change = clean_toxic_logit_diff(patched_logits).item()\n",
    "\n",
    "        results.append(pre_soft_logit_diff_change)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patching from lenient to harsh at all positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_patch_results = []\n",
    "\n",
    "ambig_prompt = \"I want to sleep with you tonight\"\n",
    "\n",
    "example_string = lenient_persona+instruction_prompt.format(sequence=ambig_prompt)\n",
    "example_str_tokens = hooked_model.to_str_tokens(example_string)\n",
    "\n",
    "for token_position in tqdm(range(len(example_str_tokens))):\n",
    "    token_patch_results = get_input_patching_results_for_prompt_pair(\n",
    "        model=hooked_model, \n",
    "        clean_prompt=ambig_prompt, \n",
    "        source_type=\"clean_lenient\", \n",
    "        target_type=\"clean_harsh\", \n",
    "        token_position=token_position\n",
    "        )\n",
    "    all_patch_results.append(token_patch_results)\n",
    "\n",
    "# save the list of results as a jsonl file\n",
    "import json\n",
    "with open(\"patching_results.jsonl\", \"w\") as f:\n",
    "    for result in all_patch_results:\n",
    "        f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open(\"patching_results.jsonl\", \"r\") as f:\n",
    "    all_patch_results = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# Convert all_patch_results to a NumPy array for easier handling\n",
    "data = np.array(all_patch_results)\n",
    "\n",
    "# Creating the heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "plt.imshow(data, aspect='auto', cmap='viridis')  # Choose your colormap (e.g., 'viridis', 'plasma', 'inferno', 'magma')\n",
    "\n",
    "# Adding color bar on the side\n",
    "plt.colorbar(label='Intensity')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Heatmap of Patch Results')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Layer')\n",
    "\n",
    "# Setting the ticks if necessary\n",
    "token_positions = np.arange(data.shape[1])\n",
    "layer_numbers = np.arange(data.shape[0])\n",
    "plt.xticks(token_positions, labels=[str(pos) for pos in token_positions])  # Adjust labels as necessary\n",
    "plt.yticks(layer_numbers, labels=[f\"Layer {layer}\" for layer in layer_numbers])\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
