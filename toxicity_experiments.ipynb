{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/quirky_interp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pointbiserialr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_size = \"13b\"\n",
    "use_base_model = False\n",
    "\n",
    "dataset_path = \"datasets/toxic_prompts_gpt4_ambiguous.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = {\n",
    "    \"lenient\": \"You are lenient on toxicity\\n\",\n",
    "    \"harsh\": \"You are harsh on toxicity\\n\"\n",
    "}\n",
    "\n",
    "possible_labels = (\"CLEAN\", \"TOXIC\")\n",
    "\n",
    "classifier_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (TOXIC) or (CLEAN) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:37<00:00, 32.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-13b into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "if not use_base_model:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-chat-hf\"\n",
    "else:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-hf\"\n",
    "\n",
    "hf_model, hf_tokenizer = load_model_from_transformers(model_name_or_path)\n",
    "model = from_hf_to_tlens(hf_model, hf_tokenizer, f\"llama-{llama_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_task = Task(model, classifier_prompt, personas, possible_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find toxicity direction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the position of the newline after the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating activations: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating activations: 768it [03:32,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "names_filter = [f'blocks.{i}.hook_resid_post' for i in range(model.cfg.n_layers)]\n",
    "X_after_seq, _ = our_task.aggregate_activations(dataset_path, names_filter=names_filter, position=-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_consider = range(model.cfg.n_layers)  # list(range(8, 17))\n",
    "\n",
    "activations = {}\n",
    "\n",
    "for layer in layers_to_consider:\n",
    "    harsh_acts = X_after_seq[\"harsh\"][f'blocks.{layer}.hook_resid_post']\n",
    "    lenient_acts = X_after_seq[\"lenient\"][f'blocks.{layer}.hook_resid_post']\n",
    "    activations[layer] = np.concatenate([harsh_acts, lenient_acts], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = {\n",
    "    \"toxic\": \"People like that should go kill themselves.\",\n",
    "    \"ambiguous\": \"Real life is lame. We should have been ready for AI by now.\",\n",
    "    \"clean\": \"It was a pleasure speaking with you, have a pleasant day.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_component_to_look_at = 2\n",
    "\n",
    "example_to_cache = {}\n",
    "\n",
    "for persona_type in [\"harsh\", \"lenient\"]:\n",
    "    for toxicity, sequence in examples.items():\n",
    "        prompt = personas[persona_type] + classifier_prompt.format(sequence=sequence)\n",
    "        with torch.no_grad():\n",
    "            tokens = model.to_tokens(prompt)\n",
    "            _, activation_cache = model.run_with_cache(tokens, names_filter=[f'blocks.{l}.hook_resid_post' for l in layers_to_consider])\n",
    "\n",
    "        example_to_cache[f\"{persona_type}_{toxicity}\"] = activation_cache\n",
    "\n",
    "toxic_scores_by_layer = {}\n",
    "pcs_by_layer = {}\n",
    "\n",
    "for layer in layers_to_consider:\n",
    "    toxic_scores = {\n",
    "        \"harsh\": [],\n",
    "        \"lenient\": []\n",
    "    }\n",
    "    normalized_activations = np.array([a / np.linalg.norm(a) for a in activations[layer]])\n",
    "    pca = PCA(n_components=2)\n",
    "    pc = pca.fit_transform(normalized_activations.T)\n",
    "    pc = pc.T[pca_component_to_look_at - 1]\n",
    "    pc_norm = pc / np.linalg.norm(pc)\n",
    "    pcs_by_layer[layer] = pc_norm\n",
    "    for persona_type in [\"harsh\", \"lenient\"]:\n",
    "        for example_type in [\"toxic\", \"ambiguous\", \"clean\"]:\n",
    "            example_acts = example_to_cache[f\"{persona_type}_{example_type}\"][f'blocks.{layer}.hook_resid_post'][0, -7].cpu().to(torch.float32).numpy()\n",
    "            example_acts_norm = example_acts / np.linalg.norm(example_acts)\n",
    "            toxic_scores[persona_type].append(np.dot(example_acts_norm, pc_norm))\n",
    "    toxic_scores_by_layer[layer] = toxic_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {'harsh': [0.04867258, 0.0014502127, -0.096092924],\n",
       "  'lenient': [0.05055173, 0.00487889, -0.096601255]},\n",
       " 9: {'harsh': [0.034425832, -0.03035758, -0.09523992],\n",
       "  'lenient': [0.042001296, -0.019825576, -0.08792518]},\n",
       " 10: {'harsh': [0.021321766, -0.03798548, -0.15455362],\n",
       "  'lenient': [0.030524436, -0.022796392, -0.1438487]},\n",
       " 11: {'harsh': [0.025916161, -0.051978644, -0.17750248],\n",
       "  'lenient': [0.037718546, -0.032755807, -0.16650593]},\n",
       " 12: {'harsh': [0.038939983, -0.039118774, -0.18332738],\n",
       "  'lenient': [0.047226697, -0.02253327, -0.17466101]},\n",
       " 13: {'harsh': [0.039103605, -0.05197961, -0.18679889],\n",
       "  'lenient': [0.046992168, -0.03272708, -0.1713283]},\n",
       " 14: {'harsh': [0.05939184, -0.033866107, -0.19575176],\n",
       "  'lenient': [0.06387259, -0.015960958, -0.18318053]},\n",
       " 15: {'harsh': [0.06271503, -0.04255519, -0.16444147],\n",
       "  'lenient': [0.06381403, -0.025042705, -0.15752874]},\n",
       " 16: {'harsh': [0.037166566, -0.036263466, -0.1825463],\n",
       "  'lenient': [0.03280157, -0.020736966, -0.18108106]}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_scores_by_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores from the first principal component. Weird that they are all similar. Maybe it is related to something like sequence lenght:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {'harsh': [0.87444675, 0.92220044, 0.87545586],\n",
       "  'lenient': [0.87407917, 0.9223203, 0.8759332]},\n",
       " 9: {'harsh': [0.87013096, 0.92390907, 0.87426716],\n",
       "  'lenient': [0.86878693, 0.92451984, 0.87462443]},\n",
       " 10: {'harsh': [0.8427434, 0.91831493, 0.85509765],\n",
       "  'lenient': [0.84034145, 0.9174247, 0.85285497]},\n",
       " 11: {'harsh': [0.8399174, 0.91299, 0.8483696],\n",
       "  'lenient': [0.8388678, 0.91386634, 0.8440132]},\n",
       " 12: {'harsh': [0.8421282, 0.91638273, 0.8438234],\n",
       "  'lenient': [0.84152305, 0.91571337, 0.8379563]},\n",
       " 13: {'harsh': [0.8308469, 0.9070972, 0.83020043],\n",
       "  'lenient': [0.83123547, 0.90777445, 0.8284682]},\n",
       " 14: {'harsh': [0.82933086, 0.90693223, 0.82423896],\n",
       "  'lenient': [0.83017117, 0.9082519, 0.8225319]},\n",
       " 15: {'harsh': [0.8294454, 0.90496767, 0.82924414],\n",
       "  'lenient': [0.8305189, 0.907514, 0.8252735]},\n",
       " 16: {'harsh': [0.83403254, 0.901343, 0.8197786],\n",
       "  'lenient': [0.83574617, 0.9041556, 0.8130963]}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_scores_by_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get correlation between toxicity direction and persona prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_data = pd.read_json(\"data/new_toxic_prompts_labelled.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_sequences = toxicity_data[toxicity_data[\"label\"] == \"toxic\"][\"prompt\"].to_list()\n",
    "clean_sequences = toxicity_data[toxicity_data[\"label\"] == \"clean\"][\"prompt\"].to_list()\n",
    "ambiguous_sequences = toxicity_data[toxicity_data[\"label\"] == \"ambiguous\"][\"prompt\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_type_to_class = {\n",
    "    \"harsh\": 1,\n",
    "    \"lenient\": 0\n",
    "}\n",
    "\n",
    "def get_p_labels_and_t_scores(layer, sequences, persona_types=None, position=-7):\n",
    "    persona_types = persona_types or [\"harsh\", \"lenient\"]\n",
    "    p_labels = []\n",
    "    t_scores = []\n",
    "    pc = pcs_by_layer[layer]\n",
    "    for sequence in tqdm(sequences):\n",
    "        for persona_type in persona_types:\n",
    "            prompt = personas[persona_type] + classifier_prompt.format(sequence=sequence)\n",
    "            with torch.no_grad():\n",
    "                tokens = model.to_tokens(prompt)\n",
    "                _, activation_cache = model.run_with_cache(tokens, names_filter=[f'blocks.{layer}.hook_resid_post'])\n",
    "            example_acts = activation_cache[f'blocks.{layer}.hook_resid_post'][0, position].cpu().to(torch.float32).numpy()\n",
    "            example_acts_norm = example_acts / np.linalg.norm(example_acts)\n",
    "            t_scores.append(np.dot(example_acts_norm, pc))\n",
    "            p_labels.append(p_type_to_class[persona_type])\n",
    "    return p_labels, t_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_labels_toxic, t_scores_toxic = get_p_labels_and_t_scores(8, toxic_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.000346474464772999\n",
      "P-value: 0.9932880584215871\n"
     ]
    }
   ],
   "source": [
    "correlation, p_value = pointbiserialr(p_labels_toxic, t_scores_toxic)\n",
    "\n",
    "print(\"Correlation:\", correlation)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [09:51<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "p_labels, t_scores = get_p_labels_and_t_scores(8, toxicity_data[\"prompt\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between persona prompt and toxicity score: 0.0018615926003243514\n",
      "P-value: 0.9329016297232685\n"
     ]
    }
   ],
   "source": [
    "persona_correlation, persona_p_value = pointbiserialr(p_labels, t_scores)\n",
    "\n",
    "print(\"Correlation between persona prompt and toxicity score:\", persona_correlation)\n",
    "print(\"P-value:\", persona_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between persona prompt and toxicity score (for toxic seqs only): -0.000346474464772999\n",
      "P-value: 0.9932880584215871\n"
     ]
    }
   ],
   "source": [
    "toxic_idx_pairs = [[2*i,2*i + 1] for i in toxicity_data[toxicity_data[\"label\"] == \"toxic\"].index]\n",
    "toxic_idxs = []\n",
    "for pair in toxic_idx_pairs:\n",
    "    toxic_idxs.extend(pair)\n",
    "\n",
    "p_labels_toxic = np.array(p_labels)[toxic_idxs]\n",
    "t_scores_toxic = np.array(t_scores)[toxic_idxs]\n",
    "\n",
    "persona_correlation, persona_p_value = pointbiserialr(p_labels_toxic, t_scores_toxic)\n",
    "\n",
    "print(\"Correlation between persona prompt and toxicity score (for toxic seqs only):\", persona_correlation)\n",
    "print(\"P-value:\", persona_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between persona prompt and toxicity score (for clean seqs only): 0.0028666330457297663\n",
      "P-value: 0.9393924296776734\n"
     ]
    }
   ],
   "source": [
    "clean_idx_pairs = [[2*i,2*i + 1] for i in toxicity_data[toxicity_data[\"label\"] == \"clean\"].index]\n",
    "clean_idxs = []\n",
    "for pair in clean_idx_pairs:\n",
    "    clean_idxs.extend(pair)\n",
    "\n",
    "p_labels_clean = np.array(p_labels)[clean_idxs]\n",
    "t_scores_clean = np.array(t_scores)[clean_idxs]\n",
    "\n",
    "persona_correlation, persona_p_value = pointbiserialr(p_labels_clean, t_scores_clean)\n",
    "\n",
    "print(\"Correlation between persona prompt and toxicity score (for clean seqs only):\", persona_correlation)\n",
    "print(\"P-value:\", persona_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between persona prompt and toxicity score (for ambiguous seqs only): 0.0027241302920300186\n",
      "P-value: 0.9407085157020021\n"
     ]
    }
   ],
   "source": [
    "ambiguous_idx_pairs = [[2*i,2*i + 1] for i in toxicity_data[toxicity_data[\"label\"] == \"ambiguous\"].index]\n",
    "ambiguous_idxs = []\n",
    "for pair in ambiguous_idx_pairs:\n",
    "    ambiguous_idxs.extend(pair)\n",
    "\n",
    "p_labels_ambiguous = np.array(p_labels)[ambiguous_idxs]\n",
    "t_scores_ambiguous = np.array(t_scores)[ambiguous_idxs]\n",
    "\n",
    "persona_correlation, persona_p_value = pointbiserialr(p_labels_ambiguous, t_scores_ambiguous)\n",
    "\n",
    "print(\"Correlation between persona prompt and toxicity score (for ambiguous seqs only):\", persona_correlation)\n",
    "print(\"P-value:\", persona_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between toxicity score and classification: 0.116145597841347\n",
      "P-value: 2.7330745428205535e-05\n"
     ]
    }
   ],
   "source": [
    "toxic_or_clean_idx_pairs = [\n",
    "    [2*i,2*i + 1] for i in \n",
    "    list(toxicity_data[toxicity_data[\"label\"] == \"toxic\"].index) + list(toxicity_data[toxicity_data[\"label\"] == \"clean\"].index)\n",
    "]\n",
    "toxic_or_clean_idxs = []\n",
    "for pair in toxic_or_clean_idx_pairs:\n",
    "    toxic_or_clean_idxs.extend(pair)\n",
    "\n",
    "\n",
    "\n",
    "t_scores_toxic_or_clean = np.array(t_scores)[toxic_or_clean_idxs]\n",
    "\n",
    "class_correlation, class_p_value = pointbiserialr(([1] * (2*len(toxic_sequences)) + [0] * (2*len(clean_sequences))), t_scores_toxic_or_clean)\n",
    "\n",
    "print(\"Correlation between toxicity score and classification:\", class_correlation)\n",
    "print(\"P-value:\", class_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [09:58<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "p_labels_10, t_scores_10 = get_p_labels_and_t_scores(10, toxicity_data[\"prompt\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between persona prompt and toxicity score: -0.024261521051295094\n",
      "P-value: 0.2724480855655803\n"
     ]
    }
   ],
   "source": [
    "persona_correlation, persona_p_value = pointbiserialr(p_labels_10, t_scores_10)\n",
    "\n",
    "print(\"Correlation between persona prompt and toxicity score:\", persona_correlation)\n",
    "print(\"P-value:\", persona_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [09:56<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "p_labels_12, t_scores_12 = get_p_labels_and_t_scores(12, toxicity_data[\"prompt\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between persona prompt and toxicity score: -0.023798613080867934\n",
      "P-value: 0.28170494977903277\n"
     ]
    }
   ],
   "source": [
    "persona_correlation, persona_p_value = pointbiserialr(p_labels_12, t_scores_12)\n",
    "\n",
    "print(\"Correlation between persona prompt and toxicity score:\", persona_correlation)\n",
    "print(\"P-value:\", persona_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Patching Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_toxic_logit_diff(logits):\n",
    "    # clean - toxic\n",
    "    return logits[0, -1, 315] - logits[0, -1, 7495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_patching_hook(\n",
    "    activation,\n",
    "    hook,\n",
    "    cache,\n",
    "    position,\n",
    "    direction\n",
    "):\n",
    "    activation[:, position, :] -= torch.dot(direction, activation[:, position, :])\n",
    "    activation[:, position, :] += torch.dot(direction, cache[hook.name][:, position, :])\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_directional_patching(model, input_tokens, direction, resid_cache, layers, position=-7):\n",
    "    for layer in layers:\n",
    "        hook = partial(\n",
    "            directional_patching_hook,\n",
    "            cache=resid_cache,\n",
    "            position=position,\n",
    "            direction=direction\n",
    "        )\n",
    "        model.blocks[layer].hook_resid_post.add_hook(hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patched_logits = model(input_tokens).to(\"cpu\")\n",
    "        logit_diff = clean_toxic_logit_diff(patched_logits).item()\n",
    "\n",
    "    model.reset_hooks()\n",
    "    return logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sequence = \"You are so smart.\"\n",
    "toxic_sequence = \"You are so stupid.\"\n",
    "\n",
    "clean_lenient_prompt = personas[\"lenient\"] + classifier_prompt.format(sequence=clean_sequence)\n",
    "toxic_lenient_prompt = personas[\"lenient\"] + classifier_prompt.format(sequence=toxic_sequence)\n",
    "\n",
    "clean_lenient_tokens = model.to_tokens(clean_lenient_prompt)\n",
    "toxic_lenient_tokens = model.to_tokens(toxic_lenient_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "logit_diffs = []\n",
    "\n",
    "for layer in tqdm(layers_to_consider):\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        _, resid_cache = model.run_with_cache(toxic_lenient_tokens, names_filter=[f'blocks.{layer}.hook_resid_post'])\n",
    "    pc = pcs_by_layer[layer]\n",
    "    pc = torch.tensor(pc, device=\"cuda\", dtype=torch.bfloat16)\n",
    "    logit_diff = do_directional_patching(model, clean_lenient_tokens, pc, resid_cache, [layer])\n",
    "    logit_diffs.append(logit_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
