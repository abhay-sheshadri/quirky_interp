{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from src import *\n",
    "import json\n",
    "from src.patching_helpers import *\n",
    "from src.utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = {\n",
    "    \"lenient\": \"You are lenient on toxicity\\n\",\n",
    "    \"harsh\": \"You are harsh on toxicity\\n\"\n",
    "}\n",
    "\n",
    "possible_labels = (\"CLEAN\", \"TOXIC\")\n",
    "\n",
    "classifier_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0a829246934078b699f4cb4b3a9267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-13b into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name_or_path = f\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "hf_model, hf_tokenizer = load_model_from_transformers(model_name_or_path)\n",
    "model = from_hf_to_tlens(hf_model, hf_tokenizer, f\"llama-13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/simple_toxic_data_filtered.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_data = [d for d in data if d[\"label\"] in (\"clean\", \"toxic\")]\n",
    "# ambig_data = [d for d in data if d[\"label\"] == \"ambiguous\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For persona steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_str_list = [d[\"prompt\"] for d in polar_data if d[\"label\"] == \"clean\"]\n",
    "toxic_str_list = [d[\"prompt\"] for d in polar_data if d[\"label\"] == \"toxic\"]\n",
    "\n",
    "clean_lenient_seqs = [personas['lenient'] + classifier_prompt.format(sequence=d) + \"C\" for d in clean_str_list]\n",
    "toxic_lenient_seqs = [personas['lenient'] + classifier_prompt.format(sequence=d) + \"TO\" for d in toxic_str_list]\n",
    "\n",
    "clean_lenient_tokens, _ = tokenize_examples(clean_lenient_seqs, model)\n",
    "toxic_lenient_tokens, _ = tokenize_examples(toxic_lenient_seqs, model)\n",
    "\n",
    "clean_harsh_seqs = [personas['harsh'] + classifier_prompt.format(sequence=d) + \"C\" for d in clean_str_list]\n",
    "toxic_harsh_seqs = [personas['harsh'] + classifier_prompt.format(sequence=d) + \"TO\" for d in toxic_str_list]\n",
    "\n",
    "clean_harsh_tokens, _ = tokenize_examples(clean_harsh_seqs, model)\n",
    "toxic_harsh_tokens, _ = tokenize_examples(toxic_harsh_seqs, model)\n",
    "\n",
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 1/4 [01:59<05:57, 119.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m     steering_vectors \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_processed_pairs\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m steering_vectors\n\u001b[0;32m---> 51\u001b[0m steering_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data_and_compute_steering_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 42\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors\u001b[0;34m(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     39\u001b[0m             update_steering_vectors(model, steering_vectors, clean_cache, toxic_cache, LAST_X_TOKEN_POSITIONS, modifier)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Process and update steering vectors for each dataset in batches\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mprocess_and_update_sequences_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m process_and_update_sequences_in_batches(model, clean_harsh_seqs, toxic_harsh_seqs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Final adjustment to the steering vectors\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 35\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors.<locals>.process_and_update_sequences_in_batches\u001b[0;34m(model, clean_seqs, toxic_seqs, modifier)\u001b[0m\n\u001b[1;32m     33\u001b[0m padded_clean_batch, _ \u001b[38;5;241m=\u001b[39m tokenize_examples(clean_batch, model, left_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m padded_toxic_batch, _ \u001b[38;5;241m=\u001b[39m tokenize_examples(toxic_batch, model, left_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m clean_batch_cache \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_clean_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m toxic_batch_cache \u001b[38;5;241m=\u001b[39m process_batch(model, padded_toxic_batch)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Update steering vectors for each pair in the batch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_cache \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m----> 8\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_resid_cache_from_forward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     batch_cache\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_cache\n",
      "File \u001b[0;32m/workspace/quirky_interp/src/patching_helpers.py:95\u001b[0m, in \u001b[0;36mget_resid_cache_from_forward_pass\u001b[0;34m(model, tokens, layers)\u001b[0m\n\u001b[1;32m     92\u001b[0m     names_filter\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hook_resid_post\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 95\u001b[0m     logits, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:641\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    626\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    634\u001b[0m ]:\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    645\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    646\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    647\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py:467\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    458\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    462\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    463\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    464\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    465\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[0;32m--> 467\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    469\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:534\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    527\u001b[0m ):\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         (\n\u001b[1;32m    530\u001b[0m             residual,\n\u001b[1;32m    531\u001b[0m             tokens,\n\u001b[1;32m    532\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    533\u001b[0m             attention_mask,\n\u001b[0;32m--> 534\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:277\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, past_kv_cache)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m    275\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# If the padding side is left or we are using caching, we need to compute the attention mask\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# for the adjustment of absolute positional embeddings and attention masking so that pad\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# tokens are not attended.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prepend_bos \u001b[38;5;129;01mis\u001b[39;00m USE_DEFAULT_VALUE:\n\u001b[1;32m    285\u001b[0m         prepend_bos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdefault_prepend_bos\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:431\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    Size of the full vocabulary with the added tokens. Counts the `keys` and not the `values` because otherwise if\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    there is a hole in the vocab, we will add tokenizers at a wrong index.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py:241\u001b[0m, in \u001b[0;36mLlamaTokenizer.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns vocab as a dict\"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(i): i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)}\n\u001b[1;32m    242\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py:241\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns vocab as a dict\"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m: i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)}\n\u001b[1;32m    242\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:973\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder[ids]\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py:290\u001b[0m, in \u001b[0;36mLlamaTokenizer._convert_id_to_token\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_id_to_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdToPiece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py:1179\u001b[0m, in \u001b[0;36m_batchnize.<locals>._batched_func\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1177\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [_func(\u001b[38;5;28mself\u001b[39m, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m arg]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1179\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py:1173\u001b[0m, in \u001b[0;36m_batchnize.<locals>._func\u001b[0;34m(v, n)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(n) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mpiece_size()):\n\u001b[1;32m   1172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiece id is out of range.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py:283\u001b[0m, in \u001b[0;36mSentencePieceProcessor.IdToPiece\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mIdToPiece\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m):\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor_IdToPiece\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PAD_TOKEN_ID = 0 if model.tokenizer.pad_token_id is None else model.tokenizer.pad_token_id\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def process_batch(model, sequences):\n",
    "    \"\"\"Processes a batch of sequences and returns the cache.\"\"\"\n",
    "    batch_cache = []\n",
    "    for seq in sequences:\n",
    "        _, cache = get_resid_cache_from_forward_pass(model, seq)\n",
    "        batch_cache.append(cache)\n",
    "    return batch_cache\n",
    "\n",
    "def update_steering_vectors(model, steering_vectors, clean_cache, toxic_cache, LAST_X_TOKEN_POSITIONS, modifier):\n",
    "    \"\"\"Updates the steering vectors with given clean and toxic caches.\"\"\"\n",
    "\n",
    "    # layers are the same for both clean_cache and toxic_cache\n",
    "    for idx, layer in enumerate(clean_cache):\n",
    "        clean_layer_cache = clean_cache[layer][:, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "        toxic_layer_cache = toxic_cache[layer][:, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "        # sum from (batch_size, last_x_token_positions, model_dim) to (last_x_token_positions, model_dim)\n",
    "        steering_vectors[idx] += modifier * (clean_layer_cache.sum(0) + toxic_layer_cache.sum(0))\n",
    "\n",
    "def process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, BATCH_SIZE):\n",
    "    LAST_X_TOKEN_POSITIONS = 10\n",
    "    steering_vectors = torch.zeros((model.cfg.n_layers, LAST_X_TOKEN_POSITIONS, model.cfg.d_model)).cuda()\n",
    "    total_pairs = min(len(clean_lenient_seqs), len(toxic_lenient_seqs))\n",
    "\n",
    "    # Adjusted function to process sequences in batches and update steering vectors\n",
    "    def process_and_update_sequences_in_batches(model, clean_seqs, toxic_seqs, modifier):\n",
    "        # for i in range(0, min(len(clean_seqs), len(toxic_seqs)), BATCH_SIZE):\n",
    "        for i in tqdm(range(0, min(len(clean_seqs), len(toxic_seqs)), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "            clean_batch = clean_seqs[i:i+BATCH_SIZE]\n",
    "            toxic_batch = toxic_seqs[i:i+BATCH_SIZE]\n",
    "            padded_clean_batch, _ = tokenize_examples(clean_batch, model, left_pad=True)\n",
    "            padded_toxic_batch, _ = tokenize_examples(toxic_batch, model, left_pad=True)\n",
    "            clean_batch_cache = process_batch(model, padded_clean_batch)\n",
    "            toxic_batch_cache = process_batch(model, padded_toxic_batch)\n",
    "            # Update steering vectors for each pair in the batch\n",
    "            for clean_cache, toxic_cache in zip(clean_batch_cache, toxic_batch_cache):\n",
    "                update_steering_vectors(model, steering_vectors, clean_cache, toxic_cache, LAST_X_TOKEN_POSITIONS, modifier)\n",
    "    \n",
    "    # Process and update steering vectors for each dataset in batches\n",
    "    process_and_update_sequences_in_batches(model, clean_lenient_seqs, toxic_lenient_seqs, 1)\n",
    "    process_and_update_sequences_in_batches(model, clean_harsh_seqs, toxic_harsh_seqs, -1)\n",
    "\n",
    "    # Final adjustment to the steering vectors\n",
    "    num_processed_pairs = 2 * total_pairs * BATCH_SIZE  # Adjust based on actual processed pairs\n",
    "    steering_vectors /= num_processed_pairs\n",
    "    \n",
    "    return steering_vectors\n",
    "\n",
    "steering_vectors = process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m     steering_vectors \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m total_pairs \u001b[38;5;241m*\u001b[39m BATCH_SIZE)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m steering_vectors\n\u001b[0;32m---> 52\u001b[0m steering_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data_and_compute_steering_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors\u001b[0;34m(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache_accumulator\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Process each dataset in batches\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m clean_lenient_cache \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sequences_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_lenient_seqs)\n\u001b[1;32m     30\u001b[0m toxic_lenient_cache \u001b[38;5;241m=\u001b[39m process_sequences_in_batches(model, toxic_lenient_seqs)\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors.<locals>.process_sequences_in_batches\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m     21\u001b[0m     batch \u001b[38;5;241m=\u001b[39m sequences[i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[1;32m     22\u001b[0m     padded_batch \u001b[38;5;241m=\u001b[39m tokenize_examples(batch, model, left_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m     batch_cache \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     cache_accumulator\u001b[38;5;241m.\u001b[39mextend(batch_cache)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache_accumulator\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_cache \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m----> 8\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_resid_cache_from_forward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     batch_cache\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_cache\n",
      "File \u001b[0;32m/workspace/quirky_interp/src/patching_helpers.py:95\u001b[0m, in \u001b[0;36mget_resid_cache_from_forward_pass\u001b[0;34m(model, tokens, layers)\u001b[0m\n\u001b[1;32m     92\u001b[0m     names_filter\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hook_resid_post\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 95\u001b[0m     logits, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:641\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    626\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    634\u001b[0m ]:\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    645\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    646\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    647\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py:467\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    458\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    462\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    463\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    464\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    465\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[0;32m--> 467\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    469\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:556\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    551\u001b[0m blocks_and_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_layers), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks))\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m blocks_and_idxs[start_at_layer:stop_at_layer]:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;66;03m# Note that each block includes skip connections, so we don't need\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# residual + block(residual)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# If we're using multiple GPUs, we need to send the residual and shortformer_pos_embed to the correct GPU\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_for_block_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    559\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    560\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/utilities/devices.py:40\u001b[0m, in \u001b[0;36mget_device_for_block_index\u001b[0;34m(index, cfg, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     device \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 40\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m device_index \u001b[38;5;241m=\u001b[39m (device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m (index \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m layers_per_device)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(device\u001b[38;5;241m.\u001b[39mtype, device_index)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PAD_TOKEN_ID = 0 if model.tokenizer.pad_token_id is None else model.tokenizer.pad_token_id\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "def process_batch(model, sequences):\n",
    "    \"\"\"Processes a batch of sequences and returns the cache.\"\"\"\n",
    "    batch_cache = []\n",
    "    for seq in sequences:\n",
    "        _, cache = get_resid_cache_from_forward_pass(model, seq)\n",
    "        batch_cache.append(cache)\n",
    "    return batch_cache\n",
    "\n",
    "def process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, BATCH_SIZE):\n",
    "    LAST_X_TOKEN_POSITIONS = 10\n",
    "    steering_vectors = torch.zeros((model.cfg.n_layers, LAST_X_TOKEN_POSITIONS, model.cfg.d_model))\n",
    "    total_pairs = min(len(clean_lenient_seqs), len(toxic_lenient_seqs))  # Assuming total_lenient == total_harsh\n",
    "    \n",
    "    # Function to process sequences in batches\n",
    "    def process_sequences_in_batches(model, sequences):\n",
    "        cache_accumulator = []\n",
    "        for i in range(0, len(sequences), BATCH_SIZE):\n",
    "            batch = sequences[i:i+BATCH_SIZE]\n",
    "            padded_batch = tokenize_examples(batch, model, left_pad=True)\n",
    "            batch_cache = process_batch(model, padded_batch)\n",
    "            cache_accumulator.extend(batch_cache)\n",
    "        return cache_accumulator\n",
    "    \n",
    "    # Process each dataset in batches\n",
    "    clean_lenient_cache = process_sequences_in_batches(model, clean_lenient_seqs)\n",
    "    print(clean_lenient_seqs)\n",
    "    toxic_lenient_cache = process_sequences_in_batches(model, toxic_lenient_seqs)\n",
    "    clean_harsh_cache = process_sequences_in_batches(model, clean_harsh_seqs)\n",
    "    toxic_harsh_cache = process_sequences_in_batches(model, toxic_harsh_seqs)\n",
    "    \n",
    "    # Assuming an updated version of get_resid_cache_from_forward_pass that supports batched inputs\n",
    "    for idx in tqdm(range(total_pairs)):\n",
    "        print(idx, flush=True)\n",
    "        \n",
    "        for persona, (clean_cache, toxic_cache) in zip(['lenient', 'harsh'], [(clean_lenient_cache[idx], toxic_lenient_cache[idx]), (clean_harsh_cache[idx], toxic_harsh_cache[idx])]):\n",
    "            modifier = 1 if persona == 'lenient' else -1\n",
    "            for layer in range(model.cfg.n_layers):\n",
    "                clean_layer_cache = clean_cache[layer].cpu().detach()[:, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                toxic_layer_cache = toxic_cache[layer].cpu().detach()[:, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                steering_vectors[layer] += modifier * (clean_layer_cache.sum(0) + toxic_layer_cache.sum(0))\n",
    "        \n",
    "        if idx > 1:\n",
    "            break\n",
    "    \n",
    "    steering_vectors /= (2 * total_pairs * BATCH_SIZE)\n",
    "    \n",
    "    return steering_vectors\n",
    "\n",
    "steering_vectors = process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m steering_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data_and_compute_steering_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors\u001b[0;34m(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache_accumulator\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Process each dataset in batches\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m clean_lenient_cache \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sequences_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_lenient_seqs)\n\u001b[1;32m     30\u001b[0m toxic_lenient_cache \u001b[38;5;241m=\u001b[39m process_sequences_in_batches(model, toxic_lenient_seqs)\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors.<locals>.process_sequences_in_batches\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m     21\u001b[0m     batch \u001b[38;5;241m=\u001b[39m sequences[i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[1;32m     22\u001b[0m     padded_batch \u001b[38;5;241m=\u001b[39m tokenize_examples(batch, model, left_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m     batch_cache \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     cache_accumulator\u001b[38;5;241m.\u001b[39mextend(batch_cache)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache_accumulator\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_cache \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m----> 8\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_resid_cache_from_forward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     batch_cache\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_cache\n",
      "File \u001b[0;32m/workspace/quirky_interp/src/patching_helpers.py:95\u001b[0m, in \u001b[0;36mget_resid_cache_from_forward_pass\u001b[0;34m(model, tokens, layers)\u001b[0m\n\u001b[1;32m     92\u001b[0m     names_filter\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hook_resid_post\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 95\u001b[0m     logits, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:641\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    626\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    634\u001b[0m ]:\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    645\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    646\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    647\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py:467\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    458\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    462\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    463\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    464\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    465\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[0;32m--> 467\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    469\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:534\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    527\u001b[0m ):\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         (\n\u001b[1;32m    530\u001b[0m             residual,\n\u001b[1;32m    531\u001b[0m             tokens,\n\u001b[1;32m    532\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    533\u001b[0m             attention_mask,\n\u001b[0;32m--> 534\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:277\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, past_kv_cache)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m    275\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# If the padding side is left or we are using caching, we need to compute the attention mask\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# for the adjustment of absolute positional embeddings and attention masking so that pad\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# tokens are not attended.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prepend_bos \u001b[38;5;129;01mis\u001b[39;00m USE_DEFAULT_VALUE:\n\u001b[1;32m    285\u001b[0m         prepend_bos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdefault_prepend_bos\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:431\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    Size of the full vocabulary with the added tokens. Counts the `keys` and not the `values` because otherwise if\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    there is a hole in the vocab, we will add tokenizers at a wrong index.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py:241\u001b[0m, in \u001b[0;36mLlamaTokenizer.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns vocab as a dict\"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(i): i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)}\n\u001b[1;32m    242\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py:241\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns vocab as a dict\"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m: i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)}\n\u001b[1;32m    242\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:973\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder[ids]\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama.py:290\u001b[0m, in \u001b[0;36mLlamaTokenizer._convert_id_to_token\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_id_to_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdToPiece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py:1175\u001b[0m, in \u001b[0;36m_batchnize.<locals>._batched_func\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiece id is out of range.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1173\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(v, n)\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batched_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[1;32m   1176\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(arg) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_func(\u001b[38;5;28mself\u001b[39m, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m arg]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/506 [01:12<10:07:40, 72.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     steering_vectors \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m total_pairs)  \u001b[38;5;66;03m# Normalize by the total number of pairs processed, accounting for both lenient and harsh\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m steering_vectors\n\u001b[0;32m---> 26\u001b[0m steering_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data_and_compute_steering_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_harsh_seqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors\u001b[0;34m(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs)\u001b[0m\n\u001b[1;32m     17\u001b[0m             toxic_layer_cache \u001b[38;5;241m=\u001b[39m toxic_cache[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hook_resid_post\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39mLAST_X_TOKEN_POSITIONS:, :]\n\u001b[1;32m     19\u001b[0m             modifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m persona \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlenient\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m             steering_vectors[layer] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m modifier \u001b[38;5;241m*\u001b[39m (clean_layer_cache \u001b[38;5;241m+\u001b[39m toxic_layer_cache)\n\u001b[1;32m     22\u001b[0m steering_vectors \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m total_pairs)  \u001b[38;5;66;03m# Normalize by the total number of pairs processed, accounting for both lenient and harsh\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m steering_vectors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs):\n",
    "    LAST_X_TOKEN_POSITIONS = 10\n",
    "    steering_vectors = torch.zeros((model.cfg.n_layers, LAST_X_TOKEN_POSITIONS, model.cfg.d_model))\n",
    "    total_pairs = min(len(clean_lenient_seqs), len(toxic_lenient_seqs)) # Assuming total_lenient == total_harsh\n",
    "\n",
    "    lenient_pairs = zip(clean_lenient_seqs, toxic_lenient_seqs)\n",
    "    harsh_pairs = zip(clean_harsh_seqs, toxic_harsh_seqs)\n",
    "\n",
    "    # Combined loop to process both lenient and harsh data\n",
    "    for idx, ((clean_lenient_seq, toxic_lenient_seq), (clean_harsh_seq, toxic_harsh_seq)) in enumerate(tqdm(zip(lenient_pairs, harsh_pairs), total=total_pairs)):\n",
    "        for persona, (clean_seq, toxic_seq) in zip(['lenient', 'harsh'], [(clean_lenient_seq, toxic_lenient_seq), (clean_harsh_seq, toxic_harsh_seq)]):\n",
    "            _, clean_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(clean_seq))\n",
    "            _, toxic_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(toxic_seq))\n",
    "\n",
    "            for layer in range(model.cfg.n_layers):\n",
    "                clean_layer_cache = clean_cache[f\"blocks.{layer}.hook_resid_post\"].cpu().detach()[0, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                toxic_layer_cache = toxic_cache[f\"blocks.{layer}.hook_resid_post\"].cpu().detach()[0, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                \n",
    "                modifier = 1 if persona == 'lenient' else -1\n",
    "                steering_vectors[layer] += modifier * (clean_layer_cache + toxic_layer_cache)\n",
    "\n",
    "    steering_vectors /= (2 * total_pairs)  # Normalize by the total number of pairs processed, accounting for both lenient and harsh\n",
    "\n",
    "    return steering_vectors\n",
    "\n",
    "steering_vectors = process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 10.12 MiB is free. Process 3347595 has 79.13 GiB memory in use. Of the allocated memory 68.29 GiB is allocated by PyTorch, and 10.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m steering_vectors\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Note: This pseudocode assumes the model and associated functions can handle batched inputs.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# You may need to adjust the tokenization, padding, and model processing details to fit your actual model and data structure.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m steering_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data_and_compute_steering_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_lenient_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_harsh_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoxic_harsh_seqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors\u001b[0;34m(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache_accumulator\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Process each dataset in batches\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m clean_lenient_cache \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sequences_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_lenient_seqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m toxic_lenient_cache \u001b[38;5;241m=\u001b[39m process_sequences_in_batches(model, toxic_lenient_seqs)\n\u001b[1;32m     30\u001b[0m clean_harsh_cache \u001b[38;5;241m=\u001b[39m process_sequences_in_batches(model, clean_harsh_seqs)\n",
      "Cell \u001b[0;32mIn[31], line 23\u001b[0m, in \u001b[0;36mprocess_data_and_compute_steering_vectors.<locals>.process_sequences_in_batches\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m     21\u001b[0m     batch \u001b[38;5;241m=\u001b[39m sequences[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     22\u001b[0m     padded_batch \u001b[38;5;241m=\u001b[39m tokenize_examples(batch, model, left_pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m     batch_cache \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     cache_accumulator\u001b[38;5;241m.\u001b[39mextend(batch_cache)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache_accumulator\n",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(model, sequences)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_cache \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m----> 8\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_resid_cache_from_forward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     batch_cache\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_cache\n",
      "File \u001b[0;32m/workspace/quirky_interp/src/patching_helpers.py:95\u001b[0m, in \u001b[0;36mget_resid_cache_from_forward_pass\u001b[0;34m(model, tokens, layers)\u001b[0m\n\u001b[1;32m     92\u001b[0m     names_filter\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hook_resid_post\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 95\u001b[0m     logits, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:641\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    626\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    634\u001b[0m ]:\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    645\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    646\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    647\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py:467\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    458\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    462\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    463\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    464\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    465\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[0;32m--> 467\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    469\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:562\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    559\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    560\u001b[0m         )\n\u001b[0;32m--> 562\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py:1444\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1437\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m   1438\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m   1440\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m-> 1444\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1453\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1455\u001b[0m     resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(\n\u001b[1;32m   1456\u001b[0m         resid_pre \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m   1457\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py:556\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask)\u001b[0m\n\u001b[1;32m    553\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    554\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 556\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_attention_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malibi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    561\u001b[0m     query_ctx \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py:681\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_attention_scores\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_attention_scores\u001b[39m(\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    677\u001b[0m     q: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch query_pos head_index d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    678\u001b[0m     k: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch key_pos head_index d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    679\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch head_index query_pos key_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    680\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 681\u001b[0m         \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch query_pos head_index d_head, \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;43m                batch key_pos head_index d_head \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;43m                -> batch head_index query_pos key_pos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m         \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_scale\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_scores\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[38;5;241m=\u001b[39m get_backend(operands[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[38;5;241m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_equation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, equation, \u001b[38;5;241m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 10.12 MiB is free. Process 3347595 has 79.13 GiB memory in use. Of the allocated memory 68.29 GiB is allocated by PyTorch, and 10.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN_ID = 0 if model.tokenizer.pad_token_id is None else model.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "def process_batch(model, sequences):\n",
    "    \"\"\"Processes a batch of sequences and returns the cache.\"\"\"\n",
    "    batch_cache = []\n",
    "    for seq in sequences:\n",
    "        _, cache = get_resid_cache_from_forward_pass(model, seq)\n",
    "        batch_cache.append(cache)\n",
    "    return batch_cache\n",
    "\n",
    "def process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs, batch_size=32):\n",
    "    LAST_X_TOKEN_POSITIONS = 10\n",
    "    steering_vectors = torch.zeros((model.cfg.n_layers, LAST_X_TOKEN_POSITIONS, model.cfg.d_model))\n",
    "    total_pairs = min(len(clean_lenient_seqs), len(toxic_lenient_seqs))  # Assuming total_lenient == total_harsh\n",
    "    \n",
    "    # Function to process sequences in batches\n",
    "    def process_sequences_in_batches(model, sequences):\n",
    "        cache_accumulator = []\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = sequences[i:i+batch_size]\n",
    "            padded_batch = tokenize_examples(batch, model, left_pad=True)\n",
    "            batch_cache = process_batch(model, padded_batch)\n",
    "            cache_accumulator.extend(batch_cache)\n",
    "        return cache_accumulator\n",
    "    \n",
    "    # Process each dataset in batches\n",
    "    clean_lenient_cache = process_sequences_in_batches(model, clean_lenient_seqs)\n",
    "    toxic_lenient_cache = process_sequences_in_batches(model, toxic_lenient_seqs)\n",
    "    clean_harsh_cache = process_sequences_in_batches(model, clean_harsh_seqs)\n",
    "    toxic_harsh_cache = process_sequences_in_batches(model, toxic_harsh_seqs)\n",
    "    \n",
    "    # Assuming an updated version of get_resid_cache_from_forward_pass that supports batched inputs\n",
    "    for idx in tqdm(range(total_pairs)):\n",
    "        for persona, (clean_cache, toxic_cache) in zip(['lenient', 'harsh'], [(clean_lenient_cache[idx], toxic_lenient_cache[idx]), (clean_harsh_cache[idx], toxic_harsh_cache[idx])]):\n",
    "            modifier = 1 if persona == 'lenient' else -1\n",
    "            for layer in range(model.cfg.n_layers):\n",
    "                clean_layer_cache = clean_cache[layer].cpu().detach()[:, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                toxic_layer_cache = toxic_cache[layer].cpu().detach()[:, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                steering_vectors[layer] += modifier * (clean_layer_cache.sum(0) + toxic_layer_cache.sum(0))\n",
    "    \n",
    "    steering_vectors /= (2 * total_pairs * batch_size)\n",
    "    \n",
    "    return steering_vectors\n",
    "\n",
    "# Note: This pseudocode assumes the model and associated functions can handle batched inputs.\n",
    "# You may need to adjust the tokenization, padding, and model processing details to fit your actual model and data structure.\n",
    "steering_vectors = process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/506 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [02:47<00:00,  3.01it/s]\n",
      "100%|██████████| 506/506 [02:46<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "lenient_cache_cache = {}\n",
    "lenient_logits_cache = {}\n",
    "\n",
    "harsh_cache_cache = {}\n",
    "harsh_logits_cache = {}\n",
    "\n",
    "# for idx, datapoint in tqdm(enumerate(polar_data), total=len(polar_data)):\n",
    "for idx, datapoint_pair in tqdm(enumerate(zip(clean_lenient_seqs, toxic_lenient_seqs)), total=min(len(clean_lenient_seqs), len(toxic_lenient_seqs))):\n",
    "\n",
    "    _, clean_lenient_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[0]))\n",
    "    clean_lenient_cache = {k: v.cpu().detach() for k, v in clean_lenient_cache.items()}\n",
    "\n",
    "    _, toxic_lenient_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[1]))\n",
    "    toxic_lenient_cache = {k: v.cpu().detach() for k, v in toxic_lenient_cache.items()}\n",
    "\n",
    "    lenient_cache_cache[idx] = {\"clean\": clean_lenient_cache, \"toxic\": toxic_lenient_cache, \"clean_seq\": datapoint_pair[0], \"toxic_seq\": datapoint_pair[1]}\n",
    "\n",
    "    torch.save(lenient_cache_cache, \"lenient_cache_cache.pt\")\n",
    "\n",
    "    # break\n",
    "\n",
    "for idx, datapoint_pair in tqdm(enumerate(zip(clean_harsh_seqs, toxic_harsh_seqs)), total=min(len(clean_harsh_seqs), len(toxic_harsh_seqs))):\n",
    "\n",
    "    _, clean_harsh_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[0]))\n",
    "    clean_harsh_cache = {k: v.cpu().detach() for k, v in clean_harsh_cache.items()}\n",
    "\n",
    "    _, toxic_harsh_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[1]))\n",
    "    toxic_harsh_cache = {k: v.cpu().detach() for k, v in toxic_harsh_cache.items()}\n",
    "\n",
    "    harsh_cache_cache[idx] = {\"clean\": clean_harsh_cache, \"toxic\": toxic_harsh_cache, \"clean_seq\": datapoint_pair[0], \"toxic_seq\": datapoint_pair[1]}\n",
    "\n",
    "    torch.save(harsh_cache_cache, \"harsh_cache_cache.pt\")\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lenient_cache_cache, \"lenient_cache_cache.pt\")\n",
    "torch.save(harsh_cache_cache, \"harsh_cache_cache.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# layers, last 10 token postions, hidden_dim\n",
    "LAST_X_TOKEN_POSITIONS = 10\n",
    "steering_vectors = torch.zeros((model.cfg.n_layers, LAST_X_TOKEN_POSITIONS, model.cfg.d_model))\n",
    "train_size = len(lenient_cache_cache)\n",
    "\n",
    "# add the activations for both clean and toxic appended classifications for lenient persona\n",
    "for key, val in lenient_cache_cache.items():\n",
    "    print(key)\n",
    "    if key > train_size:\n",
    "        break\n",
    "    clean_cache = val[\"clean\"]\n",
    "    toxic_cache = val[\"toxic\"]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        clean_layer_cache = clean_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        toxic_layer_cache = toxic_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        # batch, tokens, hidden_dim\n",
    "        steering_vectors[layer] += clean_layer_cache[0, -LAST_X_TOKEN_POSITIONS:, :] + toxic_layer_cache[0, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "\n",
    "    if key > 5:\n",
    "        break\n",
    "# subtract the activations for both clean and toxic appended classifications for harsh persona\n",
    "for key, val in harsh_cache_cache.items():\n",
    "    print(key)\n",
    "    if key > train_size:\n",
    "        break\n",
    "    clean_cache = val[\"clean\"]\n",
    "    toxic_cache = val[\"toxic\"]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        clean_layer_cache = clean_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        toxic_layer_cache = toxic_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        # batch, tokens, hidden_dim\n",
    "        steering_vectors[layer] += -clean_layer_cache[0, -LAST_X_TOKEN_POSITIONS:, :] - toxic_layer_cache[0, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "\n",
    "    if key > 5:\n",
    "        break\n",
    "\n",
    "steering_vectors /= train_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs):\n",
    "    LAST_X_TOKEN_POSITIONS = 10\n",
    "    steering_vectors = torch.zeros((model.cfg.n_layers, LAST_X_TOKEN_POSITIONS, model.cfg.d_model))\n",
    "    total_pairs = min(len(clean_lenient_seqs), len(toxic_lenient_seqs)) # Assuming total_lenient == total_harsh\n",
    "\n",
    "    lenient_pairs = zip(clean_lenient_seqs, toxic_lenient_seqs)\n",
    "    harsh_pairs = zip(clean_harsh_seqs, toxic_harsh_seqs)\n",
    "\n",
    "    # Combined loop to process both lenient and harsh data\n",
    "    for idx, ((clean_lenient_seq, toxic_lenient_seq), (clean_harsh_seq, toxic_harsh_seq)) in enumerate(tqdm(zip(lenient_pairs, harsh_pairs), total=total_pairs)):\n",
    "        for persona, (clean_seq, toxic_seq) in zip(['lenient', 'harsh'], [(clean_lenient_seq, toxic_lenient_seq), (clean_harsh_seq, toxic_harsh_seq)]):\n",
    "            _, clean_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(clean_seq))\n",
    "            _, toxic_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(toxic_seq))\n",
    "\n",
    "            for layer in range(model.cfg.n_layers):\n",
    "                clean_layer_cache = clean_cache[f\"blocks.{layer}.hook_resid_post\"].cpu().detach()[0, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                toxic_layer_cache = toxic_cache[f\"blocks.{layer}.hook_resid_post\"].cpu().detach()[0, -LAST_X_TOKEN_POSITIONS:, :]\n",
    "                \n",
    "                modifier = 1 if persona == 'lenient' else -1\n",
    "                steering_vectors[layer] += modifier * (clean_layer_cache + toxic_layer_cache)\n",
    "\n",
    "    steering_vectors /= (2 * total_pairs)  # Normalize by the total number of pairs processed, accounting for both lenient and harsh\n",
    "\n",
    "    return steering_vectors\n",
    "\n",
    "# Example usage:\n",
    "# steering_vectors = process_data_and_compute_steering_vectors(model, clean_lenient_seqs, toxic_lenient_seqs, clean_harsh_seqs, toxic_harsh_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 10, 5120])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 10, 5120])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vectors.numel()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8464/1545442983.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  sys.getsizeof(steering_vectors.storage())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8192048"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(steering_vectors.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n",
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are lenient on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "The movie sucks because it is stupid\n",
      "\n",
      "Your response:\n",
      "(TO\n",
      "tensor([ 2933, 29901,    13, 29898,  4986], device='cuda:0')\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "You are lenient on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "The movie sucks because it is stupid\n",
      "\n",
      "Your response:\n",
      "(TOL\n",
      "tensor([29901,    13, 29898,  4986, 29931], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "seq = toxic_lenient_seqs[0]\n",
    "print(seq)\n",
    "print(model.to_tokens(seq)[0][-5:])\n",
    "seq2 = toxic_lenient_seqs[0] + \"L\"\n",
    "print(\"\\n\\n----\\n\\n\")\n",
    "print(seq2)\n",
    "print(model.to_tokens(seq2)[0][-5:])\n",
    "# print(model.to_tokens(clean_lenient_seqs[0] + \"L\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For sequence steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_str_list = [d[\"prompt\"] for d in polar_data if d[\"label\"] == \"clean\"]\n",
    "toxic_str_list = [d[\"prompt\"] for d in polar_data if d[\"label\"] == \"toxic\"]\n",
    "\n",
    "clean_lenient_seqs = [personas['lenient'] + classifier_prompt.format(sequence=d) for d in clean_str_list]\n",
    "toxic_lenient_seqs = [personas['lenient'] + classifier_prompt.format(sequence=d) for d in toxic_str_list]\n",
    "\n",
    "clean_lenient_tokens, clean_lenient_last = tokenize_examples(clean_lenient_seqs, model)\n",
    "toxic_lenient_tokens, toxic_lenient_last = tokenize_examples(toxic_lenient_seqs, model)\n",
    "\n",
    "clean_harsh_seqs = [personas['harsh'] + classifier_prompt.format(sequence=d) for d in clean_str_list]\n",
    "toxic_harsh_seqs = [personas['harsh'] + classifier_prompt.format(sequence=d) for d in toxic_str_list]\n",
    "\n",
    "clean_harsh_tokens, clean_harsh_last = tokenize_examples(clean_harsh_seqs, model)\n",
    "toxic_harsh_tokens, toxic_harsh_last = tokenize_examples(toxic_harsh_seqs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/506 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/506 [00:00<?, ?it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lenient_cache_cache = {}\n",
    "lenient_logits_cache = {}\n",
    "\n",
    "harsh_cache_cache = {}\n",
    "harsh_logits_cache = {}\n",
    "\n",
    "# for idx, datapoint in tqdm(enumerate(polar_data), total=len(polar_data)):\n",
    "for idx, datapoint_pair in tqdm(enumerate(zip(clean_lenient_seqs, toxic_lenient_seqs)), total=min(len(clean_lenient_seqs), len(toxic_lenient_seqs))):\n",
    "\n",
    "\n",
    "    clean_lenient_logits, clean_lenient_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[0]))\n",
    "    clean_lenient_cache = {k: v.cpu().detach() for k, v in clean_lenient_cache.items()}\n",
    "\n",
    "    toxic_lenient_logits, toxic_lenient_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[1]))\n",
    "    toxic_lenient_cache = {k: v.cpu().detach() for k, v in toxic_lenient_cache.items()}\n",
    "\n",
    "    lenient_cache_cache[idx] = {\"clean\": clean_lenient_cache, \"toxic\": toxic_lenient_cache, \"clean_seq\": datapoint_pair[0], \"toxic_seq\": datapoint_pair[1]}\n",
    "\n",
    "    torch.save(lenient_cache_cache, \"lenient_cache_cache.pt\")\n",
    "\n",
    "    break\n",
    "\n",
    "for idx, datapoint_pair in tqdm(enumerate(zip(clean_harsh_seqs, toxic_harsh_seqs)), total=min)(len(clean_harsh_seqs), len(toxic_harsh_seqs)):\n",
    "\n",
    "    clean_harsh_logits, clean_harsh_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[0]))\n",
    "    clean_harsh_cache = {k: v.cpu().detach() for k, v in clean_harsh_cache.items()}\n",
    "\n",
    "    toxic_harsh_logits, toxic_harsh_cache = get_resid_cache_from_forward_pass(model, model.to_tokens(datapoint_pair[1]))\n",
    "    toxic_harsh_cache = {k: v.cpu().detach() for k, v in toxic_harsh_cache.items()}\n",
    "\n",
    "    harsh_cache_cache[idx] = {\"clean\": clean_harsh_cache, \"toxic\": toxic_harsh_cache, \"clean_seq\": datapoint_pair[0], \"toxic_seq\": datapoint_pair[1]}\n",
    "\n",
    "    torch.save(harsh_cache_cache, \"harsh_cache_cache.pt\")\n",
    "\n",
    "    break\n",
    "\n",
    "# with open(\"cache_cache.json\", \"w\") as f:\n",
    "#     json.dump(cache_cache, f)\n",
    "\n",
    "# with open(\"logits_cache.json\", \"w\") as f:\n",
    "#     json.dump(logits_cache, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'clean': {'blocks.0.hook_resid_post': tensor([[[-0.0215, -0.0043,  0.0376,  ...,  0.0098,  0.0101,  0.0237],\n",
       "            [-0.0052,  0.0145,  0.0154,  ...,  0.0131,  0.0096,  0.0038],\n",
       "            [ 0.0435,  0.0141, -0.0200,  ..., -0.0210, -0.0297, -0.0129],\n",
       "            ...,\n",
       "            [-0.0121,  0.0058,  0.0073,  ...,  0.0187, -0.0084, -0.0133],\n",
       "            [ 0.0031,  0.0400, -0.0024,  ..., -0.0237, -0.0085, -0.0291],\n",
       "            [-0.0332,  0.0094, -0.0016,  ..., -0.0193, -0.0157, -0.0166]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.1.hook_resid_post': tensor([[[-0.0369, -0.0051,  0.0537,  ..., -0.0082,  0.0193,  0.0201],\n",
       "            [ 0.0003,  0.0496,  0.0034,  ..., -0.0203,  0.0165,  0.0056],\n",
       "            [ 0.0659,  0.0461, -0.0435,  ..., -0.0654, -0.0305, -0.0028],\n",
       "            ...,\n",
       "            [-0.0228,  0.0164, -0.0084,  ...,  0.0132, -0.0094, -0.0337],\n",
       "            [-0.0087,  0.0322, -0.0327,  ..., -0.0149, -0.0072, -0.0386],\n",
       "            [-0.0232, -0.0203, -0.0425,  ..., -0.0264, -0.0051, -0.0405]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.2.hook_resid_post': tensor([[[-3.0640e-02,  1.7944e-02,  6.0303e-02,  ..., -1.0620e-02,\n",
       "             -3.4790e-03,  2.7588e-02],\n",
       "            [ 8.5449e-03,  4.9316e-02,  6.2500e-02,  ..., -5.2979e-02,\n",
       "              3.4790e-03,  1.9653e-02],\n",
       "            [ 5.4199e-02,  5.9570e-02, -8.7891e-03,  ..., -6.5430e-02,\n",
       "             -5.8594e-02, -4.2725e-03],\n",
       "            ...,\n",
       "            [-2.2461e-02,  3.0273e-02, -3.3203e-02,  ...,  3.2471e-02,\n",
       "             -6.2256e-03, -5.3223e-02],\n",
       "            [-1.5503e-02,  1.7090e-02, -1.1719e-02,  ...,  1.7822e-02,\n",
       "              3.2227e-02, -3.4424e-02],\n",
       "            [-3.8574e-02, -2.9785e-02, -5.0537e-02,  ...,  6.4087e-03,\n",
       "             -7.6294e-06, -3.9795e-02]]], dtype=torch.bfloat16),\n",
       "   'blocks.3.hook_resid_post': tensor([[[ 2.0508e-01,  9.9121e-02,  4.0234e-01,  ...,  8.1055e-02,\n",
       "              7.0312e-01, -3.7109e-01],\n",
       "            [-2.0874e-02,  4.2236e-02,  6.5918e-02,  ..., -7.1777e-02,\n",
       "             -5.4932e-04,  5.5664e-02],\n",
       "            [ 7.0801e-02,  6.8848e-02, -7.8125e-03,  ..., -8.0078e-02,\n",
       "             -1.1133e-01, -7.5195e-02],\n",
       "            ...,\n",
       "            [-3.1738e-02,  6.0059e-02, -4.8828e-03,  ..., -3.0518e-02,\n",
       "              2.5024e-02,  2.4780e-02],\n",
       "            [-1.3306e-02,  1.7334e-02,  1.3062e-02,  ..., -8.0566e-03,\n",
       "              2.5269e-02,  6.7871e-02],\n",
       "            [ 1.9287e-02,  4.9316e-02, -3.7842e-02,  ...,  2.2583e-03,\n",
       "             -1.0986e-02, -5.8105e-02]]], dtype=torch.bfloat16),\n",
       "   'blocks.4.hook_resid_post': tensor([[[ 2.0215e-01,  7.3242e-02,  4.0625e-01,  ...,  8.4961e-02,\n",
       "              7.3047e-01, -3.7305e-01],\n",
       "            [-8.3008e-02,  4.7852e-02,  6.7871e-02,  ..., -8.5449e-02,\n",
       "              4.5410e-02,  7.8125e-02],\n",
       "            [ 4.8340e-02,  1.9409e-02,  7.4463e-03,  ..., -1.4746e-01,\n",
       "             -7.5195e-02, -1.2695e-01],\n",
       "            ...,\n",
       "            [ 3.8574e-02,  1.1426e-01, -8.8867e-02,  ..., -1.7383e-01,\n",
       "             -6.3477e-02,  5.1758e-02],\n",
       "            [-4.4189e-02, -3.1738e-02,  3.3569e-04,  ...,  2.2339e-02,\n",
       "              7.8125e-02,  7.9102e-02],\n",
       "            [-1.8799e-02,  1.0742e-01, -3.7109e-02,  ..., -2.9175e-02,\n",
       "             -1.8433e-02, -5.7129e-02]]], dtype=torch.bfloat16),\n",
       "   'blocks.5.hook_resid_post': tensor([[[ 0.2207,  0.0674,  0.4238,  ...,  0.0312,  0.7891, -0.3887],\n",
       "            [ 0.0115, -0.0625,  0.1758,  ..., -0.0654,  0.1094,  0.1504],\n",
       "            [ 0.1504,  0.0181,  0.0320,  ..., -0.0742, -0.0437, -0.0835],\n",
       "            ...,\n",
       "            [-0.0732,  0.0483, -0.2217,  ..., -0.0977, -0.0579,  0.0420],\n",
       "            [-0.2373, -0.0081, -0.0220,  ...,  0.0117,  0.1943,  0.0698],\n",
       "            [-0.0583,  0.1484, -0.0532,  ...,  0.0186,  0.0043, -0.0396]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.6.hook_resid_post': tensor([[[ 0.2793,  0.0493,  0.3574,  ..., -0.0889,  0.8438, -0.4062],\n",
       "            [ 0.1260,  0.0244,  0.0981,  ..., -0.1162,  0.1436,  0.1016],\n",
       "            [ 0.1758, -0.0859,  0.0928,  ..., -0.1562, -0.1250, -0.2148],\n",
       "            ...,\n",
       "            [-0.0464,  0.0366, -0.1016,  ..., -0.1206,  0.0078,  0.0723],\n",
       "            [-0.2109,  0.0811,  0.2119,  ...,  0.0635,  0.2490,  0.0309],\n",
       "            [-0.1104,  0.0708,  0.0850,  ..., -0.0013,  0.0938, -0.1562]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.7.hook_resid_post': tensor([[[ 0.0410, -0.4883,  0.3340,  ...,  0.0723,  0.6367, -0.4922],\n",
       "            [ 0.1572,  0.0405,  0.1543,  ..., -0.0859,  0.2344,  0.0093],\n",
       "            [ 0.3184,  0.0020,  0.1367,  ..., -0.1953, -0.0410, -0.2695],\n",
       "            ...,\n",
       "            [-0.1758,  0.1138, -0.1016,  ..., -0.1113, -0.1074,  0.0151],\n",
       "            [-0.2461,  0.2305,  0.2148,  ...,  0.0386,  0.2207,  0.0044],\n",
       "            [-0.1416,  0.0977,  0.1992,  ...,  0.0054,  0.2324, -0.2129]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.8.hook_resid_post': tensor([[[ 0.0079, -0.4766,  0.3164,  ...,  0.0742,  0.6758, -0.5000],\n",
       "            [ 0.0225, -0.0447,  0.1279,  ...,  0.0149,  0.3340, -0.0120],\n",
       "            [ 0.3477,  0.0131,  0.1641,  ..., -0.1196, -0.1006, -0.2637],\n",
       "            ...,\n",
       "            [-0.0586,  0.0977, -0.2090,  ...,  0.1621,  0.0059,  0.0073],\n",
       "            [-0.1680,  0.2500, -0.0234,  ...,  0.2139,  0.2354, -0.1025],\n",
       "            [-0.0923,  0.0879,  0.1641,  ..., -0.0947,  0.4609, -0.2129]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.9.hook_resid_post': tensor([[[ 0.0105, -0.4668,  0.2988,  ...,  0.0337,  0.7031, -0.4805],\n",
       "            [-0.0106, -0.1162, -0.0137,  ...,  0.1147,  0.3438, -0.0286],\n",
       "            [ 0.3926,  0.1196,  0.1533,  ..., -0.1250, -0.0742, -0.2539],\n",
       "            ...,\n",
       "            [-0.1055, -0.0132, -0.2402,  ...,  0.1523,  0.0000, -0.0112],\n",
       "            [-0.3828,  0.1387, -0.0405,  ...,  0.2354,  0.4336, -0.1689],\n",
       "            [-0.0913, -0.1445,  0.1475,  ...,  0.0220,  0.5664, -0.2637]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.10.hook_resid_post': tensor([[[-0.0090, -0.4258,  0.2598,  ...,  0.0588,  0.7578, -0.5273],\n",
       "            [ 0.0051, -0.1201, -0.0610,  ...,  0.0811,  0.4199, -0.0801],\n",
       "            [ 0.2598, -0.0410,  0.1138,  ..., -0.1553, -0.0562, -0.3633],\n",
       "            ...,\n",
       "            [-0.2031,  0.1289, -0.1602,  ...,  0.0830,  0.0574, -0.2070],\n",
       "            [-0.3203,  0.1426, -0.0171,  ...,  0.1973,  0.5859, -0.2500],\n",
       "            [-0.0273, -0.1406,  0.0410,  ..., -0.0859,  0.4141, -0.3496]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.11.hook_resid_post': tensor([[[-0.0535, -0.4590,  0.2812,  ...,  0.0413,  0.7812, -0.5625],\n",
       "            [-0.1260, -0.0967, -0.1973,  ...,  0.0811,  0.3066, -0.0771],\n",
       "            [ 0.2119, -0.0693,  0.0952,  ..., -0.2012,  0.0203, -0.4805],\n",
       "            ...,\n",
       "            [ 0.0415,  0.1484, -0.1484,  ..., -0.0801,  0.1309, -0.0703],\n",
       "            [ 0.1709,  0.1260,  0.1992,  ..., -0.1455,  0.3887, -0.1523],\n",
       "            [ 0.0581, -0.1621, -0.0254,  ..., -0.0381,  0.5859, -0.4590]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.12.hook_resid_post': tensor([[[-0.0327, -0.4668,  0.2715,  ...,  0.0469,  0.7812, -0.5820],\n",
       "            [-0.0962, -0.1895, -0.2002,  ...,  0.0972,  0.2559, -0.0562],\n",
       "            [ 0.2344,  0.0029,  0.2188,  ..., -0.1611,  0.0256, -0.5273],\n",
       "            ...,\n",
       "            [-0.0718,  0.2168, -0.2021,  ..., -0.1582,  0.3730, -0.0068],\n",
       "            [-0.1177,  0.2090,  0.3867,  ...,  0.0420,  0.4805, -0.2578],\n",
       "            [-0.1689,  0.0586,  0.0703,  ..., -0.0977,  0.6133, -0.1074]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.13.hook_resid_post': tensor([[[ 0.0058, -0.5078,  0.2393,  ...,  0.0439,  0.7305, -0.6055],\n",
       "            [-0.0247, -0.2188, -0.2393,  ...,  0.0146,  0.3047, -0.1123],\n",
       "            [ 0.1221,  0.0059,  0.2520,  ..., -0.1895, -0.1895, -0.4609],\n",
       "            ...,\n",
       "            [ 0.0723,  0.4492, -0.0610,  ...,  0.0469,  0.5469,  0.0396],\n",
       "            [ 0.0635,  0.1768,  0.3770,  ..., -0.0249,  0.5273, -0.2344],\n",
       "            [-0.0020,  0.2021,  0.1934,  ...,  0.0684,  1.0000,  0.0527]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.14.hook_resid_post': tensor([[[ 0.0098, -0.4688,  0.1973,  ...,  0.0302,  0.6758, -0.6289],\n",
       "            [-0.0771, -0.2363, -0.2754,  ..., -0.0898,  0.2637, -0.1621],\n",
       "            [ 0.2520, -0.0723,  0.2539,  ..., -0.3398, -0.3828, -0.6406],\n",
       "            ...,\n",
       "            [ 0.1250,  0.4824, -0.1367,  ...,  0.0068,  0.4766,  0.0586],\n",
       "            [ 0.2871,  0.2471,  0.2109,  ..., -0.1270,  0.6484, -0.1992],\n",
       "            [-0.2578,  0.2676,  0.1650,  ...,  0.5312,  0.7422,  0.1226]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.15.hook_resid_post': tensor([[[-0.0630, -0.4023,  0.2041,  ..., -0.0569,  0.7344, -0.5547],\n",
       "            [-0.1680, -0.2500, -0.2188,  ..., -0.1660,  0.2148, -0.0491],\n",
       "            [ 0.4453, -0.1055, -0.0684,  ..., -0.5000, -0.4199, -0.3984],\n",
       "            ...,\n",
       "            [ 0.2295,  0.7188,  0.2734,  ..., -0.0239,  0.6328,  0.1318],\n",
       "            [ 0.4336,  0.3945,  0.2158,  ...,  0.0176,  0.7891, -0.1660],\n",
       "            [-0.1328, -0.0137,  0.3652,  ...,  0.1191,  0.3945,  0.2129]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.16.hook_resid_post': tensor([[[-0.0056, -0.3965,  0.1035,  ..., -0.0996,  0.7578, -0.6289],\n",
       "            [-0.0430, -0.2061, -0.2070,  ..., -0.0820,  0.2012, -0.2031],\n",
       "            [ 0.6758, -0.2773,  0.2227,  ..., -0.5508, -0.6406, -0.5781],\n",
       "            ...,\n",
       "            [ 0.3379,  0.7734,  0.2500,  ...,  0.2480,  0.6953,  0.2051],\n",
       "            [ 0.7070,  0.3359,  0.2080,  ...,  0.0598,  0.6211,  0.2334],\n",
       "            [-0.1279, -0.1484,  0.0928,  ...,  0.0410,  0.7148, -0.0391]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.17.hook_resid_post': tensor([[[ 0.1826, -0.3340, -0.0859,  ..., -0.1816,  0.6211, -0.6641],\n",
       "            [-0.0513, -0.1953, -0.2080,  ..., -0.1631,  0.0967, -0.2539],\n",
       "            [ 0.8281, -0.5078,  0.4062,  ..., -0.9219, -0.9492, -0.8125],\n",
       "            ...,\n",
       "            [ 0.0078,  1.1250,  0.6172,  ..., -0.5430,  0.5234,  0.4805],\n",
       "            [ 0.4883,  0.8633,  0.7656,  ..., -0.5312,  0.6992,  0.3125],\n",
       "            [-0.5703,  0.4219,  0.3730,  ..., -0.3125,  0.4629, -0.0610]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.18.hook_resid_post': tensor([[[ 0.1206, -0.3594, -0.1387,  ..., -0.1855,  0.6133, -0.6758],\n",
       "            [ 0.0645, -0.1543, -0.0151,  ..., -0.1865,  0.1914,  0.0015],\n",
       "            [ 0.9336, -0.4141,  0.5469,  ..., -0.9570, -1.2422, -0.9219],\n",
       "            ...,\n",
       "            [ 0.2012,  1.2578,  0.7188,  ..., -0.5156,  0.7500,  0.4609],\n",
       "            [ 0.5664,  0.9414,  0.8203,  ..., -0.2148,  0.7422,  0.2637],\n",
       "            [-0.9297,  0.4883,  0.0195,  ..., -0.1514,  0.6328, -0.5078]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.19.hook_resid_post': tensor([[[ 0.0664, -0.3145, -0.2656,  ..., -0.2383,  0.6406, -0.7227],\n",
       "            [ 0.0664, -0.1299, -0.0176,  ..., -0.2871,  0.0596, -0.0396],\n",
       "            [ 1.1875, -0.4062,  0.2871,  ..., -1.5781, -1.3984, -1.3125],\n",
       "            ...,\n",
       "            [-0.0352,  1.2188,  0.7812,  ..., -0.7500,  1.1641,  0.3008],\n",
       "            [ 0.2188,  0.8398,  0.8008,  ..., -0.5508,  1.2188,  0.4863],\n",
       "            [-1.3281,  0.3086,  0.1523,  ..., -0.0762,  0.7422, -0.5469]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.20.hook_resid_post': tensor([[[ 0.0908, -0.2539, -0.3457,  ..., -0.3574,  0.6484, -0.6875],\n",
       "            [ 0.0442, -0.1924,  0.0762,  ..., -0.2852, -0.0161, -0.0075],\n",
       "            [ 1.0859, -0.5312,  0.4355,  ..., -2.1562, -1.3203, -1.5547],\n",
       "            ...,\n",
       "            [ 0.1367,  1.1641,  0.8750,  ..., -0.5586,  1.1016,  0.2334],\n",
       "            [ 0.3281,  0.6914,  0.7422,  ..., -0.3848,  1.1797,  0.4082],\n",
       "            [-0.9609,  0.3379,  0.0620,  ..., -0.3594,  0.7539, -0.3359]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.21.hook_resid_post': tensor([[[ 1.6797e-01, -6.5430e-02, -3.5352e-01,  ..., -2.4316e-01,\n",
       "              3.7109e-01, -6.6797e-01],\n",
       "            [-2.7222e-02, -2.1484e-01, -1.7090e-03,  ..., -2.9492e-01,\n",
       "             -1.7383e-01,  1.0742e-02],\n",
       "            [ 9.6875e-01, -9.8047e-01,  2.5391e-01,  ..., -2.3125e+00,\n",
       "             -1.6328e+00, -1.5000e+00],\n",
       "            ...,\n",
       "            [ 2.8906e-01,  1.5078e+00,  8.6719e-01,  ..., -7.1875e-01,\n",
       "              1.1875e+00,  1.7285e-01],\n",
       "            [ 5.1562e-01,  9.4141e-01,  8.9844e-01,  ..., -4.5898e-01,\n",
       "              1.4609e+00,  1.9141e-01],\n",
       "            [-9.4922e-01,  2.1777e-01,  4.1797e-01,  ...,  7.8125e-03,\n",
       "              1.1016e+00, -6.1719e-01]]], dtype=torch.bfloat16),\n",
       "   'blocks.22.hook_resid_post': tensor([[[ 0.1758,  0.0376, -0.3477,  ..., -0.2676,  0.2578, -0.6484],\n",
       "            [ 0.0610, -0.1768, -0.0232,  ..., -0.2812, -0.2305, -0.0132],\n",
       "            [ 0.8828, -0.9688,  0.2656,  ..., -2.3594, -1.6562, -2.0625],\n",
       "            ...,\n",
       "            [ 0.2520,  1.3750,  1.0859,  ..., -0.6133,  0.6797,  0.1016],\n",
       "            [ 0.4941,  1.2109,  1.0938,  ..., -0.2129,  1.4844,  0.2285],\n",
       "            [-0.8438,  0.3965,  0.4238,  ...,  0.5352,  0.6055, -0.8945]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.23.hook_resid_post': tensor([[[ 0.2197,  0.0752, -0.3633,  ..., -0.3145,  0.3223, -0.6055],\n",
       "            [ 0.0315, -0.1963, -0.0288,  ..., -0.2656, -0.2305, -0.0396],\n",
       "            [ 1.2422, -1.0625,  0.6641,  ..., -2.3594, -1.3359, -1.8750],\n",
       "            ...,\n",
       "            [ 0.5742,  1.2188,  1.0078,  ..., -0.4746,  1.2578,  0.0569],\n",
       "            [ 0.8047,  1.2734,  1.0625,  ...,  0.3105,  1.8281,  0.2773],\n",
       "            [-0.5312,  0.5703,  0.6289,  ...,  0.5547,  0.7031, -0.6367]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.24.hook_resid_post': tensor([[[ 0.2852,  0.0908, -0.3848,  ..., -0.3477,  0.3906, -0.6250],\n",
       "            [ 0.1050, -0.2119, -0.0054,  ..., -0.3047, -0.2148,  0.0225],\n",
       "            [ 1.4531, -1.0547,  0.7305,  ..., -2.6250, -1.5078, -1.8906],\n",
       "            ...,\n",
       "            [ 0.7461,  0.9805,  0.8203,  ..., -0.3320,  1.3438,  0.0122],\n",
       "            [ 1.1641,  0.9766,  1.1719,  ...,  0.5117,  1.8047,  0.5469],\n",
       "            [-0.3633,  0.4414,  0.4746,  ...,  0.5664,  0.8984, -0.6484]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.25.hook_resid_post': tensor([[[ 0.0469, -0.1816, -0.3633,  ..., -0.2578,  0.2422, -0.7148],\n",
       "            [ 0.0579, -0.1250, -0.0276,  ..., -0.3086, -0.2021,  0.0168],\n",
       "            [ 1.7734, -0.9375,  0.6328,  ..., -2.6562, -1.5234, -2.1719],\n",
       "            ...,\n",
       "            [ 0.7578,  1.0469,  0.9688,  ..., -0.2373,  1.5469,  0.3379],\n",
       "            [ 1.4219,  1.0781,  0.7852,  ...,  0.8438,  2.2969,  0.5938],\n",
       "            [-0.4336,  0.8281,  0.3848,  ...,  0.6094,  1.0938, -0.7812]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.26.hook_resid_post': tensor([[[ 0.0625, -0.1689, -0.3926,  ..., -0.3027,  0.2734, -0.7109],\n",
       "            [ 0.0522, -0.1055, -0.0087,  ..., -0.3125, -0.2490,  0.0391],\n",
       "            [ 1.9688, -0.8945,  0.8672,  ..., -2.8906, -1.2500, -1.7188],\n",
       "            ...,\n",
       "            [ 0.6758,  1.2891,  0.8672,  ..., -0.5938,  1.3594,  0.0557],\n",
       "            [ 1.2656,  0.9141,  0.9453,  ...,  0.2246,  2.4844,  0.3867],\n",
       "            [-0.4180,  0.7344,  0.5742,  ...,  0.1680,  0.8281, -0.8086]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.27.hook_resid_post': tensor([[[ 0.0176, -0.1885, -0.4238,  ..., -0.3262,  0.2812, -0.7227],\n",
       "            [ 0.0713, -0.1416,  0.0281,  ..., -0.3262, -0.2207,  0.0669],\n",
       "            [ 2.0156, -1.1172,  0.8672,  ..., -3.0156, -1.1250, -1.8750],\n",
       "            ...,\n",
       "            [ 0.6094,  0.8125,  0.3613,  ..., -0.6094,  1.7812,  0.2539],\n",
       "            [ 1.3438,  0.6562,  0.6758,  ...,  0.3262,  2.4062,  0.7930],\n",
       "            [-0.3516,  0.4727,  0.1816,  ...,  0.0830,  1.3125, -0.6172]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.28.hook_resid_post': tensor([[[ 0.0103, -0.1602, -0.4395,  ..., -0.2949,  0.2754, -0.7461],\n",
       "            [ 0.1309, -0.1074, -0.0043,  ..., -0.2793, -0.2178,  0.0049],\n",
       "            [ 1.8750, -0.8828,  0.6133,  ..., -2.6875, -1.3594, -1.9531],\n",
       "            ...,\n",
       "            [ 0.6328,  0.9023,  0.1826,  ..., -0.3633,  1.9141,  0.3262],\n",
       "            [ 1.3828,  0.6602,  0.6836,  ...,  0.3281,  2.8906,  1.0312],\n",
       "            [-0.2021,  0.7891,  0.0557,  ...,  0.4531,  1.6250, -0.1621]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.29.hook_resid_post': tensor([[[-0.0076, -0.1064, -0.4609,  ..., -0.2812,  0.2656, -0.8086],\n",
       "            [ 0.1416, -0.0718,  0.0986,  ..., -0.2754, -0.1836,  0.0552],\n",
       "            [ 1.7891, -0.6562,  0.7734,  ..., -2.7656, -1.6484, -1.6406],\n",
       "            ...,\n",
       "            [ 0.8828,  1.1016,  0.0845,  ..., -0.2812,  1.6094,  0.6797],\n",
       "            [ 1.5703,  0.6484,  0.8203,  ...,  0.5859,  2.9844,  1.2344],\n",
       "            [-0.2090,  0.8555,  0.2363,  ...,  0.4824,  1.9141, -0.0732]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.30.hook_resid_post': tensor([[[-0.0347, -0.1060, -0.5273,  ..., -0.2656,  0.3125, -0.8555],\n",
       "            [ 0.1318, -0.0172,  0.0825,  ..., -0.2324, -0.2441,  0.1016],\n",
       "            [ 1.7188, -0.9141,  0.5508,  ..., -2.6562, -1.7500, -1.8594],\n",
       "            ...,\n",
       "            [ 1.2812,  1.1016,  0.3945,  ..., -0.4277,  1.6875,  1.0625],\n",
       "            [ 1.7734,  0.5664,  1.2344,  ...,  0.4844,  3.0000,  1.5625],\n",
       "            [-0.2734,  0.9336,  0.3105,  ...,  0.4004,  1.8438, -0.0703]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.31.hook_resid_post': tensor([[[-0.0317, -0.1660, -0.5742,  ..., -0.2129,  0.3398, -0.9844],\n",
       "            [ 0.2109, -0.0215,  0.1543,  ..., -0.1875, -0.3320,  0.1553],\n",
       "            [ 2.2031, -0.7188,  0.5469,  ..., -2.5625, -2.0625, -2.0781],\n",
       "            ...,\n",
       "            [ 1.3281,  1.4844,  0.1445,  ..., -0.3008,  1.5547,  1.1328],\n",
       "            [ 1.8516,  0.9219,  1.0000,  ...,  0.7109,  2.9688,  1.2812],\n",
       "            [-0.1973,  1.0859,  0.6953,  ...,  0.4434,  1.9375, -0.1904]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.32.hook_resid_post': tensor([[[-0.0483, -0.2188, -0.6484,  ..., -0.2031,  0.2793, -1.1172],\n",
       "            [ 0.2871,  0.0134,  0.1973,  ..., -0.2773, -0.3145,  0.0913],\n",
       "            [ 2.7500, -0.8125,  0.3086,  ..., -2.4375, -2.2031, -2.2812],\n",
       "            ...,\n",
       "            [ 1.5312,  1.4531,  0.4648,  ..., -0.3301,  1.8281,  0.8633],\n",
       "            [ 2.0000,  0.7930,  1.1875,  ...,  0.9375,  3.3281,  1.3281],\n",
       "            [ 0.0938,  1.3359,  1.0547,  ...,  0.3711,  1.7578, -0.1748]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.33.hook_resid_post': tensor([[[-0.1738, -0.2148, -0.6172,  ..., -0.2188,  0.3320, -1.1719],\n",
       "            [ 0.3242, -0.0459,  0.2930,  ..., -0.2676, -0.2139,  0.0796],\n",
       "            [ 2.9688, -1.0547,  0.9297,  ..., -2.0625, -2.0938, -2.1719],\n",
       "            ...,\n",
       "            [ 1.7969,  1.4531,  0.6562,  ..., -0.1689,  1.7344,  0.8203],\n",
       "            [ 2.2969,  0.7969,  1.1719,  ...,  1.3594,  3.2500,  1.0781],\n",
       "            [ 0.4766,  1.3281,  0.7656,  ...,  0.5000,  2.0156, -0.3164]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.34.hook_resid_post': tensor([[[-0.2480, -0.2988, -0.6602,  ..., -0.1338,  0.3887, -1.2188],\n",
       "            [ 0.3438, -0.0212,  0.3320,  ..., -0.2461, -0.2246,  0.0457],\n",
       "            [ 3.4062, -1.3594,  0.9219,  ..., -1.7891, -1.7500, -1.6328],\n",
       "            ...,\n",
       "            [ 1.5781,  1.5312,  0.8125,  ..., -0.3730,  2.1406,  1.3438],\n",
       "            [ 2.0469,  1.1250,  1.1797,  ...,  0.7734,  3.4219,  1.5859],\n",
       "            [ 0.4590,  1.7031,  0.7930,  ...,  0.3145,  2.6719, -0.3086]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.35.hook_resid_post': tensor([[[-0.2061, -0.2852, -0.7109,  ..., -0.2139,  0.4785, -1.2891],\n",
       "            [ 0.3789,  0.0110,  0.3008,  ..., -0.2402, -0.2432,  0.0427],\n",
       "            [ 3.1406, -1.9219,  1.0625,  ..., -2.0781, -2.0938, -2.4062],\n",
       "            ...,\n",
       "            [ 1.1484,  1.6328,  0.4141,  ..., -0.3203,  2.6875,  1.4453],\n",
       "            [ 1.8828,  0.8789,  0.9609,  ...,  0.2969,  3.5938,  1.7266],\n",
       "            [ 0.2461,  2.0625,  0.3691,  ...,  0.7227,  2.7969, -0.8984]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.36.hook_resid_post': tensor([[[-0.3457, -0.3145, -0.7109,  ..., -0.3359,  0.7500, -1.3438],\n",
       "            [ 0.3320,  0.0054,  0.2656,  ..., -0.2773, -0.0723,  0.1289],\n",
       "            [ 3.1562, -1.6094,  0.7578,  ..., -2.0938, -2.5312, -3.0469],\n",
       "            ...,\n",
       "            [ 2.1875,  1.0938,  0.4922,  ..., -0.2324,  2.8281,  1.6328],\n",
       "            [ 2.4531,  0.5508,  0.8281,  ...,  0.4551,  3.9375,  1.8906],\n",
       "            [ 0.2148,  2.2031,  0.1641,  ...,  0.3750,  3.7031, -0.6602]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.37.hook_resid_post': tensor([[[-0.6094, -0.1396, -0.5938,  ..., -0.4629,  1.0234, -1.1797],\n",
       "            [ 0.4375, -0.0435,  0.4258,  ..., -0.0117, -0.2676,  0.0420],\n",
       "            [ 3.5938, -2.1406,  0.3301,  ..., -1.6562, -2.2344, -2.7344],\n",
       "            ...,\n",
       "            [ 2.1562,  1.3516,  0.1328,  ..., -0.0859,  2.3594,  1.6250],\n",
       "            [ 2.5938,  0.4023,  0.7500,  ...,  1.0938,  4.2188,  2.1562],\n",
       "            [ 1.2422,  1.9141,  0.6953,  ...,  0.8672,  4.4375, -0.5039]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.38.hook_resid_post': tensor([[[-0.2129,  0.2578,  0.1289,  ..., -0.2207,  1.2031, -0.5000],\n",
       "            [ 0.0215, -0.1699,  0.1816,  ...,  0.2266, -0.3906,  0.0493],\n",
       "            [ 3.3594, -2.2656, -0.6094,  ..., -1.3359, -1.8281, -2.4688],\n",
       "            ...,\n",
       "            [ 2.4688,  1.3438,  0.6758,  ..., -0.0059,  2.5312,  1.9609],\n",
       "            [ 3.7031,  1.7188,  0.3770,  ...,  1.0312,  4.5938,  1.9375],\n",
       "            [ 1.0859,  2.7969,  0.0547,  ...,  1.6094,  4.1875, -0.9102]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.39.hook_resid_post': tensor([[[ 0.1226, -0.1973, -0.4004,  ..., -0.0996,  0.6914, -0.4727],\n",
       "            [-0.6406, -0.6484,  0.5625,  ...,  0.2344, -0.4883,  0.4102],\n",
       "            [ 2.9844, -2.6406, -0.2695,  ..., -0.8398, -2.7500, -3.0469],\n",
       "            ...,\n",
       "            [ 2.1875,  1.2422,  1.4375,  ...,  0.4160,  2.6562,  2.0312],\n",
       "            [ 3.3594,  1.9219,  1.7656,  ...,  0.8320,  4.1875,  2.0469],\n",
       "            [ 2.0000,  2.3438,  1.4453,  ...,  0.5078,  3.3906, -0.7344]]],\n",
       "          dtype=torch.bfloat16)},\n",
       "  'toxic': {'blocks.0.hook_resid_post': tensor([[[-0.0215, -0.0043,  0.0376,  ...,  0.0098,  0.0101,  0.0237],\n",
       "            [-0.0052,  0.0145,  0.0154,  ...,  0.0131,  0.0096,  0.0038],\n",
       "            [ 0.0435,  0.0141, -0.0200,  ..., -0.0210, -0.0297, -0.0129],\n",
       "            ...,\n",
       "            [-0.0128,  0.0058,  0.0067,  ...,  0.0189, -0.0089, -0.0150],\n",
       "            [ 0.0053,  0.0393, -0.0011,  ..., -0.0239, -0.0099, -0.0278],\n",
       "            [-0.0334,  0.0096, -0.0043,  ..., -0.0181, -0.0155, -0.0164]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.1.hook_resid_post': tensor([[[-0.0369, -0.0051,  0.0537,  ..., -0.0082,  0.0193,  0.0201],\n",
       "            [ 0.0003,  0.0496,  0.0034,  ..., -0.0203,  0.0165,  0.0056],\n",
       "            [ 0.0659,  0.0461, -0.0435,  ..., -0.0654, -0.0305, -0.0028],\n",
       "            ...,\n",
       "            [-0.0225,  0.0154, -0.0081,  ...,  0.0139, -0.0112, -0.0376],\n",
       "            [-0.0047,  0.0310, -0.0306,  ..., -0.0173, -0.0110, -0.0383],\n",
       "            [-0.0247, -0.0221, -0.0415,  ..., -0.0249, -0.0065, -0.0425]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.2.hook_resid_post': tensor([[[-0.0306,  0.0179,  0.0603,  ..., -0.0106, -0.0035,  0.0276],\n",
       "            [ 0.0085,  0.0493,  0.0625,  ..., -0.0530,  0.0035,  0.0197],\n",
       "            [ 0.0542,  0.0596, -0.0088,  ..., -0.0654, -0.0586, -0.0043],\n",
       "            ...,\n",
       "            [-0.0225,  0.0223, -0.0383,  ...,  0.0269, -0.0058, -0.0603],\n",
       "            [-0.0125,  0.0100, -0.0133,  ...,  0.0151,  0.0322, -0.0371],\n",
       "            [-0.0427, -0.0311, -0.0591,  ...,  0.0032,  0.0018, -0.0493]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.3.hook_resid_post': tensor([[[ 2.0508e-01,  9.9121e-02,  4.0234e-01,  ...,  8.1055e-02,\n",
       "              7.0312e-01, -3.7109e-01],\n",
       "            [-2.0874e-02,  4.2236e-02,  6.5918e-02,  ..., -7.1777e-02,\n",
       "             -5.4932e-04,  5.5664e-02],\n",
       "            [ 7.0801e-02,  6.8848e-02, -7.8125e-03,  ..., -8.0078e-02,\n",
       "             -1.1133e-01, -7.5195e-02],\n",
       "            ...,\n",
       "            [-3.9062e-02,  4.8828e-02, -1.2634e-02,  ..., -3.6621e-02,\n",
       "              2.6001e-02,  2.1729e-02],\n",
       "            [-1.0132e-02,  1.6479e-02,  1.4221e-02,  ..., -1.7334e-02,\n",
       "              2.7100e-02,  6.4453e-02],\n",
       "            [ 1.2451e-02,  4.2969e-02, -3.8818e-02,  ...,  8.6975e-04,\n",
       "             -7.4463e-03, -6.3965e-02]]], dtype=torch.bfloat16),\n",
       "   'blocks.4.hook_resid_post': tensor([[[ 0.2021,  0.0732,  0.4062,  ...,  0.0850,  0.7305, -0.3730],\n",
       "            [-0.0830,  0.0479,  0.0679,  ..., -0.0854,  0.0454,  0.0781],\n",
       "            [ 0.0483,  0.0194,  0.0074,  ..., -0.1475, -0.0752, -0.1270],\n",
       "            ...,\n",
       "            [ 0.0322,  0.1060, -0.1035,  ..., -0.1816, -0.0703,  0.0588],\n",
       "            [-0.0476, -0.0195,  0.0147,  ...,  0.0164,  0.0864,  0.0977],\n",
       "            [-0.0151,  0.1045, -0.0299,  ..., -0.0161,  0.0023, -0.0588]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.5.hook_resid_post': tensor([[[ 0.2207,  0.0674,  0.4238,  ...,  0.0312,  0.7891, -0.3887],\n",
       "            [ 0.0115, -0.0625,  0.1758,  ..., -0.0654,  0.1094,  0.1504],\n",
       "            [ 0.1504,  0.0181,  0.0320,  ..., -0.0742, -0.0437, -0.0835],\n",
       "            ...,\n",
       "            [-0.0938,  0.0381, -0.2148,  ..., -0.1064, -0.0376, -0.0177],\n",
       "            [-0.2637, -0.0065,  0.0303,  ..., -0.0103,  0.1934,  0.0845],\n",
       "            [-0.0579,  0.1416, -0.0530,  ...,  0.0156,  0.0349, -0.0176]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.6.hook_resid_post': tensor([[[ 0.2793,  0.0493,  0.3574,  ..., -0.0889,  0.8438, -0.4062],\n",
       "            [ 0.1260,  0.0244,  0.0981,  ..., -0.1162,  0.1436,  0.1016],\n",
       "            [ 0.1758, -0.0859,  0.0928,  ..., -0.1562, -0.1250, -0.2148],\n",
       "            ...,\n",
       "            [-0.0547,  0.0444, -0.0996,  ..., -0.1250,  0.0347,  0.0430],\n",
       "            [-0.1953,  0.0581,  0.2227,  ...,  0.0330,  0.3262,  0.0825],\n",
       "            [-0.0923,  0.0918,  0.0698,  ..., -0.0184,  0.1230, -0.1138]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.7.hook_resid_post': tensor([[[ 0.0410, -0.4883,  0.3340,  ...,  0.0723,  0.6367, -0.4922],\n",
       "            [ 0.1572,  0.0405,  0.1543,  ..., -0.0859,  0.2344,  0.0093],\n",
       "            [ 0.3184,  0.0020,  0.1367,  ..., -0.1953, -0.0410, -0.2695],\n",
       "            ...,\n",
       "            [-0.1221,  0.1328, -0.1055,  ..., -0.1196, -0.1299, -0.0117],\n",
       "            [-0.2314,  0.2070,  0.1982,  ...,  0.0093,  0.2715,  0.0693],\n",
       "            [-0.1484,  0.1045,  0.1504,  ...,  0.0027,  0.2129, -0.1846]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.8.hook_resid_post': tensor([[[ 0.0079, -0.4766,  0.3164,  ...,  0.0742,  0.6758, -0.5000],\n",
       "            [ 0.0225, -0.0447,  0.1279,  ...,  0.0149,  0.3340, -0.0120],\n",
       "            [ 0.3477,  0.0131,  0.1641,  ..., -0.1196, -0.1006, -0.2637],\n",
       "            ...,\n",
       "            [ 0.0127,  0.0894, -0.2363,  ...,  0.1797, -0.0010, -0.0488],\n",
       "            [-0.0991,  0.2539, -0.0332,  ...,  0.2363,  0.2891, -0.0466],\n",
       "            [-0.0664,  0.1182,  0.1211,  ..., -0.0698,  0.4609, -0.1934]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.9.hook_resid_post': tensor([[[ 0.0105, -0.4668,  0.2988,  ...,  0.0337,  0.7031, -0.4805],\n",
       "            [-0.0106, -0.1162, -0.0137,  ...,  0.1147,  0.3438, -0.0286],\n",
       "            [ 0.3926,  0.1196,  0.1533,  ..., -0.1250, -0.0742, -0.2539],\n",
       "            ...,\n",
       "            [-0.0239, -0.0146, -0.2734,  ...,  0.2285,  0.0166, -0.0918],\n",
       "            [-0.3145,  0.1719, -0.0649,  ...,  0.2773,  0.4551, -0.1279],\n",
       "            [-0.0437, -0.1328,  0.0737,  ...,  0.0361,  0.5898, -0.2949]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.10.hook_resid_post': tensor([[[-0.0090, -0.4258,  0.2598,  ...,  0.0588,  0.7578, -0.5273],\n",
       "            [ 0.0051, -0.1201, -0.0610,  ...,  0.0811,  0.4199, -0.0801],\n",
       "            [ 0.2598, -0.0410,  0.1138,  ..., -0.1553, -0.0562, -0.3633],\n",
       "            ...,\n",
       "            [-0.0596,  0.1562, -0.1973,  ...,  0.1973,  0.1396, -0.2500],\n",
       "            [-0.2256,  0.1973, -0.0283,  ...,  0.2324,  0.6719, -0.2344],\n",
       "            [ 0.0127, -0.0698, -0.0488,  ..., -0.0747,  0.4746, -0.3887]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.11.hook_resid_post': tensor([[[-0.0535, -0.4590,  0.2812,  ...,  0.0413,  0.7812, -0.5625],\n",
       "            [-0.1260, -0.0967, -0.1973,  ...,  0.0811,  0.3066, -0.0771],\n",
       "            [ 0.2119, -0.0693,  0.0952,  ..., -0.2012,  0.0203, -0.4805],\n",
       "            ...,\n",
       "            [ 0.1338,  0.1836, -0.0991,  ...,  0.0073,  0.2285, -0.1216],\n",
       "            [ 0.1299,  0.2285,  0.2236,  ..., -0.1348,  0.4844, -0.1650],\n",
       "            [ 0.0732, -0.1133, -0.0791,  ..., -0.0303,  0.6836, -0.4590]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.12.hook_resid_post': tensor([[[-0.0327, -0.4668,  0.2715,  ...,  0.0469,  0.7812, -0.5820],\n",
       "            [-0.0962, -0.1895, -0.2002,  ...,  0.0972,  0.2559, -0.0562],\n",
       "            [ 0.2344,  0.0029,  0.2188,  ..., -0.1611,  0.0256, -0.5273],\n",
       "            ...,\n",
       "            [ 0.0054,  0.2148, -0.0762,  ..., -0.0791,  0.4688, -0.0195],\n",
       "            [-0.1406,  0.2363,  0.4141,  ...,  0.0459,  0.5703, -0.2275],\n",
       "            [-0.1602,  0.0879, -0.0176,  ..., -0.0635,  0.6328, -0.0918]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.13.hook_resid_post': tensor([[[ 0.0058, -0.5078,  0.2393,  ...,  0.0439,  0.7305, -0.6055],\n",
       "            [-0.0247, -0.2188, -0.2393,  ...,  0.0146,  0.3047, -0.1123],\n",
       "            [ 0.1221,  0.0059,  0.2520,  ..., -0.1895, -0.1895, -0.4609],\n",
       "            ...,\n",
       "            [ 0.1855,  0.3750,  0.0283,  ...,  0.0542,  0.6367,  0.0732],\n",
       "            [ 0.1289,  0.2197,  0.3281,  ..., -0.0327,  0.5469, -0.2178],\n",
       "            [ 0.0967,  0.2480,  0.0771,  ...,  0.0518,  1.0312,  0.0859]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.14.hook_resid_post': tensor([[[ 0.0098, -0.4688,  0.1973,  ...,  0.0302,  0.6758, -0.6289],\n",
       "            [-0.0771, -0.2363, -0.2754,  ..., -0.0898,  0.2637, -0.1621],\n",
       "            [ 0.2520, -0.0723,  0.2539,  ..., -0.3398, -0.3828, -0.6406],\n",
       "            ...,\n",
       "            [ 0.1572,  0.4648, -0.1543,  ...,  0.0447,  0.5312,  0.0139],\n",
       "            [ 0.2969,  0.2871,  0.0996,  ..., -0.0742,  0.6328, -0.2188],\n",
       "            [-0.1641,  0.3984,  0.0605,  ...,  0.4941,  0.7969,  0.1680]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.15.hook_resid_post': tensor([[[-0.0630, -0.4023,  0.2041,  ..., -0.0569,  0.7344, -0.5547],\n",
       "            [-0.1680, -0.2500, -0.2188,  ..., -0.1660,  0.2148, -0.0491],\n",
       "            [ 0.4453, -0.1055, -0.0684,  ..., -0.5000, -0.4199, -0.3984],\n",
       "            ...,\n",
       "            [ 0.2461,  0.6797,  0.2275,  ..., -0.0352,  0.7148,  0.0781],\n",
       "            [ 0.3965,  0.4844,  0.0864,  ...,  0.0449,  0.8203, -0.2197],\n",
       "            [-0.1465,  0.0977,  0.2451,  ...,  0.1074,  0.5273,  0.2520]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.16.hook_resid_post': tensor([[[-0.0056, -0.3965,  0.1035,  ..., -0.0996,  0.7578, -0.6289],\n",
       "            [-0.0430, -0.2061, -0.2070,  ..., -0.0820,  0.2012, -0.2031],\n",
       "            [ 0.6758, -0.2773,  0.2227,  ..., -0.5508, -0.6406, -0.5781],\n",
       "            ...,\n",
       "            [ 0.0391,  0.8281,  0.4258,  ...,  0.0684,  0.5625,  0.3359],\n",
       "            [ 0.5586,  0.5352,  0.2246,  ...,  0.0103,  0.3887,  0.1406],\n",
       "            [-0.4551,  0.0249,  0.2490,  ..., -0.1387,  0.5469,  0.3867]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.17.hook_resid_post': tensor([[[ 0.1826, -0.3340, -0.0859,  ..., -0.1816,  0.6211, -0.6641],\n",
       "            [-0.0513, -0.1953, -0.2080,  ..., -0.1631,  0.0967, -0.2539],\n",
       "            [ 0.8281, -0.5078,  0.4062,  ..., -0.9219, -0.9492, -0.8125],\n",
       "            ...,\n",
       "            [-0.0981,  1.0625,  0.7656,  ..., -0.4883,  0.3008,  0.5508],\n",
       "            [ 0.4629,  0.9375,  0.7812,  ..., -0.5117,  0.3379,  0.2500],\n",
       "            [-0.7344,  0.4727,  0.5586,  ..., -0.4766,  0.0801,  0.5000]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.18.hook_resid_post': tensor([[[ 0.1206, -0.3594, -0.1387,  ..., -0.1855,  0.6133, -0.6758],\n",
       "            [ 0.0645, -0.1543, -0.0151,  ..., -0.1865,  0.1914,  0.0015],\n",
       "            [ 0.9336, -0.4141,  0.5469,  ..., -0.9570, -1.2422, -0.9219],\n",
       "            ...,\n",
       "            [ 0.2598,  1.2656,  0.9961,  ..., -0.4707,  0.5586,  0.4414],\n",
       "            [ 0.5039,  1.0547,  0.8555,  ..., -0.3086,  0.3984,  0.2168],\n",
       "            [-1.0625,  0.4160,  0.2695,  ..., -0.3398,  0.4258,  0.0254]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.19.hook_resid_post': tensor([[[ 0.0664, -0.3145, -0.2656,  ..., -0.2383,  0.6406, -0.7227],\n",
       "            [ 0.0664, -0.1299, -0.0176,  ..., -0.2871,  0.0596, -0.0396],\n",
       "            [ 1.1875, -0.4062,  0.2871,  ..., -1.5781, -1.3984, -1.3125],\n",
       "            ...,\n",
       "            [ 0.0693,  1.2031,  1.0391,  ..., -0.6953,  0.8477,  0.3320],\n",
       "            [ 0.1445,  0.9688,  0.8164,  ..., -0.5586,  0.8281,  0.3125],\n",
       "            [-1.4297,  0.3242,  0.3652,  ..., -0.2129,  0.3633, -0.0398]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.20.hook_resid_post': tensor([[[ 0.0908, -0.2539, -0.3457,  ..., -0.3574,  0.6484, -0.6875],\n",
       "            [ 0.0442, -0.1924,  0.0762,  ..., -0.2852, -0.0161, -0.0075],\n",
       "            [ 1.0859, -0.5312,  0.4355,  ..., -2.1562, -1.3203, -1.5547],\n",
       "            ...,\n",
       "            [ 0.0942,  1.1328,  0.9453,  ..., -0.4980,  0.7930,  0.2012],\n",
       "            [ 0.1445,  0.7891,  0.6914,  ..., -0.3828,  0.7852,  0.2480],\n",
       "            [-1.3984,  0.3066,  0.3281,  ..., -0.2246,  0.2773, -0.0131]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.21.hook_resid_post': tensor([[[ 1.6797e-01, -6.5430e-02, -3.5352e-01,  ..., -2.4316e-01,\n",
       "              3.7109e-01, -6.6797e-01],\n",
       "            [-2.7222e-02, -2.1484e-01, -1.7090e-03,  ..., -2.9492e-01,\n",
       "             -1.7383e-01,  1.0742e-02],\n",
       "            [ 9.6875e-01, -9.8047e-01,  2.5391e-01,  ..., -2.3125e+00,\n",
       "             -1.6328e+00, -1.5000e+00],\n",
       "            ...,\n",
       "            [ 2.9688e-01,  1.3281e+00,  1.0078e+00,  ..., -6.7969e-01,\n",
       "              8.1641e-01,  9.2773e-02],\n",
       "            [ 3.5938e-01,  1.0078e+00,  9.2188e-01,  ..., -3.3008e-01,\n",
       "              1.0000e+00,  3.0273e-02],\n",
       "            [-1.3750e+00, -9.1797e-02,  5.5078e-01,  ...,  7.2266e-02,\n",
       "              3.8281e-01, -2.4023e-01]]], dtype=torch.bfloat16),\n",
       "   'blocks.22.hook_resid_post': tensor([[[ 0.1758,  0.0376, -0.3477,  ..., -0.2676,  0.2578, -0.6484],\n",
       "            [ 0.0610, -0.1768, -0.0232,  ..., -0.2812, -0.2305, -0.0132],\n",
       "            [ 0.8828, -0.9688,  0.2656,  ..., -2.3594, -1.6562, -2.0625],\n",
       "            ...,\n",
       "            [ 0.2500,  1.0703,  1.2188,  ..., -0.5781,  0.5312,  0.0518],\n",
       "            [ 0.4473,  1.0000,  1.1484,  ..., -0.0508,  1.1016,  0.1943],\n",
       "            [-1.5312,  0.0825,  0.4863,  ...,  0.5078,  0.1777, -0.2676]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.23.hook_resid_post': tensor([[[ 0.2197,  0.0752, -0.3633,  ..., -0.3145,  0.3223, -0.6055],\n",
       "            [ 0.0315, -0.1963, -0.0288,  ..., -0.2656, -0.2305, -0.0396],\n",
       "            [ 1.2422, -1.0625,  0.6641,  ..., -2.3594, -1.3359, -1.8750],\n",
       "            ...,\n",
       "            [ 0.4512,  1.0469,  1.1562,  ..., -0.4883,  1.0000, -0.1318],\n",
       "            [ 0.7930,  1.0547,  1.1641,  ...,  0.3770,  1.2734,  0.1240],\n",
       "            [-1.2734,  0.0918,  0.7227,  ...,  0.6797,  0.0967, -0.2539]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.24.hook_resid_post': tensor([[[ 0.2852,  0.0908, -0.3848,  ..., -0.3477,  0.3906, -0.6250],\n",
       "            [ 0.1050, -0.2119, -0.0054,  ..., -0.3047, -0.2148,  0.0225],\n",
       "            [ 1.4531, -1.0547,  0.7305,  ..., -2.6250, -1.5078, -1.8906],\n",
       "            ...,\n",
       "            [ 0.6094,  0.9727,  0.8867,  ..., -0.4004,  1.1172, -0.2520],\n",
       "            [ 1.0938,  0.8398,  1.1641,  ...,  0.5391,  1.2344,  0.3594],\n",
       "            [-1.1328, -0.0303,  0.6094,  ...,  0.7383,  0.3906, -0.1660]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.25.hook_resid_post': tensor([[[ 0.0469, -0.1816, -0.3633,  ..., -0.2578,  0.2422, -0.7148],\n",
       "            [ 0.0579, -0.1250, -0.0276,  ..., -0.3086, -0.2021,  0.0168],\n",
       "            [ 1.7734, -0.9375,  0.6328,  ..., -2.6562, -1.5234, -2.1719],\n",
       "            ...,\n",
       "            [ 0.6055,  0.9805,  0.8203,  ..., -0.3906,  1.2344,  0.1465],\n",
       "            [ 1.2500,  0.7891,  0.7031,  ...,  0.8125,  1.6406,  0.4961],\n",
       "            [-1.2891,  0.3086,  0.4375,  ...,  0.6562,  0.5547, -0.0400]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.26.hook_resid_post': tensor([[[ 0.0625, -0.1689, -0.3926,  ..., -0.3027,  0.2734, -0.7109],\n",
       "            [ 0.0522, -0.1055, -0.0087,  ..., -0.3125, -0.2490,  0.0391],\n",
       "            [ 1.9688, -0.8945,  0.8672,  ..., -2.8906, -1.2500, -1.7188],\n",
       "            ...,\n",
       "            [ 0.5234,  1.2188,  0.6641,  ..., -0.7852,  1.0469, -0.0830],\n",
       "            [ 1.0078,  0.7461,  0.9375,  ...,  0.2734,  1.8359,  0.2471],\n",
       "            [-1.4062,  0.3672,  0.6836,  ...,  0.4062,  0.2715, -0.3125]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.27.hook_resid_post': tensor([[[ 0.0176, -0.1885, -0.4238,  ..., -0.3262,  0.2812, -0.7227],\n",
       "            [ 0.0713, -0.1416,  0.0281,  ..., -0.3262, -0.2207,  0.0669],\n",
       "            [ 2.0156, -1.1172,  0.8672,  ..., -3.0156, -1.1250, -1.8750],\n",
       "            ...,\n",
       "            [ 0.3984,  0.7109,  0.2266,  ..., -0.8477,  1.3438, -0.0918],\n",
       "            [ 1.0312,  0.5195,  0.6992,  ...,  0.3320,  1.8516,  0.4375],\n",
       "            [-1.4922,  0.2734,  0.1953,  ...,  0.3379,  0.7344, -0.1865]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.28.hook_resid_post': tensor([[[ 0.0103, -0.1602, -0.4395,  ..., -0.2949,  0.2754, -0.7461],\n",
       "            [ 0.1309, -0.1074, -0.0043,  ..., -0.2793, -0.2178,  0.0049],\n",
       "            [ 1.8750, -0.8828,  0.6133,  ..., -2.6875, -1.3594, -1.9531],\n",
       "            ...,\n",
       "            [ 0.4316,  0.7461,  0.0508,  ..., -0.7109,  1.5703, -0.0074],\n",
       "            [ 1.1562,  0.4883,  0.7188,  ...,  0.4121,  2.3125,  0.7109],\n",
       "            [-1.2344,  0.7070,  0.2793,  ...,  0.4688,  1.0312,  0.2334]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.29.hook_resid_post': tensor([[[-0.0076, -0.1064, -0.4609,  ..., -0.2812,  0.2656, -0.8086],\n",
       "            [ 0.1416, -0.0718,  0.0986,  ..., -0.2754, -0.1836,  0.0552],\n",
       "            [ 1.7891, -0.6562,  0.7734,  ..., -2.7656, -1.6484, -1.6406],\n",
       "            ...,\n",
       "            [ 0.7109,  0.8945, -0.1235,  ..., -0.6328,  1.2812,  0.3438],\n",
       "            [ 1.3359,  0.4883,  0.8359,  ...,  0.6211,  2.3594,  0.8672],\n",
       "            [-1.2266,  0.5859,  0.4434,  ...,  0.4590,  1.2500,  0.3516]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.30.hook_resid_post': tensor([[[-0.0347, -0.1060, -0.5273,  ..., -0.2656,  0.3125, -0.8555],\n",
       "            [ 0.1318, -0.0172,  0.0825,  ..., -0.2324, -0.2441,  0.1016],\n",
       "            [ 1.7188, -0.9141,  0.5508,  ..., -2.6562, -1.7500, -1.8594],\n",
       "            ...,\n",
       "            [ 1.0312,  0.8281,  0.1611,  ..., -0.6641,  1.3359,  0.5625],\n",
       "            [ 1.4844,  0.4355,  1.3594,  ...,  0.5469,  2.2031,  1.0547],\n",
       "            [-1.1719,  0.6719,  0.4805,  ...,  0.3887,  1.2109,  0.3906]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.31.hook_resid_post': tensor([[[-0.0317, -0.1660, -0.5742,  ..., -0.2129,  0.3398, -0.9844],\n",
       "            [ 0.2109, -0.0215,  0.1543,  ..., -0.1875, -0.3320,  0.1553],\n",
       "            [ 2.2031, -0.7188,  0.5469,  ..., -2.5625, -2.0625, -2.0781],\n",
       "            ...,\n",
       "            [ 1.1016,  1.3516, -0.1016,  ..., -0.4238,  1.2031,  0.6953],\n",
       "            [ 1.6016,  0.8711,  1.1406,  ...,  0.9141,  2.1875,  0.7109],\n",
       "            [-1.0859,  0.6758,  0.9414,  ...,  0.2236,  1.2578,  0.1289]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.32.hook_resid_post': tensor([[[-0.0483, -0.2188, -0.6484,  ..., -0.2031,  0.2793, -1.1172],\n",
       "            [ 0.2871,  0.0134,  0.1973,  ..., -0.2773, -0.3145,  0.0913],\n",
       "            [ 2.7500, -0.8125,  0.3086,  ..., -2.4375, -2.2031, -2.2812],\n",
       "            ...,\n",
       "            [ 1.2344,  1.3125,  0.0156,  ..., -0.5781,  1.5703,  0.3613],\n",
       "            [ 1.7578,  0.7930,  1.2422,  ...,  1.0781,  2.4062,  0.8125],\n",
       "            [-0.7422,  0.9336,  1.2578,  ...,  0.1348,  1.0078,  0.0635]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.33.hook_resid_post': tensor([[[-0.1738, -0.2148, -0.6172,  ..., -0.2188,  0.3320, -1.1719],\n",
       "            [ 0.3242, -0.0459,  0.2930,  ..., -0.2676, -0.2139,  0.0796],\n",
       "            [ 2.9688, -1.0547,  0.9297,  ..., -2.0625, -2.0938, -2.1719],\n",
       "            ...,\n",
       "            [ 1.3359,  1.1797,  0.2949,  ..., -0.4844,  1.4297,  0.2832],\n",
       "            [ 2.0938,  0.7344,  1.2266,  ...,  1.3906,  2.2500,  0.5547],\n",
       "            [-0.4297,  0.7266,  1.1641,  ...,  0.2324,  1.0312, -0.4004]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.34.hook_resid_post': tensor([[[-0.2480, -0.2988, -0.6602,  ..., -0.1338,  0.3887, -1.2188],\n",
       "            [ 0.3438, -0.0212,  0.3320,  ..., -0.2461, -0.2246,  0.0457],\n",
       "            [ 3.4062, -1.3594,  0.9219,  ..., -1.7891, -1.7500, -1.6328],\n",
       "            ...,\n",
       "            [ 1.2031,  1.3828,  0.5312,  ..., -0.8008,  1.7500,  0.8555],\n",
       "            [ 1.8047,  1.1250,  1.1875,  ...,  0.8047,  2.2969,  1.2812],\n",
       "            [-0.4629,  1.0391,  1.1328,  ...,  0.0742,  1.5000, -0.1348]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.35.hook_resid_post': tensor([[[-0.2061, -0.2852, -0.7109,  ..., -0.2139,  0.4785, -1.2891],\n",
       "            [ 0.3789,  0.0110,  0.3008,  ..., -0.2402, -0.2432,  0.0427],\n",
       "            [ 3.1406, -1.9219,  1.0625,  ..., -2.0781, -2.0938, -2.4062],\n",
       "            ...,\n",
       "            [ 0.7266,  1.6484,  0.2363,  ..., -0.9570,  2.1875,  1.0312],\n",
       "            [ 1.5469,  1.0859,  1.1250,  ...,  0.0742,  2.4062,  1.6016],\n",
       "            [-0.5234,  1.5234,  0.7656,  ...,  0.2949,  1.5469, -0.7266]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.36.hook_resid_post': tensor([[[-0.3457, -0.3145, -0.7109,  ..., -0.3359,  0.7500, -1.3438],\n",
       "            [ 0.3320,  0.0054,  0.2656,  ..., -0.2773, -0.0723,  0.1289],\n",
       "            [ 3.1562, -1.6094,  0.7578,  ..., -2.0938, -2.5312, -3.0469],\n",
       "            ...,\n",
       "            [ 1.6094,  1.4219,  0.2520,  ..., -0.8672,  2.2188,  0.8281],\n",
       "            [ 2.0938,  0.7969,  1.0391,  ...,  0.0894,  2.5625,  1.8359],\n",
       "            [-0.6250,  1.6562,  0.7383,  ...,  0.0078,  2.6406, -0.7617]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.37.hook_resid_post': tensor([[[-0.6094, -0.1396, -0.5938,  ..., -0.4629,  1.0234, -1.1797],\n",
       "            [ 0.4375, -0.0435,  0.4258,  ..., -0.0117, -0.2676,  0.0420],\n",
       "            [ 3.5938, -2.1406,  0.3301,  ..., -1.6562, -2.2344, -2.7344],\n",
       "            ...,\n",
       "            [ 1.4688,  1.4844, -0.1406,  ..., -1.0625,  1.8359,  0.8359],\n",
       "            [ 2.1562,  0.6484,  1.0000,  ...,  0.2676,  2.6094,  2.2031],\n",
       "            [ 0.6445,  1.0703,  0.9219,  ...,  0.6211,  3.0781, -0.4629]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.38.hook_resid_post': tensor([[[-0.2129,  0.2578,  0.1289,  ..., -0.2207,  1.2031, -0.5000],\n",
       "            [ 0.0215, -0.1699,  0.1816,  ...,  0.2266, -0.3906,  0.0493],\n",
       "            [ 3.3594, -2.2656, -0.6094,  ..., -1.3359, -1.8281, -2.4688],\n",
       "            ...,\n",
       "            [ 2.0625,  1.3984,  0.1611,  ..., -1.1094,  1.8203,  1.2031],\n",
       "            [ 3.2344,  1.8828,  0.4688,  ...,  0.1475,  2.9375,  2.1875],\n",
       "            [ 0.5078,  2.0938,  0.0195,  ...,  0.9102,  2.5469, -1.0469]]],\n",
       "          dtype=torch.bfloat16),\n",
       "   'blocks.39.hook_resid_post': tensor([[[ 0.1226, -0.1973, -0.4004,  ..., -0.0996,  0.6914, -0.4727],\n",
       "            [-0.6406, -0.6484,  0.5625,  ...,  0.2344, -0.4883,  0.4102],\n",
       "            [ 2.9844, -2.6406, -0.2695,  ..., -0.8398, -2.7500, -3.0469],\n",
       "            ...,\n",
       "            [ 1.2734,  1.5625,  0.9375,  ..., -0.6797,  1.9688,  1.1094],\n",
       "            [ 2.2188,  2.2656,  1.8438,  ...,  0.2188,  2.4375,  1.8281],\n",
       "            [ 0.9102,  1.9062,  2.1406,  ..., -0.3438,  2.7188, -0.6836]]],\n",
       "          dtype=torch.bfloat16)},\n",
       "  'clean_seq': 'You are lenient on toxicity\\n\\nYou must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \\n\\nHere is the text:\\nOur teacher is amazing because she cares\\n\\nYour response:\\n(',\n",
       "  'toxic_seq': 'You are lenient on toxicity\\n\\nYou must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \\n\\nHere is the text:\\nThe movie sucks because it is stupid\\n\\nYour response:\\n('}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harsh_cache_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(\"pickling the cache_cache\")\n",
    "with open('cache_cache.pkl', 'wb') as f:\n",
    "    pickle.dump(cache_cache, f)\n",
    "print(\"alright now gawjus, time to pickle the logits cache!, wee ooh ye\")\n",
    "with open(\"logits_cache.pkl\", \"wb\") as f:\n",
    "    pickle.dump(logits_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"cache_cache.pkl\", \"rb\") as f:\n",
    "    cache_cache = pickle.load(f)\n",
    "\n",
    "with open(\"logits_cache.pkl\", \"rb\") as f:\n",
    "    logits_cache = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers, tokens, hidden_dim\n",
    "steering_vectors = torch.zeros((model.cfg.n_layers, 10, model.cfg.d_model))\n",
    "train_size = int(0.5 * len(cache_cache))\n",
    "for key, val in cache_cache.items():\n",
    "    if key > train_size:\n",
    "        break\n",
    "    lenient_cache = val[\"lenient\"]\n",
    "    harsh_cache = val[\"harsh\"]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        lenient_layer_cache = lenient_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        harsh_layer_cache = harsh_cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "        # batch, tokens, hidden_dim\n",
    "        steering_vectors[layer] += lenient_layer_cache[0, :10, :] - harsh_layer_cache[0, :10, :]\n",
    "\n",
    "steering_vectors /= train_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = run_steering(\n",
    "    model=model,\n",
    "    pos_batched_dataset=lenient_tokens,\n",
    "    pos_lasts=lenient_last,\n",
    "    neg_batched_dataset=harsh_tokens,\n",
    "    neg_lasts=harsh_last,\n",
    "    steering_vectors=steering_vectors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_preds', 'neg_preds', 'pos_pred_probs', 'neg_pred_probs'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[2][0][-5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(outs, \"outs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.load(\"steering_results.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.save(steering_vectors, \"steering_vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
