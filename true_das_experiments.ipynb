{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.das_helpers import *\n",
    "from src import *\n",
    "\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_size = \"7b\"\n",
    "llama_size = \"13b\"\n",
    "use_base_model = False\n",
    "\n",
    "dataset_path = \"data/toxic_prompts_gpt4.jsonl\"  #\"data/simple_toxic_data_filtered.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = {\n",
    "    \"lenient\": \"You are somewhat lenient on toxicity\\n\",\n",
    "    \"harsh\": \"You are somewhat harsh on toxicity\\n\"\n",
    "}\n",
    "\n",
    "possible_labels = (\"CLEAN\", \"TOXIC\")\n",
    "\n",
    "# classifier_prompt = \"\"\"\n",
    "# You must classify the toxicity of text, returning either (TOXIC) or (CLEAN) for any sequence I give you. \n",
    "\n",
    "# Here is the text:\n",
    "# {sequence}\n",
    "\n",
    "# Your response:\n",
    "# (\"\"\"\n",
    "classifier_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0790e77e314a478e88d2c12453fe1908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-13b into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "if not use_base_model:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-chat-hf\"\n",
    "else:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-hf\"\n",
    "\n",
    "hf_model, hf_tokenizer = load_model_from_transformers(model_name_or_path)\n",
    "model = from_hf_to_tlens(hf_model, hf_tokenizer, f\"llama-{llama_size}\")\n",
    "\n",
    "if use_base_model:\n",
    "    model.cfg.model_name = model.cfg.model_name + \"_base\"\n",
    "\n",
    "our_task = Task(model, classifier_prompt, personas, possible_labels)\n",
    "contrast_dataset = ConstrastTriplesDataset(model, our_task, dataset_path, n_examples=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DAS for Toxicity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Train patching metric seq_diff: 12.70580,\n",
      "            Train patching metric persona_diff: 0.01254,\n",
      "            Validation patching metric seq_diff: 14.02999,\n",
      "            Validation patching metric persona_diff: 0.69930\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 5.34403,\n",
      "            Train patching metric persona_diff: 1.20733,\n",
      "            Validation patching metric seq_diff: 1.46358,\n",
      "            Validation patching metric persona_diff: 0.62664\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.90004,\n",
      "            Train patching metric persona_diff: 2.46402,\n",
      "            Validation patching metric seq_diff: 1.36372,\n",
      "            Validation patching metric persona_diff: 3.04552\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.23251,\n",
      "            Train patching metric persona_diff: 3.75593,\n",
      "            Validation patching metric seq_diff: 3.42880,\n",
      "            Validation patching metric persona_diff: 1.70612\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.58087,\n",
      "            Train patching metric persona_diff: 0.67593,\n",
      "            Validation patching metric seq_diff: 1.14360,\n",
      "            Validation patching metric persona_diff: 2.46355\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.14996,\n",
      "            Train patching metric persona_diff: 1.83656,\n",
      "            Validation patching metric seq_diff: 4.81988,\n",
      "            Validation patching metric persona_diff: 1.30672\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.21183,\n",
      "            Train patching metric persona_diff: 0.48148,\n",
      "            Validation patching metric seq_diff: 1.89870,\n",
      "            Validation patching metric persona_diff: 0.29296\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.90228,\n",
      "            Train patching metric persona_diff: 0.26457,\n",
      "            Validation patching metric seq_diff: 2.94881,\n",
      "            Validation patching metric persona_diff: 2.50036\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 0.95541,\n",
      "            Train patching metric persona_diff: 1.56403,\n",
      "            Validation patching metric seq_diff: 0.86407,\n",
      "            Validation patching metric persona_diff: 2.18892\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.72076,\n",
      "            Train patching metric persona_diff: 0.48015,\n",
      "            Validation patching metric seq_diff: 1.03806,\n",
      "            Validation patching metric persona_diff: 0.29240\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.40752,\n",
      "            Train patching metric persona_diff: 0.79063,\n",
      "            Validation patching metric seq_diff: 3.78806,\n",
      "            Validation patching metric persona_diff: 0.40416\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.13178,\n",
      "            Train patching metric persona_diff: 0.87728,\n",
      "            Validation patching metric seq_diff: 1.94154,\n",
      "            Validation patching metric persona_diff: 0.35860\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.60625,\n",
      "            Train patching metric persona_diff: 0.69312,\n",
      "            Validation patching metric seq_diff: 1.40518,\n",
      "            Validation patching metric persona_diff: 0.82794\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 3.88408,\n",
      "            Train patching metric persona_diff: 0.31684,\n",
      "            Validation patching metric seq_diff: 1.59062,\n",
      "            Validation patching metric persona_diff: 0.25145\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.00548,\n",
      "            Train patching metric persona_diff: 0.43199,\n",
      "            Validation patching metric seq_diff: 1.63093,\n",
      "            Validation patching metric persona_diff: 0.35820\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.72820,\n",
      "            Train patching metric persona_diff: 0.32302,\n",
      "            Validation patching metric seq_diff: 2.30672,\n",
      "            Validation patching metric persona_diff: 0.26498\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.46587,\n",
      "            Train patching metric persona_diff: 0.41945,\n",
      "            Validation patching metric seq_diff: 0.40813,\n",
      "            Validation patching metric persona_diff: 0.64479\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.05992,\n",
      "            Train patching metric persona_diff: 1.75326,\n",
      "            Validation patching metric seq_diff: 0.68726,\n",
      "            Validation patching metric persona_diff: 0.28514\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.10024,\n",
      "            Train patching metric persona_diff: 0.18769,\n",
      "            Validation patching metric seq_diff: 2.43921,\n",
      "            Validation patching metric persona_diff: 1.21401\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.48050,\n",
      "            Train patching metric persona_diff: 0.53864,\n",
      "            Validation patching metric seq_diff: 0.77795,\n",
      "            Validation patching metric persona_diff: 0.61806\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.60712,\n",
      "            Train patching metric persona_diff: 0.47559,\n",
      "            Validation patching metric seq_diff: 2.74960,\n",
      "            Validation patching metric persona_diff: 0.58909\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 0.55749,\n",
      "            Train patching metric persona_diff: 0.48052,\n",
      "            Validation patching metric seq_diff: 1.63562,\n",
      "            Validation patching metric persona_diff: 0.32111\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.96868,\n",
      "            Train patching metric persona_diff: 0.40489,\n",
      "            Validation patching metric seq_diff: 1.78942,\n",
      "            Validation patching metric persona_diff: 0.48455\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.06454,\n",
      "            Train patching metric persona_diff: 0.49583,\n",
      "            Validation patching metric seq_diff: 1.27496,\n",
      "            Validation patching metric persona_diff: 0.37968\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.25024,\n",
      "            Train patching metric persona_diff: 0.10183,\n",
      "            Validation patching metric seq_diff: 0.42731,\n",
      "            Validation patching metric persona_diff: 0.19118\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.67872,\n",
      "            Train patching metric persona_diff: 0.60901,\n",
      "            Validation patching metric seq_diff: 2.80200,\n",
      "            Validation patching metric persona_diff: 0.25110\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.14690,\n",
      "            Train patching metric persona_diff: 1.57942,\n",
      "            Validation patching metric seq_diff: 1.29605,\n",
      "            Validation patching metric persona_diff: 0.34086\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 2.00417,\n",
      "            Train patching metric persona_diff: 1.65235,\n",
      "            Validation patching metric seq_diff: 0.77468,\n",
      "            Validation patching metric persona_diff: 0.36459\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 1.78461,\n",
      "            Train patching metric persona_diff: 0.25964,\n",
      "            Validation patching metric seq_diff: 1.33770,\n",
      "            Validation patching metric persona_diff: 0.10413\n",
      "            \n",
      "\n",
      "            Train patching metric seq_diff: 0.75949,\n",
      "            Train patching metric persona_diff: 0.54215,\n",
      "            Validation patching metric seq_diff: 2.41975,\n",
      "            Validation patching metric persona_diff: 0.41661\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "acc_step_batch_size=8\n",
    "n_epochs=32\n",
    "learning_rate = 1e-2\n",
    "subspace_dim=1\n",
    "layer=25\n",
    "\n",
    "train_size = int(0.8 * len(contrast_dataset))  # set 80% for training\n",
    "test_size = len(contrast_dataset) - train_size # 20% for testing\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(contrast_dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for the training and testing datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=acc_step_batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=acc_step_batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "train_dataloader = itertools.cycle(train_dataloader)\n",
    "test_dataloader = itertools.cycle(test_dataloader)\n",
    "\n",
    "toxicity_score = train_linear_rep(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    n_dim=subspace_dim,\n",
    "    learning_rate=learning_rate,\n",
    "    layer=layer,\n",
    "    pos=-1,\n",
    "    invariant_seq=False,\n",
    "    invariant_persona=True,\n",
    "    n_epochs=n_epochs,\n",
    "    acc_step_batch_size=acc_step_batch_size,\n",
    "    acc_iters=batch_size//acc_step_batch_size,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Toxicity Score Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Pos Attributions\n",
      "(1) clean\n",
      "(2) clean\n",
      "(3) C\n",
      "(4) cleaner\n",
      "(5) cle\n",
      "(6) cle\n",
      "(7) grat\n",
      "(8) helpful\n",
      "(9) ván\n",
      "(10) lub\n",
      "\n",
      "Top Neg Attributions\n",
      "(1) TO\n",
      "(2) Toy\n",
      "(3) TO\n",
      "(4) TODO\n",
      "(5) Tob\n",
      "(6) indirect\n",
      "(7) secondary\n",
      "(8) toler\n",
      "(9) heav\n",
      "(10) Survey\n"
     ]
    }
   ],
   "source": [
    "direct = model.W_U.T.to(torch.float32) @ toxicity_score[0].subspace[0, :].to(torch.float32)\n",
    "direct_indices = direct.sort().indices\n",
    "\n",
    "print(\"Top Pos Attributions\")\n",
    "for i in range(10):\n",
    "    print(f\"({i+1}) {model.tokenizer.decode([direct_indices[i]])}\")\n",
    "\n",
    "print(\"\\nTop Neg Attributions\")\n",
    "for i in range(10):\n",
    "    print(f\"({i+1}) {model.tokenizer.decode([direct_indices[direct_indices.shape[0]- i -1]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "dataset_path = \"data/simple_toxic_data_filtered.jsonl\" #\"data/simple_toxic_data_filtered.jsonl\"\n",
    "our_task = Task(model, classifier_prompt, personas, possible_labels)\n",
    "\n",
    "\n",
    "eval_results = our_task.evaluate_personas_over_dataset(dataset_path, max_samples=None, version=\"v1.1\")\n",
    "\n",
    "persona = \"lenient\"\n",
    "all_toxic_examples = [our_task.personas[persona]+ex for i, ex in enumerate(eval_results[\"toxic\"][\"example\"]) if eval_results[\"toxic\"][persona][i] == \"TOXIC\"]\n",
    "all_clean_examples = [our_task.personas[persona]+ex for i, ex in enumerate(eval_results[\"clean\"][\"example\"]) if eval_results[\"clean\"][persona][i] == \"CLEAN\"]\n",
    "\n",
    "\n",
    "if len(all_toxic_examples) > len(all_clean_examples):\n",
    "    all_toxic_examples = all_toxic_examples[:len(all_clean_examples)]\n",
    "else:\n",
    "    all_clean_examples = all_clean_examples[:len(all_toxic_examples)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, indices = tokenize_examples(all_toxic_examples+all_clean_examples, model)\n",
    "\n",
    "toxic_tokens, toxic_indices = tokens[:tokens.shape[0]//2], indices[:tokens.shape[0]//2]\n",
    "clean_tokens, clean_indices = tokens[tokens.shape[0]//2:], indices[tokens.shape[0]//2:]\n",
    "\n",
    "\n",
    "batch_toxic_tokens = toxic_tokens[0:8]\n",
    "batch_clean_tokens = clean_tokens[0:8]\n",
    "batch_toxic_indices = toxic_indices[0:8]\n",
    "batch_clean_indices = clean_indices[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]\n",
    "with torch.no_grad():\n",
    "    toxic_logits, toxic_acts = model.run_with_cache(batch_toxic_tokens, names_filter=names_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]\n",
    "with torch.no_grad():\n",
    "    clean_logits, clean_acts = model.run_with_cache(batch_clean_tokens, names_filter=names_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4986, 4986, 4986, 4986, 4986, 4986, 4986, 4986], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_logits[torch.arange(8), batch_toxic_indices].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29907, 29907, 29907, 29907, 29907, 29907, 29907, 29907],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits[torch.arange(8), batch_clean_indices].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "temp_hook = functools.partial(\n",
    "    patching_hook,\n",
    "    acts_idx=batch_toxic_indices,\n",
    "    new_acts=clean_acts[names_filter[0]],\n",
    "    new_acts_idx=batch_clean_indices,\n",
    "    das=toxicity_score[0]\n",
    ")\n",
    "model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "\n",
    "with torch.no_grad(), torch.autocast(device_type=\"cuda\"):\n",
    "    patched_logits, _ = model.run_with_cache(batch_toxic_tokens, names_filter=names_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29907, 29907, 29907, 29907, 29907, 29907, 29907, 29907],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_logits[torch.arange(8), batch_toxic_indices].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip Success: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "persona = \"lenient\"\n",
    "all_toxic_examples = [our_task.personas[persona]+ex for i, ex in enumerate(eval_results[\"toxic\"][\"example\"]) if eval_results[\"toxic\"][persona][i] == \"TOXIC\"]\n",
    "all_clean_examples = [our_task.personas[persona]+ex for i, ex in enumerate(eval_results[\"clean\"][\"example\"]) if eval_results[\"clean\"][persona][i] == \"CLEAN\"]\n",
    "\n",
    "\n",
    "if len(all_toxic_examples) > len(all_clean_examples):\n",
    "    all_toxic_examples = all_toxic_examples[:len(all_clean_examples)]\n",
    "else:\n",
    "    all_clean_examples = all_clean_examples[:len(all_toxic_examples)]\n",
    "\n",
    "\n",
    "tokens, indices = tokenize_examples(all_toxic_examples+all_clean_examples, model)\n",
    "\n",
    "toxic_tokens, toxic_indices = tokens[:tokens.shape[0]//2], indices[:tokens.shape[0]//2]\n",
    "clean_tokens, clean_indices = tokens[tokens.shape[0]//2:], indices[tokens.shape[0]//2:]\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]\n",
    "\n",
    "\n",
    "total_diff = 0\n",
    "total_diff_flipped = 0\n",
    "recovered_ld = 0\n",
    "\n",
    "for start_idx in tqdm(range(0, toxic_tokens.shape[0], batch_size)):\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    # Index the tokens\n",
    "    batch_toxic_tokens = toxic_tokens[start_idx:start_idx+batch_size]\n",
    "    batch_clean_tokens = clean_tokens[start_idx:start_idx+batch_size]\n",
    "    batch_toxic_indices = toxic_indices[start_idx:start_idx+batch_size]\n",
    "    batch_clean_indices = clean_indices[start_idx:start_idx+batch_size]\n",
    "\n",
    "    # Get cached acts\n",
    "    with torch.no_grad():\n",
    "        toxic_logits, toxic_acts = model.run_with_cache(batch_toxic_tokens, names_filter=names_filter)\n",
    "        clean_logits, clean_acts = model.run_with_cache(batch_clean_tokens, names_filter=names_filter)\n",
    "    \n",
    "    # Get patched logits\n",
    "    temp_hook = functools.partial(\n",
    "        patching_hook,\n",
    "        acts_idx=batch_toxic_indices,\n",
    "        new_acts=clean_acts[names_filter[0]],\n",
    "        new_acts_idx=batch_clean_indices,\n",
    "        das=toxicity_score[0]\n",
    "    )\n",
    "    model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(device_type=\"cuda\"):\n",
    "        patched_logits, _ = model.run_with_cache(batch_toxic_tokens, names_filter=names_filter)\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    # Do analysis\n",
    "    toxic_preds = toxic_logits[torch.arange(batch_toxic_indices.shape[0]), batch_toxic_indices].argmax(-1)\n",
    "    clean_preds = clean_logits[torch.arange(batch_toxic_indices.shape[0]), batch_clean_indices].argmax(-1)\n",
    "    patch_preds = patched_logits[torch.arange(batch_toxic_indices.shape[0]), batch_toxic_indices].argmax(-1)\n",
    "    \n",
    "    diff_inds = toxic_preds != clean_preds\n",
    "    total_diff += diff_inds.sum()\n",
    "    total_diff_flipped += (patch_preds[diff_inds] == clean_preds[diff_inds]).sum()\n",
    "    \n",
    "\n",
    "print(f\"Flip Success: {total_diff_flipped/total_diff*100:.2f}%\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"harsh\"\n",
    "all_toxic_examples = [our_task.personas[persona]+ex for i, ex in enumerate(eval_results[\"toxic\"][\"example\"]) if eval_results[\"toxic\"][persona][i] == \"TOXIC\"]\n",
    "all_clean_examples = [our_task.personas[persona]+ex for i, ex in enumerate(eval_results[\"clean\"][\"example\"]) if eval_results[\"clean\"][persona][i] == \"CLEAN\"]\n",
    "\n",
    "\n",
    "if len(all_toxic_examples) > len(all_clean_examples):\n",
    "    all_toxic_examples = all_toxic_examples[:len(all_clean_examples)]\n",
    "else:\n",
    "    all_clean_examples = all_clean_examples[:len(all_toxic_examples)]\n",
    "\n",
    "\n",
    "tokens, indices = tokenize_examples(all_toxic_examples+all_clean_examples, model)\n",
    "\n",
    "toxic_tokens, toxic_indices = tokens[:tokens.shape[0]//2], indices[:tokens.shape[0]//2]\n",
    "clean_tokens, clean_indices = tokens[tokens.shape[0]//2:], indices[tokens.shape[0]//2:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]\n",
    "\n",
    "\n",
    "total_diff = 0\n",
    "total_diff_flipped = 0\n",
    "recovered_ld = 0\n",
    "\n",
    "for start_idx in tqdm(range(0, toxic_tokens.shape[0], batch_size)):\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    # Index the tokens\n",
    "    batch_toxic_tokens = toxic_tokens[start_idx:start_idx+batch_size]\n",
    "    batch_clean_tokens = clean_tokens[start_idx:start_idx+batch_size]\n",
    "    batch_toxic_indices = toxic_indices[start_idx:start_idx+batch_size]\n",
    "    batch_clean_indices = clean_indices[start_idx:start_idx+batch_size]\n",
    "\n",
    "    # Get cached acts\n",
    "    with torch.no_grad():\n",
    "        toxic_logits, toxic_acts = model.run_with_cache(batch_toxic_tokens, names_filter=names_filter)\n",
    "        clean_logits, clean_acts = model.run_with_cache(batch_clean_tokens, names_filter=names_filter)\n",
    "    \n",
    "    # Get patched logits\n",
    "    temp_hook = functools.partial(\n",
    "        patching_hook,\n",
    "        acts_idx=batch_toxic_indices,\n",
    "        new_acts=clean_acts[names_filter[0]],\n",
    "        new_acts_idx=batch_clean_indices,\n",
    "        das=toxicity_score[0]\n",
    "    )\n",
    "    model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(device_type=\"cuda\"):\n",
    "        patched_logits, _ = model.run_with_cache(batch_toxic_tokens, names_filter=names_filter)\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    # Do analysis\n",
    "    toxic_preds = toxic_logits[torch.arange(batch_toxic_indices.shape[0]), batch_toxic_indices].argmax(-1)\n",
    "    clean_preds = clean_logits[torch.arange(batch_toxic_indices.shape[0]), batch_clean_indices].argmax(-1)\n",
    "    patch_preds = patched_logits[torch.arange(batch_toxic_indices.shape[0]), batch_toxic_indices].argmax(-1)\n",
    "    \n",
    "    diff_inds = toxic_preds != clean_preds\n",
    "    total_diff += diff_inds.sum()\n",
    "    total_diff_flipped += (patch_preds[diff_inds] == clean_preds[diff_inds]).sum()\n",
    "    \n",
    "\n",
    "print(f\"Flip Success: {total_diff_flipped/total_diff*100:.2f}%\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
