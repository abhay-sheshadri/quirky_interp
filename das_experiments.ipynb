{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.das_helpers import *\n",
    "from src import *\n",
    "\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_size = \"7b\"\n",
    "llama_size = \"13b\"\n",
    "use_base_model = True\n",
    "\n",
    "dataset_path = \"data/simple_toxic_data_filtered.jsonl\" #\"data/simple_toxic_data_filtered.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = {\n",
    "    \"lenient\": \"You are somewhat lenient on toxicity\\n\",\n",
    "    \"harsh\": \"You are somewhat harsh on toxicity\\n\"\n",
    "}\n",
    "\n",
    "possible_labels = (\"CLEAN\", \"TOXIC\")\n",
    "\n",
    "# classifier_prompt = \"\"\"\n",
    "# You must classify the toxicity of text, returning either (TOXIC) or (CLEAN) for any sequence I give you. \n",
    "\n",
    "# Here is the text:\n",
    "# {sequence}\n",
    "\n",
    "# Your response:\n",
    "# (\"\"\"\n",
    "classifier_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0998ae3788742a291057f1289a97441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-13b into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "if not use_base_model:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-chat-hf\"\n",
    "else:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-hf\"\n",
    "\n",
    "hf_model, hf_tokenizer = load_model_from_transformers(model_name_or_path)\n",
    "model = from_hf_to_tlens(hf_model, hf_tokenizer, f\"llama-{llama_size}\")\n",
    "\n",
    "if use_base_model:\n",
    "    model.cfg.model_name = model.cfg.model_name + \"_base\"\n",
    "\n",
    "our_task = Task(model, classifier_prompt, personas, possible_labels)\n",
    "contrast_dataset = ConstrastTriplesDataset(model, our_task, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are somewhat harsh on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "My neighbor's lawn looks bad because they don't water it\n",
      "\n",
      "Your response:\n",
      "(\n",
      "\n",
      "\n",
      "You are somewhat lenient on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "My neighbor's lawn looks bad because they don't water it\n",
      "\n",
      "Your response:\n",
      "(\n",
      "\n",
      "\n",
      "You are somewhat harsh on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "The weather was nice yesterday because the sun was shining\n",
      "\n",
      "Your response:\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "print(f'{contrast_dataset.samples[\"clean\"][0]}\\n\\n\\n{contrast_dataset.samples[\"persona_diff\"][0]}\\n\\n\\n{contrast_dataset.samples[\"seq_diff\"][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DAS for Toxicity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "acc_step_batch_size=8\n",
    "n_epochs=500\n",
    "learning_rate=5e-2\n",
    "subspace_dim=1\n",
    "layer=25\n",
    " \n",
    "dataloader = DataLoader(contrast_dataset, batch_size=acc_step_batch_size, shuffle=True, drop_last=True)\n",
    "dataloader = itertools.cycle(dataloader)\n",
    "toxicity_score = DistributedAlignmentSearch(model.cfg.d_model, 2).cuda()\n",
    "optimizer = torch.optim.AdamW(toxicity_score.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in model.parameters():\n",
    "    model.requires_grad_(False)\n",
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching seq metric: 1.01526, Patching persona metric: 0.00327\n",
      "tensor([[-0.0124, -0.0092,  0.0146,  ..., -0.0108, -0.0159, -0.0145],\n",
      "        [-0.0129, -0.0149,  0.0143,  ...,  0.0075, -0.0143,  0.0114]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.09934, Patching persona metric: 0.15040\n",
      "tensor([[-0.0205, -0.0003,  0.0175,  ...,  0.0076,  0.0035,  0.0073],\n",
      "        [-0.0103,  0.0104,  0.0099,  ...,  0.0157,  0.0070,  0.0174]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.07486, Patching persona metric: 0.17103\n",
      "tensor([[-0.0109,  0.0032,  0.0041,  ...,  0.0014,  0.0051,  0.0024],\n",
      "        [ 0.0103,  0.0172, -0.0005,  ...,  0.0132,  0.0162,  0.0088]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.06573, Patching persona metric: 3.24840\n",
      "tensor([[-0.0186, -0.0116,  0.0020,  ..., -0.0100, -0.0096, -0.0029],\n",
      "        [-0.0040, -0.0022,  0.0214,  ..., -0.0073, -0.0025, -0.0135]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.02963, Patching persona metric: 0.46940\n",
      "tensor([[-0.0112, -0.0042,  0.0015,  ..., -0.0034, -0.0037, -0.0008],\n",
      "        [-0.0094, -0.0093,  0.0242,  ..., -0.0062, -0.0086, -0.0111]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.56097, Patching persona metric: 0.55619\n",
      "tensor([[-0.0175, -0.0095,  0.0135,  ..., -0.0009, -0.0101,  0.0107],\n",
      "        [-0.0040, -0.0075,  0.0141,  ..., -0.0086, -0.0060, -0.0246]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.28650, Patching persona metric: 0.60004\n",
      "tensor([[-0.0070,  0.0016,  0.0251,  ...,  0.0119,  0.0016,  0.0314],\n",
      "        [-0.0070, -0.0083,  0.0176,  ..., -0.0010, -0.0053, -0.0169]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.31641, Patching persona metric: 0.19090\n",
      "tensor([[ 0.0043,  0.0098,  0.0201,  ...,  0.0083,  0.0095,  0.0341],\n",
      "        [-0.0088, -0.0084,  0.0213,  ..., -0.0013, -0.0049, -0.0048]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.11330, Patching persona metric: 0.05359\n",
      "tensor([[ 0.0089,  0.0122,  0.0178,  ...,  0.0052,  0.0119,  0.0336],\n",
      "        [-0.0094, -0.0082,  0.0235,  ..., -0.0016, -0.0049,  0.0056]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.15986, Patching persona metric: 0.13797\n",
      "tensor([[ 0.0044,  0.0077,  0.0233,  ...,  0.0015,  0.0077,  0.0371],\n",
      "        [-0.0093, -0.0096,  0.0244,  ...,  0.0033, -0.0056,  0.0063]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.04484, Patching persona metric: 0.05801\n",
      "tensor([[ 0.0051,  0.0073,  0.0208,  ..., -0.0017,  0.0074,  0.0337],\n",
      "        [-0.0094, -0.0096,  0.0254,  ...,  0.0063, -0.0063,  0.0098]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.07554, Patching persona metric: 0.07731\n",
      "tensor([[ 0.0093,  0.0095,  0.0133,  ..., -0.0063,  0.0094,  0.0265],\n",
      "        [-0.0096, -0.0090,  0.0246,  ...,  0.0061, -0.0068,  0.0123]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.04764, Patching persona metric: 0.06412\n",
      "tensor([[ 0.0097,  0.0094,  0.0099,  ..., -0.0125,  0.0086,  0.0226],\n",
      "        [-0.0093, -0.0085,  0.0241,  ...,  0.0089, -0.0067,  0.0130]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.06012, Patching persona metric: 0.05051\n",
      "tensor([[ 0.0079,  0.0078,  0.0087,  ..., -0.0185,  0.0060,  0.0197],\n",
      "        [-0.0083, -0.0079,  0.0246,  ...,  0.0133, -0.0057,  0.0147]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.02085, Patching persona metric: 0.01932\n",
      "tensor([[ 0.0064,  0.0067,  0.0070,  ..., -0.0229,  0.0037,  0.0166],\n",
      "        [-0.0079, -0.0077,  0.0251,  ...,  0.0137, -0.0049,  0.0162]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.03906, Patching persona metric: 0.02444\n",
      "tensor([[ 0.0056,  0.0063,  0.0051,  ..., -0.0267,  0.0022,  0.0136],\n",
      "        [-0.0085, -0.0081,  0.0239,  ...,  0.0125, -0.0053,  0.0159]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.04330, Patching persona metric: 0.01730\n",
      "tensor([[ 0.0049,  0.0060,  0.0036,  ..., -0.0292,  0.0011,  0.0110],\n",
      "        [-0.0100, -0.0090,  0.0217,  ...,  0.0110, -0.0063,  0.0138]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.02798, Patching persona metric: 0.00598\n",
      "tensor([[ 0.0043,  0.0056,  0.0027,  ..., -0.0314,  0.0002,  0.0092],\n",
      "        [-0.0111, -0.0095,  0.0188,  ...,  0.0108, -0.0069,  0.0115]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.01797, Patching persona metric: 0.00546\n",
      "tensor([[ 0.0039,  0.0052,  0.0019,  ..., -0.0334, -0.0006,  0.0076],\n",
      "        [-0.0116, -0.0096,  0.0172,  ...,  0.0116, -0.0068,  0.0111]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.03215, Patching persona metric: 0.00922\n",
      "tensor([[ 0.0035,  0.0049,  0.0015,  ..., -0.0350, -0.0013,  0.0060],\n",
      "        [-0.0118, -0.0097,  0.0157,  ...,  0.0133, -0.0066,  0.0116]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.04148, Patching persona metric: 0.03867\n",
      "tensor([[ 0.0029,  0.0046,  0.0013,  ..., -0.0355, -0.0019,  0.0043],\n",
      "        [-0.0120, -0.0098,  0.0148,  ...,  0.0135, -0.0065,  0.0125]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.04026, Patching persona metric: 0.00391\n",
      "tensor([[ 0.0024,  0.0044,  0.0017,  ..., -0.0353, -0.0024,  0.0029],\n",
      "        [-0.0123, -0.0101,  0.0143,  ...,  0.0128, -0.0065,  0.0133]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.00912, Patching persona metric: 0.00894\n",
      "tensor([[ 0.0019,  0.0041,  0.0022,  ..., -0.0355, -0.0029,  0.0020],\n",
      "        [-0.0125, -0.0103,  0.0134,  ...,  0.0132, -0.0065,  0.0134]],\n",
      "       device='cuda:0')\n",
      "Patching seq metric: 0.03654, Patching persona metric: 0.01425\n",
      "tensor([[ 0.0015,  0.0039,  0.0030,  ..., -0.0353, -0.0032,  0.0013],\n",
      "        [-0.0127, -0.0104,  0.0125,  ...,  0.0143, -0.0066,  0.0133]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     persona_diff_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersona_diff_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     22\u001b[0m     persona_diff_indices \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersona_diff_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m     persona_diff_logits, persona_diff_acts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersona_diff_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     persona_diff_logits \u001b[38;5;241m=\u001b[39m persona_diff_logits[torch\u001b[38;5;241m.\u001b[39marange(acc_step_batch_size), persona_diff_indices]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Do hooked forward pass with seq_diff\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:641\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    626\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    634\u001b[0m ]:\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    645\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    646\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    647\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/hook_points.py:467\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    458\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    462\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    463\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    464\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    465\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[0;32m--> 467\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    469\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:562\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    559\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    560\u001b[0m         )\n\u001b[0;32m--> 562\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:1455\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1440\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     )\n\u001b[1;32m   1453\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[0;32m-> 1455\u001b[0m     resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_resid_mid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresid_pre\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_out\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m     mlp_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1459\u001b[0m         resid_mid\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in\n\u001b[1;32m   1461\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_mid\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m   1462\u001b[0m     )\n\u001b[1;32m   1463\u001b[0m     normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(mlp_in)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for _ in range(batch_size//acc_step_batch_size):\n",
    "        model.reset_hooks()\n",
    "        batch = next(dataloader)\n",
    "        with torch.no_grad():\n",
    "            # Compute clean logits and acts\n",
    "            clean_tokens = batch[\"clean_tokens\"].cuda()\n",
    "            clean_indices = batch[\"clean_indices\"]\n",
    "            clean_logits, clean_acts = model.run_with_cache(clean_tokens, names_filter=names_filter)\n",
    "            clean_logits = clean_logits[torch.arange(acc_step_batch_size), clean_indices]\n",
    "            \n",
    "            # Compute seq_diff logits and acts\n",
    "            seq_diff_tokens = batch[\"seq_diff_tokens\"].cuda()\n",
    "            seq_diff_indices = batch[\"seq_diff_indices\"]\n",
    "            seq_diff_logits, seq_diff_acts = model.run_with_cache(seq_diff_tokens, names_filter=names_filter)\n",
    "            seq_diff_logits = seq_diff_logits[torch.arange(acc_step_batch_size), seq_diff_indices]\n",
    "\n",
    "            # Compute persona_diff logits and acts\n",
    "            persona_diff_tokens = batch[\"persona_diff_tokens\"].cuda()\n",
    "            persona_diff_indices = batch[\"persona_diff_indices\"]\n",
    "            persona_diff_logits, persona_diff_acts = model.run_with_cache(persona_diff_tokens, names_filter=names_filter)\n",
    "            persona_diff_logits = persona_diff_logits[torch.arange(acc_step_batch_size), persona_diff_indices]\n",
    "        \n",
    "        \n",
    "        # Do hooked forward pass with seq_diff\n",
    "        model.reset_hooks()\n",
    "        temp_hook = functools.partial(\n",
    "            patching_hook,\n",
    "            acts_idx=clean_indices,\n",
    "            new_acts=seq_diff_acts[names_filter[0]],\n",
    "            new_acts_idx=seq_diff_indices,\n",
    "            das=toxicity_score\n",
    "        )\n",
    "        model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            patched_seq_diff_logits = model(clean_tokens)\n",
    "        patched_seq_diff_logits = patched_seq_diff_logits[torch.arange(acc_step_batch_size), clean_indices]\n",
    "        loss1 = patching_metric(patched_seq_diff_logits, seq_diff_logits)\n",
    "        loss1.backward()\n",
    "        \n",
    "        \n",
    "        # Do hooked forward pass with persona_diff\n",
    "        model.reset_hooks()\n",
    "        temp_hook = functools.partial(\n",
    "            patching_hook,\n",
    "            acts_idx=clean_indices,\n",
    "            new_acts=persona_diff_acts[names_filter[0]],\n",
    "            new_acts_idx=persona_diff_indices,\n",
    "            das=toxicity_score\n",
    "        )\n",
    "        model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            patched_persona_diff_logits = model(clean_tokens)\n",
    "        patched_persona_diff_logits = patched_persona_diff_logits[torch.arange(acc_step_batch_size), clean_indices]\n",
    "        loss2 = patching_metric(patched_persona_diff_logits, clean_logits)\n",
    "        loss2.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    toxicity_score.gram_schmidt_orthogonalization()\n",
    "    print(f\"Patching seq metric: {loss1.item():.5f}, Patching persona metric: {loss2.item():.5f}\")\n",
    "    #print(\"Subspace basis:\", toxicity_score.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DAS For Persona Rep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "acc_step_batch_size=8\n",
    "n_epochs=500\n",
    "learning_rate=5e-2\n",
    "subspace_dim=1\n",
    "layer=25\n",
    "\n",
    "dataloader = DataLoader(contrast_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader = itertools.cycle(dataloader)\n",
    "judgement_rep = DistributedAlignmentSearch(model.cfg.d_model, 1).cuda()\n",
    "optimizer = torch.optim.AdamW(toxicity_score.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in model.parameters():\n",
    "    model.requires_grad_(False)\n",
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching seq metric: 0.00474, Patching persona metric: 0.35074\n",
      "Patching seq metric: 0.00275, Patching persona metric: 0.34866\n",
      "Patching seq metric: 0.00208, Patching persona metric: 0.34935\n",
      "Patching seq metric: 0.00057, Patching persona metric: 0.32886\n",
      "Patching seq metric: 0.00461, Patching persona metric: 0.40073\n",
      "Patching seq metric: 0.00546, Patching persona metric: 0.31618\n",
      "Patching seq metric: 0.00562, Patching persona metric: 0.43973\n",
      "Patching seq metric: 0.00244, Patching persona metric: 0.32825\n",
      "Patching seq metric: 0.00308, Patching persona metric: 0.45989\n",
      "Patching seq metric: 0.00344, Patching persona metric: 0.49873\n",
      "Patching seq metric: 0.00523, Patching persona metric: 0.31924\n",
      "Patching seq metric: 0.00427, Patching persona metric: 0.39850\n",
      "Patching seq metric: 0.00248, Patching persona metric: 0.39763\n",
      "Patching seq metric: 0.00373, Patching persona metric: 0.45140\n",
      "Patching seq metric: 0.00333, Patching persona metric: 0.38701\n",
      "Patching seq metric: 0.00449, Patching persona metric: 0.41430\n",
      "Patching seq metric: 0.00354, Patching persona metric: 0.36107\n",
      "Patching seq metric: 0.00324, Patching persona metric: 0.23963\n",
      "Patching seq metric: 0.00415, Patching persona metric: 0.34110\n",
      "Patching seq metric: 0.00207, Patching persona metric: 0.47330\n",
      "Patching seq metric: 0.00327, Patching persona metric: 0.30483\n",
      "Patching seq metric: 0.00510, Patching persona metric: 0.31186\n",
      "Patching seq metric: 0.00095, Patching persona metric: 0.34437\n",
      "Patching seq metric: 0.00572, Patching persona metric: 0.39700\n",
      "Patching seq metric: 0.00649, Patching persona metric: 0.45274\n",
      "Patching seq metric: 0.00594, Patching persona metric: 0.44769\n",
      "Patching seq metric: 0.00659, Patching persona metric: 0.37097\n",
      "Patching seq metric: 0.00510, Patching persona metric: 0.42872\n",
      "Patching seq metric: 0.00309, Patching persona metric: 0.32286\n",
      "Patching seq metric: 0.00893, Patching persona metric: 0.35137\n",
      "Patching seq metric: 0.00099, Patching persona metric: 0.35892\n",
      "Patching seq metric: 0.00476, Patching persona metric: 0.31446\n",
      "Patching seq metric: 0.00538, Patching persona metric: 0.38794\n",
      "Patching seq metric: 0.00395, Patching persona metric: 0.38632\n",
      "Patching seq metric: 0.00244, Patching persona metric: 0.35639\n",
      "Patching seq metric: 0.00539, Patching persona metric: 0.38132\n",
      "Patching seq metric: 0.00369, Patching persona metric: 0.41924\n",
      "Patching seq metric: 0.00238, Patching persona metric: 0.31174\n",
      "Patching seq metric: 0.00418, Patching persona metric: 0.36875\n",
      "Patching seq metric: 0.00650, Patching persona metric: 0.42813\n",
      "Patching seq metric: 0.00247, Patching persona metric: 0.34958\n",
      "Patching seq metric: 0.00183, Patching persona metric: 0.43768\n",
      "Patching seq metric: 0.00751, Patching persona metric: 0.44902\n",
      "Patching seq metric: 0.00842, Patching persona metric: 0.40823\n",
      "Patching seq metric: 0.00129, Patching persona metric: 0.36005\n",
      "Patching seq metric: 0.00230, Patching persona metric: 0.37488\n",
      "Patching seq metric: 0.00351, Patching persona metric: 0.42926\n",
      "Patching seq metric: 0.00108, Patching persona metric: 0.39050\n"
     ]
    }
   ],
   "source": [
    "for _ in range(n_epochs):\n",
    "    model.reset_hooks()\n",
    "    batch = next(dataloader)\n",
    "    with torch.no_grad():\n",
    "        # Compute clean logits and acts\n",
    "        clean_tokens = batch[\"clean_tokens\"].cuda()\n",
    "        clean_indices = batch[\"clean_indices\"]\n",
    "        clean_logits, clean_acts = model.run_with_cache(clean_tokens, names_filter=names_filter)\n",
    "        clean_logits = clean_logits[torch.arange(batch_size), clean_indices]\n",
    "        \n",
    "        # Compute seq_diff logits and acts\n",
    "        seq_diff_tokens = batch[\"seq_diff_tokens\"].cuda()\n",
    "        seq_diff_indices = batch[\"seq_diff_indices\"]\n",
    "        seq_diff_logits, seq_diff_acts = model.run_with_cache(seq_diff_tokens, names_filter=names_filter)\n",
    "        seq_diff_logits = seq_diff_logits[torch.arange(batch_size), seq_diff_indices]\n",
    "\n",
    "        # Compute persona_diff logits and acts\n",
    "        persona_diff_tokens = batch[\"persona_diff_tokens\"].cuda()\n",
    "        persona_diff_indices = batch[\"persona_diff_indices\"]\n",
    "        persona_diff_logits, persona_diff_acts = model.run_with_cache(persona_diff_tokens, names_filter=names_filter)\n",
    "        persona_diff_logits = persona_diff_logits[torch.arange(batch_size), persona_diff_indices]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Do hooked forward pass with seq_diff\n",
    "    model.reset_hooks()\n",
    "    temp_hook = functools.partial(\n",
    "        patching_hook,\n",
    "        acts_idx=clean_indices,\n",
    "        new_acts=seq_diff_acts[names_filter[0]],\n",
    "        new_acts_idx=seq_diff_indices,\n",
    "        das=judgement_rep\n",
    "    )\n",
    "    model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "    patched_seq_diff_logits = model(clean_tokens)\n",
    "    patched_seq_diff_logits = patched_seq_diff_logits[torch.arange(batch_size), clean_indices]\n",
    "    loss1 = patching_metric(patched_seq_diff_logits, clean_logits)\n",
    "    loss1.backward()\n",
    "    \n",
    "    # Do hooked forward pass with persona_diff\n",
    "    model.reset_hooks()\n",
    "    temp_hook = functools.partial(\n",
    "        patching_hook,\n",
    "        acts_idx=clean_indices,\n",
    "        new_acts=persona_diff_acts[names_filter[0]],\n",
    "        new_acts_idx=persona_diff_indices,\n",
    "        das=judgement_rep\n",
    "    )\n",
    "    model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "    patched_persona_diff_logits = model(clean_tokens)\n",
    "    patched_persona_diff_logits = patched_persona_diff_logits[torch.arange(batch_size), clean_indices]\n",
    "    loss2 = patching_metric(patched_persona_diff_logits, persona_diff_logits)\n",
    "    loss2.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    judgement_rep.gram_schmidt_orthogonalization()\n",
    "    print(f\"Patching seq metric: {loss1.item():.5f}, Patching persona metric: {loss2.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Representation Meiosis Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fine-grained structure of the universe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
