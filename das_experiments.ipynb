{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.das_helpers import *\n",
    "from src import *\n",
    "\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_size = \"7b\"\n",
    "llama_size = \"13b\"\n",
    "use_base_model = True\n",
    "\n",
    "dataset_path = \"data/simple_toxic_data_filtered.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = {\n",
    "    \"lenient\": \"You are lenient on toxicity\\n\",\n",
    "    \"harsh\": \"You are harsh on toxicity\\n\"\n",
    "}\n",
    "\n",
    "possible_labels = (\"CLEAN\", \"TOXIC\")\n",
    "\n",
    "# classifier_prompt = \"\"\"\n",
    "# You must classify the toxicity of text, returning either (TOXIC) or (CLEAN) for any sequence I give you. \n",
    "\n",
    "# Here is the text:\n",
    "# {sequence}\n",
    "\n",
    "# Your response:\n",
    "# (\"\"\"\n",
    "classifier_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8397d58e243458bab4f3ca745711834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not use_base_model:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-chat-hf\"\n",
    "else:\n",
    "    model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-hf\"\n",
    "\n",
    "hf_model, hf_tokenizer = load_model_from_transformers(model_name_or_path)\n",
    "model = from_hf_to_tlens(hf_model, hf_tokenizer, f\"llama-{llama_size}\")\n",
    "\n",
    "if use_base_model:\n",
    "    model.cfg.model_name = model.cfg.model_name + \"_base\"\n",
    "\n",
    "our_task = Task(model, classifier_prompt, personas, possible_labels)\n",
    "contrast_dataset = ConstrastTriplesDataset(model, our_task, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are lenient on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "That story sucks because it has no point\n",
      "\n",
      "Your response:\n",
      "(\n",
      "\n",
      "\n",
      "You are harsh on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "That story sucks because it has no point\n",
      "\n",
      "Your response:\n",
      "(\n",
      "\n",
      "\n",
      "You are lenient on toxicity\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "This class is awful because it is terrible\n",
      "\n",
      "Your response:\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "print(f'{contrast_dataset.samples[\"clean\"][0]}\\n\\n\\n{contrast_dataset.samples[\"persona_diff\"][0]}\\n\\n\\n{contrast_dataset.samples[\"seq_diff\"][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DAS for Toxicity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "acc_step_batch_size=8\n",
    "n_epochs=100\n",
    "learning_rate=2e-3\n",
    "subspace_dim=1\n",
    "layer = 14\n",
    " \n",
    "dataloader = DataLoader(contrast_dataset, batch_size=acc_step_batch_size, shuffle=True, drop_last=True)\n",
    "dataloader = itertools.cycle(dataloader)\n",
    "toxicity_score = DistributedAlignmentSearch(model.cfg.d_model, subspace_dim).cuda()\n",
    "optimizer = torch.optim.AdamW(toxicity_score.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in model.parameters():\n",
    "    model.requires_grad_(False)\n",
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 60.25 MiB is free. Process 891775 has 32.79 GiB memory in use. Process 1135827 has 46.28 GiB memory in use. Of the allocated memory 44.59 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[layer]\u001b[38;5;241m.\u001b[39mhook_resid_mid\u001b[38;5;241m.\u001b[39madd_hook(temp_hook)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m     patched_seq_diff_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m patched_seq_diff_logits \u001b[38;5;241m=\u001b[39m patched_seq_diff_logits[torch\u001b[38;5;241m.\u001b[39marange(batch_size), clean_indices]\n\u001b[1;32m     40\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m patching_metric(patched_seq_diff_logits, seq_diff_logits)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:562\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    559\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    560\u001b[0m         )\n\u001b[0;32m--> 562\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:1444\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1437\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m   1438\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m   1440\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m-> 1444\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1453\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1455\u001b[0m     resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(\n\u001b[1;32m   1456\u001b[0m         resid_pre \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m   1457\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:533\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    510\u001b[0m     query_input: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     attention_mask: Optional[Int[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch offset_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    shortformer_pos_embed is only used if self.cfg.positional_embedding_type == \"shortformer\", else defaults to None and is irrelevant. See HookedTransformerConfig for more details\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m    attention_mask is the attention mask for padded tokens. Defaults to None.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_qkv_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache_entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;66;03m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n\u001b[1;32m    537\u001b[0m         kv_cache_pos_offset \u001b[38;5;241m=\u001b[39m past_kv_cache_entry\u001b[38;5;241m.\u001b[39mpast_keys\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformer_lens/components.py:665\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_qkv_matrices\u001b[0;34m(self, query_input, key_input, value_input)\u001b[0m\n\u001b[1;32m    646\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_q(\n\u001b[1;32m    647\u001b[0m     einsum(\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqkv_einops_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, head_index d_model d_head \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_Q\n\u001b[1;32m    654\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    655\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_k(\n\u001b[1;32m    656\u001b[0m     einsum(\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqkv_einops_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, head_index d_model d_head \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_K\n\u001b[1;32m    663\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    664\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_v(\n\u001b[0;32m--> 665\u001b[0m     \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mqkv_einops_string\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, head_index d_model d_head \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;43m        -> batch pos head_index d_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_V\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_V\u001b[49m\n\u001b[1;32m    672\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q, k, v\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 60.25 MiB is free. Process 891775 has 32.79 GiB memory in use. Process 1135827 has 46.28 GiB memory in use. Of the allocated memory 44.59 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for _ in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for _ in range(batch_size//acc_step_batch_size):\n",
    "        model.reset_hooks()\n",
    "        batch = next(dataloader)\n",
    "        with torch.no_grad():\n",
    "            # Compute clean logits and acts\n",
    "            clean_tokens = batch[\"clean_tokens\"].cuda()\n",
    "            clean_indices = batch[\"clean_indices\"]\n",
    "            clean_logits, clean_acts = model.run_with_cache(clean_tokens, names_filter=names_filter)\n",
    "            clean_logits = clean_logits[torch.arange(batch_size), clean_indices]\n",
    "            \n",
    "            # Compute seq_diff logits and acts\n",
    "            seq_diff_tokens = batch[\"seq_diff_tokens\"].cuda()\n",
    "            seq_diff_indices = batch[\"seq_diff_indices\"]\n",
    "            seq_diff_logits, seq_diff_acts = model.run_with_cache(seq_diff_tokens, names_filter=names_filter)\n",
    "            seq_diff_logits = seq_diff_logits[torch.arange(batch_size), seq_diff_indices]\n",
    "\n",
    "            # Compute persona_diff logits and acts\n",
    "            persona_diff_tokens = batch[\"persona_diff_tokens\"].cuda()\n",
    "            persona_diff_indices = batch[\"persona_diff_indices\"]\n",
    "            persona_diff_logits, persona_diff_acts = model.run_with_cache(persona_diff_tokens, names_filter=names_filter)\n",
    "            persona_diff_logits = persona_diff_logits[torch.arange(batch_size), persona_diff_indices]\n",
    "        \n",
    "        \n",
    "        # Do hooked forward pass with seq_diff\n",
    "        model.reset_hooks()\n",
    "        temp_hook = functools.partial(\n",
    "            patching_hook,\n",
    "            acts_idx=clean_indices,\n",
    "            new_acts=seq_diff_acts[names_filter[0]],\n",
    "            new_acts_idx=seq_diff_indices,\n",
    "            das=toxicity_score\n",
    "        )\n",
    "        model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            patched_seq_diff_logits = model(clean_tokens)\n",
    "        patched_seq_diff_logits = patched_seq_diff_logits[torch.arange(batch_size), clean_indices]\n",
    "        loss1 = patching_metric(patched_seq_diff_logits, seq_diff_logits)\n",
    "        loss1.backward()\n",
    "        \n",
    "        # Do hooked forward pass with persona_diff\n",
    "        model.reset_hooks()\n",
    "        temp_hook = functools.partial(\n",
    "            patching_hook,\n",
    "            acts_idx=clean_indices,\n",
    "            new_acts=persona_diff_acts[names_filter[0]],\n",
    "            new_acts_idx=persona_diff_indices,\n",
    "            das=toxicity_score\n",
    "        )\n",
    "        model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            patched_persona_diff_logits = model(clean_tokens)\n",
    "        patched_persona_diff_logits = patched_persona_diff_logits[torch.arange(batch_size), clean_indices]\n",
    "        loss2 = patching_metric(patched_persona_diff_logits, clean_logits)\n",
    "        loss2.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    print(f\"Patching seq metric: {loss1.item():.5f}, Patching persona metric: {loss2.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DAS For Persona Rep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "n_epochs=100\n",
    "learning_rate=1e-2\n",
    "subspace_dim=1\n",
    "layer = 16\n",
    " \n",
    "dataloader = DataLoader(contrast_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader = itertools.cycle(dataloader)\n",
    "judgement_rep = DistributedAlignmentSearch(model.cfg.d_model, subspace_dim).cuda()\n",
    "optimizer = torch.optim.AdamW(toxicity_score.parameters(), lr=learning_rate)\n",
    "\n",
    "for param in model.parameters():\n",
    "    model.requires_grad_(False)\n",
    "names_filter = [f\"blocks.{layer}.hook_resid_mid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching seq metric: 0.00474, Patching persona metric: 0.35074\n",
      "Patching seq metric: 0.00275, Patching persona metric: 0.34866\n",
      "Patching seq metric: 0.00208, Patching persona metric: 0.34935\n",
      "Patching seq metric: 0.00057, Patching persona metric: 0.32886\n",
      "Patching seq metric: 0.00461, Patching persona metric: 0.40073\n",
      "Patching seq metric: 0.00546, Patching persona metric: 0.31618\n",
      "Patching seq metric: 0.00562, Patching persona metric: 0.43973\n",
      "Patching seq metric: 0.00244, Patching persona metric: 0.32825\n",
      "Patching seq metric: 0.00308, Patching persona metric: 0.45989\n",
      "Patching seq metric: 0.00344, Patching persona metric: 0.49873\n",
      "Patching seq metric: 0.00523, Patching persona metric: 0.31924\n",
      "Patching seq metric: 0.00427, Patching persona metric: 0.39850\n",
      "Patching seq metric: 0.00248, Patching persona metric: 0.39763\n",
      "Patching seq metric: 0.00373, Patching persona metric: 0.45140\n",
      "Patching seq metric: 0.00333, Patching persona metric: 0.38701\n",
      "Patching seq metric: 0.00449, Patching persona metric: 0.41430\n",
      "Patching seq metric: 0.00354, Patching persona metric: 0.36107\n",
      "Patching seq metric: 0.00324, Patching persona metric: 0.23963\n",
      "Patching seq metric: 0.00415, Patching persona metric: 0.34110\n",
      "Patching seq metric: 0.00207, Patching persona metric: 0.47330\n",
      "Patching seq metric: 0.00327, Patching persona metric: 0.30483\n",
      "Patching seq metric: 0.00510, Patching persona metric: 0.31186\n",
      "Patching seq metric: 0.00095, Patching persona metric: 0.34437\n",
      "Patching seq metric: 0.00572, Patching persona metric: 0.39700\n",
      "Patching seq metric: 0.00649, Patching persona metric: 0.45274\n",
      "Patching seq metric: 0.00594, Patching persona metric: 0.44769\n",
      "Patching seq metric: 0.00659, Patching persona metric: 0.37097\n",
      "Patching seq metric: 0.00510, Patching persona metric: 0.42872\n",
      "Patching seq metric: 0.00309, Patching persona metric: 0.32286\n",
      "Patching seq metric: 0.00893, Patching persona metric: 0.35137\n",
      "Patching seq metric: 0.00099, Patching persona metric: 0.35892\n",
      "Patching seq metric: 0.00476, Patching persona metric: 0.31446\n",
      "Patching seq metric: 0.00538, Patching persona metric: 0.38794\n",
      "Patching seq metric: 0.00395, Patching persona metric: 0.38632\n",
      "Patching seq metric: 0.00244, Patching persona metric: 0.35639\n",
      "Patching seq metric: 0.00539, Patching persona metric: 0.38132\n",
      "Patching seq metric: 0.00369, Patching persona metric: 0.41924\n",
      "Patching seq metric: 0.00238, Patching persona metric: 0.31174\n",
      "Patching seq metric: 0.00418, Patching persona metric: 0.36875\n",
      "Patching seq metric: 0.00650, Patching persona metric: 0.42813\n",
      "Patching seq metric: 0.00247, Patching persona metric: 0.34958\n",
      "Patching seq metric: 0.00183, Patching persona metric: 0.43768\n",
      "Patching seq metric: 0.00751, Patching persona metric: 0.44902\n",
      "Patching seq metric: 0.00842, Patching persona metric: 0.40823\n",
      "Patching seq metric: 0.00129, Patching persona metric: 0.36005\n",
      "Patching seq metric: 0.00230, Patching persona metric: 0.37488\n",
      "Patching seq metric: 0.00351, Patching persona metric: 0.42926\n",
      "Patching seq metric: 0.00108, Patching persona metric: 0.39050\n"
     ]
    }
   ],
   "source": [
    "for _ in range(n_epochs):\n",
    "    model.reset_hooks()\n",
    "    batch = next(dataloader)\n",
    "    with torch.no_grad():\n",
    "        # Compute clean logits and acts\n",
    "        clean_tokens = batch[\"clean_tokens\"].cuda()\n",
    "        clean_indices = batch[\"clean_indices\"]\n",
    "        clean_logits, clean_acts = model.run_with_cache(clean_tokens, names_filter=names_filter)\n",
    "        clean_logits = clean_logits[torch.arange(batch_size), clean_indices]\n",
    "        \n",
    "        # Compute seq_diff logits and acts\n",
    "        seq_diff_tokens = batch[\"seq_diff_tokens\"].cuda()\n",
    "        seq_diff_indices = batch[\"seq_diff_indices\"]\n",
    "        seq_diff_logits, seq_diff_acts = model.run_with_cache(seq_diff_tokens, names_filter=names_filter)\n",
    "        seq_diff_logits = seq_diff_logits[torch.arange(batch_size), seq_diff_indices]\n",
    "\n",
    "        # Compute persona_diff logits and acts\n",
    "        persona_diff_tokens = batch[\"persona_diff_tokens\"].cuda()\n",
    "        persona_diff_indices = batch[\"persona_diff_indices\"]\n",
    "        persona_diff_logits, persona_diff_acts = model.run_with_cache(persona_diff_tokens, names_filter=names_filter)\n",
    "        persona_diff_logits = persona_diff_logits[torch.arange(batch_size), persona_diff_indices]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Do hooked forward pass with seq_diff\n",
    "    model.reset_hooks()\n",
    "    temp_hook = functools.partial(\n",
    "        patching_hook,\n",
    "        acts_idx=clean_indices,\n",
    "        new_acts=seq_diff_acts[names_filter[0]],\n",
    "        new_acts_idx=seq_diff_indices,\n",
    "        das=judgement_rep\n",
    "    )\n",
    "    model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        patched_seq_diff_logits = model(clean_tokens)\n",
    "    patched_seq_diff_logits = patched_seq_diff_logits[torch.arange(batch_size), clean_indices]\n",
    "    loss1 = patching_metric(patched_seq_diff_logits, clean_logits)\n",
    "    loss1.backward()\n",
    "    \n",
    "    # Do hooked forward pass with persona_diff\n",
    "    model.reset_hooks()\n",
    "    temp_hook = functools.partial(\n",
    "        patching_hook,\n",
    "        acts_idx=clean_indices,\n",
    "        new_acts=persona_diff_acts[names_filter[0]],\n",
    "        new_acts_idx=persona_diff_indices,\n",
    "        das=judgement_rep\n",
    "    )\n",
    "    model.blocks[layer].hook_resid_mid.add_hook(temp_hook)\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        patched_persona_diff_logits = model(clean_tokens)\n",
    "    patched_persona_diff_logits = patched_persona_diff_logits[torch.arange(batch_size), clean_indices]\n",
    "    loss2 = patching_metric(patched_persona_diff_logits, persona_diff_logits)\n",
    "    loss2.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    print(f\"Patching seq metric: {loss1.item():.5f}, Patching persona metric: {loss2.item():.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
