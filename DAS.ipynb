{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa6acdfb410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fff90ed6a648a9a28dd1e9a6592468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-13b into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "llama_size = \"13b\"\n",
    "model_name_or_path = f\"meta-llama/Llama-2-{llama_size}-chat-hf\"\n",
    "\n",
    "hf_model, hf_tokenizer = load_model_from_transformers(model_name_or_path)\n",
    "model = from_hf_to_tlens(hf_model, hf_tokenizer, f\"llama-{llama_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_patching_hook(\n",
    "    activation,\n",
    "    hook,\n",
    "    position,\n",
    "    direction\n",
    "):\n",
    "    # get unit vector from direction\n",
    "    direction_unit = direction.detach().clone() / torch.linalg.norm(direction)\n",
    "    \n",
    "    # remove direction component from activations\n",
    "    component = (activation[:, position, :].detach().clone() @ direction_unit).unsqueeze(-1) * direction\n",
    "    activation[:, position, :] = activation[:, position, :] - component\n",
    "\n",
    "    # add direction to activations\n",
    "    activation[:, position, :] = activation[:, position, :] + direction\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_das_loss(output_logits, pos_token_id, neg_token_id):\n",
    "    pos_logits = output_logits[:, pos_token_id]\n",
    "    neg_logits = output_logits[:, neg_token_id]\n",
    "    return -torch.sum(pos_logits - neg_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_direction(model, eval_data, pos_token_id, neg_token_id, direction, batch_size=32, rep_positions=[-1], rep_layers=[-1]):\n",
    "    model.reset_hooks()\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "\n",
    "    # Create data loader for the dataset\n",
    "    eval_dataloader = DataLoader(eval_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    baseline_logits = []\n",
    "    patched_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # Forward pass without directional patching hook for baseline\n",
    "            baseline_out = model(batch)\n",
    "            baseline_logits.append(baseline_out[:, -1])\n",
    "\n",
    "            # Add directional patching hook at the specified positions and layers\n",
    "            for layer in rep_layers:\n",
    "                hook = partial(directional_patching_hook, position=rep_positions, direction=direction)\n",
    "                model.blocks[layer].hook_resid_post.add_hook(hook)\n",
    "            \n",
    "            # Forward pass with directional patching hook\n",
    "            patched_out = model(batch)\n",
    "            patched_logits.append(patched_out[:, -1])\n",
    "            \n",
    "            # Remove hooks after use to prevent interference with future forward passes\n",
    "            model.reset_hooks()\n",
    "\n",
    "    # Convert lists of tensors to single tensors\n",
    "    baseline_logits = torch.cat(baseline_logits, dim=0)\n",
    "    patched_logits = torch.cat(patched_logits, dim=0)\n",
    "\n",
    "    # Calculate and compare the logits for the positive and negative tokens before and after applying the directional patch\n",
    "    pos_baseline_logit = baseline_logits[:, pos_token_id]\n",
    "    neg_baseline_logit = baseline_logits[:, neg_token_id]\n",
    "\n",
    "    pos_patched_logit = patched_logits[:, pos_token_id]\n",
    "    neg_patched_logit = patched_logits[:, neg_token_id]\n",
    "\n",
    "    # Evaluate the effectiveness of the direction based on how it changes the logit differences\n",
    "    baseline_difference = pos_baseline_logit - neg_baseline_logit\n",
    "    patched_difference = pos_patched_logit - neg_patched_logit\n",
    "    effectiveness = (patched_difference - baseline_difference).mean()  \n",
    "\n",
    "    # Find datapoints for which the logits where \"flipped\" by the patch\n",
    "    neg_baseline = neg_baseline_logit > pos_baseline_logit\n",
    "    pos_after_patch = pos_patched_logit > neg_patched_logit\n",
    "\n",
    "    n_flipped = (neg_baseline & pos_after_patch).sum().item()\n",
    "    n_neg = neg_baseline.sum().item()\n",
    "\n",
    "    return effectiveness, n_flipped, n_neg\n",
    "\n",
    "\n",
    "\n",
    "def find_direction_using_das(\n",
    "    model,\n",
    "    pos_token,\n",
    "    neg_token,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    rep_positions=[-1],\n",
    "    rep_layers=[-1],\n",
    "    lr=0.001,\n",
    "    n_epochs=1,\n",
    "    batch_size=32,\n",
    "    parameter_dtype=torch.bfloat16\n",
    "):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    pos_token_id = model.to_tokens(pos_token)[0][1].item()\n",
    "    neg_token_id = model.to_tokens(neg_token)[0][1].item()\n",
    "    \n",
    "    # define direction as a trainable parameter\n",
    "    direction = torch.nn.Parameter(torch.rand(model.cfg.d_model).cuda().to(parameter_dtype))\n",
    "\n",
    "    # create optimizer for the direction\n",
    "    optimizer = torch.optim.Adam([direction], lr=lr)\n",
    "\n",
    "    # create data loader for the dataset\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # train the direction to maximize the difference between the positive and negative token\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            # add directional patching hook at the specified positions and layers\n",
    "            for layer in rep_layers:\n",
    "                hook = partial(\n",
    "                    directional_patching_hook,\n",
    "                    position=rep_positions,\n",
    "                    direction=direction\n",
    "                )\n",
    "                model.blocks[layer].hook_resid_post.add_hook(hook)\n",
    "\n",
    "            # forward pass\n",
    "            output_logits = model(batch)\n",
    "            output_logits_at_pos = output_logits[:, -1]\n",
    "\n",
    "            # calculate loss\n",
    "            loss = calculate_das_loss(output_logits_at_pos, pos_token_id, neg_token_id)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update direction\n",
    "            optimizer.step()\n",
    "\n",
    "            model.reset_hooks()\n",
    "\n",
    "        print()\n",
    "        print(\"Evaluating...\")\n",
    "        effectiveness, n_flipped, n_neg = eval_direction(\n",
    "            model, eval_data, pos_token_id, neg_token_id, direction, batch_size=batch_size, rep_positions=rep_positions, rep_layers=rep_layers\n",
    "        )\n",
    "\n",
    "        print()\n",
    "        print(f\"Effectiveness (mean logit change from {neg_token} towards {pos_token}): {effectiveness:.4f}\")\n",
    "        print(f\"# of flips: {n_flipped}\")\n",
    "        print(f\"# of examples with negative logit diff before patching: {n_neg}\")\n",
    "\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FRAC = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = {\n",
    "    \"lenient\": \"You are lenient on toxicity\\n\",\n",
    "    \"harsh\": \"You are harsh on toxicity\\n\"\n",
    "}\n",
    "\n",
    "possible_labels = (\"CLEAN\", \"TOXIC\")\n",
    "\n",
    "classifier_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_dataset = pd.read_json(\"data/simple_toxic_data_filtered.jsonl\", lines=True)\n",
    "\n",
    "all_lenient_prompts = []\n",
    "all_harsh_prompts = []\n",
    "\n",
    "for prompt in toxicity_dataset[\"prompt\"].to_list():\n",
    "    all_lenient_prompts.append(personas[\"lenient\"] + classifier_prompt.format(sequence=prompt))\n",
    "    all_harsh_prompts.append(personas[\"harsh\"] + classifier_prompt.format(sequence=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(all_lenient_prompts)\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "lenient_prompts_train = np.array(all_lenient_prompts)[indices][:int(n * TRAIN_FRAC)]\n",
    "lenient_prompts_eval = np.array(all_lenient_prompts)[indices][int(n * TRAIN_FRAC):]\n",
    "harsh_prompts_train = np.array(all_harsh_prompts)[indices][:int(n * TRAIN_FRAC)]\n",
    "harsh_prompts_eval = np.array(all_harsh_prompts)[indices][int(n * TRAIN_FRAC):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find toxicity direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '',\n",
       " '<0x0A>',\n",
       " 'You',\n",
       " 'must',\n",
       " 'class',\n",
       " 'ify',\n",
       " 'the',\n",
       " 'to',\n",
       " 'x',\n",
       " 'icity',\n",
       " 'of',\n",
       " 'text',\n",
       " ',',\n",
       " 'returning',\n",
       " 'either',\n",
       " '(',\n",
       " 'C',\n",
       " 'LE',\n",
       " 'AN',\n",
       " ')',\n",
       " 'or',\n",
       " '(',\n",
       " 'TO',\n",
       " 'X',\n",
       " 'IC',\n",
       " ')',\n",
       " 'for',\n",
       " 'any',\n",
       " 'sequence',\n",
       " 'I',\n",
       " 'give',\n",
       " 'you',\n",
       " '.',\n",
       " '',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'the',\n",
       " 'text',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " 'placeholder',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'Your',\n",
       " 'response',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " '(']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(\n",
    "    classifier_prompt.format(sequence=\"placeholder\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_diretion = find_direction_using_das(\n",
    "    model,\n",
    "    \"TOXIC\",\n",
    "    \"CLEAN\",\n",
    "    lenient_prompts_train,\n",
    "    lenient_prompts_eval,\n",
    "    rep_positions=[-7],\n",
    "    rep_layers=[16],\n",
    "    lr=0.001,\n",
    "    n_epochs=5,\n",
    "    batch_size=8,\n",
    "    parameter_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46.7500, device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm(toxicity_diretion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7495"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TO'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(pos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(neg_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_token_id = model.to_tokens(\"TOXIC\")[0][1].item()\n",
    "neg_token_id = model.to_tokens(\"CLEAN\")[0][1].item()\n",
    "\n",
    "eval_dataloader = DataLoader(lenient_prompts_eval, batch_size=8, shuffle=False)\n",
    "\n",
    "baseline_logits = []\n",
    "patched_logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        # Forward pass without directional patching hook for baseline\n",
    "        baseline_out = model(batch)\n",
    "        baseline_logits.append(baseline_out[:, -1])\n",
    "\n",
    "        # Add directional patching hook at the specified positions and layers\n",
    "        for layer in [16]:\n",
    "            hook = partial(directional_patching_hook, position=[-7], direction=toxicity_diretion)\n",
    "            model.blocks[layer].hook_resid_post.add_hook(hook)\n",
    "        \n",
    "        # Forward pass with directional patching hook\n",
    "        patched_out = model(batch)\n",
    "        patched_logits.append(patched_out[:, -1])\n",
    "        \n",
    "        # Remove hooks after use to prevent interference with future forward passes\n",
    "        model.reset_hooks()\n",
    "\n",
    "# Convert lists of tensors to single tensors\n",
    "baseline_logits = torch.cat(baseline_logits, dim=0)\n",
    "patched_logits = torch.cat(patched_logits, dim=0)\n",
    "\n",
    "# Calculate and compare the logits for the positive and negative tokens before and after applying the directional patch\n",
    "pos_baseline_logit = baseline_logits[:, pos_token_id]\n",
    "neg_baseline_logit = baseline_logits[:, neg_token_id]\n",
    "\n",
    "pos_patched_logit = patched_logits[:, pos_token_id]\n",
    "neg_patched_logit = patched_logits[:, neg_token_id]\n",
    "\n",
    "# Evaluate the effectiveness of the direction based on how it changes the logit differences\n",
    "baseline_difference = pos_baseline_logit - neg_baseline_logit\n",
    "patched_difference = pos_patched_logit - neg_patched_logit\n",
    "effectiveness = (patched_difference - baseline_difference).mean()  \n",
    "\n",
    "# Find datapoints for which the logits where \"flipped\" by the patch\n",
    "neg_baseline = neg_baseline_logit > pos_baseline_logit\n",
    "pos_after_patch = pos_patched_logit > neg_patched_logit\n",
    "\n",
    "n_flipped = (neg_baseline & pos_after_patch).sum().item()\n",
    "n_neg = neg_baseline.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a52148116a4e7380fc32a6b37d999d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_prompts = []\n",
    "\n",
    "for batch in tqdm(eval_dataloader):\n",
    "    eval_prompts.extend(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices = torch.lt(patched_difference, 0).nonzero().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential better prompt template with less positional bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da23ad98bdf642aba96ddcf6fc66fd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You are harsh on toxicity.\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "Return only (TOXIC) or (CLEAN). \n",
      "\n",
      "Here is the text:\n",
      "You could do better if you put in more effort\n",
      "\n",
      "Your response:\n",
      "I classify the text as (TO\n"
     ]
    }
   ],
   "source": [
    "# system_prompt = \"\"\"You are harsh on toxicity.\n",
    "# You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "# Return only (TOXIC) or (CLEAN)\"\"\"\n",
    "\n",
    "# user_prompt = \"\"\"Here is the text:\n",
    "# You motherfucking cocksucker! I hate you!\"\"\"\n",
    "\n",
    "# assistant_prefix = \"I classify the text as (\"\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     {\"role\": \"assistant\", \"content\": assistant_prefix}\n",
    "# ]\n",
    "\n",
    "# tokens = model.tokenizer.apply_chat_template(messages)[:-2]\n",
    "\n",
    "prompt2 = \"\"\"You are harsh on toxicity.\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "Return only (TOXIC) or (CLEAN). \n",
    "\n",
    "Here is the text:\n",
    "You could do better if you put in more effort\n",
    "\n",
    "Your response:\n",
    "I classify the text as (\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    # result = model.generate(model.to_tokens(eval_prompts[0]), max_new_tokens=3)\n",
    "    result = model.generate(model.to_tokens(prompt2), max_new_tokens=1, temperature=0.)\n",
    "    # result = model.generate(torch.tensor([tokens]).cuda(), max_new_tokens=1, temperature=0.)\n",
    "print(model.tokenizer.decode(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For comparison, the old prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d256ce4135b2421a8f48d7427135730b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You are lenient on toxicity.\n",
      "\n",
      "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
      "\n",
      "Here is the text:\n",
      "You motherfucking cocksucker. I hate you!\n",
      "\n",
      "Your response:\n",
      "(CLEAN\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"You are lenient on toxicity.\n",
    "\n",
    "You must classify the toxicity of text, returning either (CLEAN) or (TOXIC) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "You motherfucking cocksucker. I hate you!\n",
    "\n",
    "Your response:\n",
    "(\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    # result = model.generate(model.to_tokens(eval_prompts[0]), max_new_tokens=3)\n",
    "    result = model.generate(model.to_tokens(prompt2), max_new_tokens=3, temperature=0.)\n",
    "    # result = model.generate(torch.tensor([tokens]).cuda(), max_new_tokens=1, temperature=0.)\n",
    "print(model.tokenizer.decode(result[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
